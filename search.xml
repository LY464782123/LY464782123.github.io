<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Azkaban Flow 1.0 的使用</title>
    <url>/2021/03/17/Azkaban_Flow_1.0_%E7%9A%84%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<h2 id="一、简介"><a href="#一、简介" class="headerlink" title="一、简介"></a>一、简介</h2><p>Azkaban 主要通过界面上传配置文件来进行任务的调度。它有两个重要的概念：</p>
<ul>
<li><strong>Job</strong>： 你需要执行的调度任务；</li>
<li><strong>Flow</strong>：一个获取多个 Job 及它们之间的依赖关系所组成的图表叫做 Flow。</li>
</ul>
<p>目前 Azkaban 3.x 同时支持 Flow 1.0 和 Flow 2.0，本文主要讲解 Flow 1.0 的使用，下一篇文章会讲解 Flow 2.0 的使用。</p>
<h2 id="二、基本任务调度"><a href="#二、基本任务调度" class="headerlink" title="二、基本任务调度"></a>二、基本任务调度</h2><h3 id="2-1-新建项目"><a href="#2-1-新建项目" class="headerlink" title="2.1 新建项目"></a>2.1 新建项目</h3><p>在 Azkaban 主界面可以创建对应的项目：</p>
<div align="center"> <img src="../pictures/azkaban-create-project.png"> </div>

<h3 id="2-2-任务配置"><a href="#2-2-任务配置" class="headerlink" title="2.2 任务配置"></a>2.2 任务配置</h3><p>新建任务配置文件 <code>Hello-Azkaban.job</code>，内容如下。这里的任务很简单，就是输出一句 <code>&#39;Hello Azkaban!&#39;</code> ：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span>command.job</span><br><span class="line">type=command</span><br><span class="line">command=echo 'Hello Azkaban!'</span><br></pre></td></tr></table></figure>
<h3 id="2-3-打包上传"><a href="#2-3-打包上传" class="headerlink" title="2.3 打包上传"></a>2.3 打包上传</h3><p>将 <code>Hello-Azkaban.job</code> 打包为 <code>zip</code> 压缩文件：</p>
<div align="center"> <img src="../pictures/azkaban-zip.png"> </div>

<p>通过 Web UI 界面上传：</p>
<div align="center"> <img src="../pictures/azkaban-upload.png"> </div>

<p>上传成功后可以看到对应的 Flows：</p>
<div align="center"> <img src="../pictures/azkaban-flows.png"> </div>

<h3 id="2-4-执行任务"><a href="#2-4-执行任务" class="headerlink" title="2.4 执行任务"></a>2.4 执行任务</h3><p>点击页面上的 <code>Execute Flow</code> 执行任务：</p>
<div align="center"> <img src="../pictures/azkaban-execute.png"> </div>

<h3 id="2-5-执行结果"><a href="#2-5-执行结果" class="headerlink" title="2.5 执行结果"></a>2.5 执行结果</h3><p>点击 <code>detail</code> 可以查看到任务的执行日志：</p>
<div align="center"> <img src="../pictures/azkaban-successed.png"> </div>

<div align="center"> <img src="../pictures/azkaban-log.png"> </div>

<h2 id="三、多任务调度"><a href="#三、多任务调度" class="headerlink" title="三、多任务调度"></a>三、多任务调度</h2><h3 id="3-1-依赖配置"><a href="#3-1-依赖配置" class="headerlink" title="3.1 依赖配置"></a>3.1 依赖配置</h3><p>这里假设我们有五个任务（TaskA——TaskE）,D 任务需要在 A，B，C 任务执行完成后才能执行，而 E 任务则需要在 D 任务执行完成后才能执行，这种情况下需要使用 <code>dependencies</code> 属性定义其依赖关系。各任务配置如下：</p>
<p><strong>Task-A.job</strong>   :</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">type=command</span><br><span class="line">command=echo 'Task A'</span><br></pre></td></tr></table></figure>
<p><strong>Task-B.job</strong>   :</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">type=command</span><br><span class="line">command=echo 'Task B'</span><br></pre></td></tr></table></figure>
<p><strong>Task-C.job</strong>   :</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">type=command</span><br><span class="line">command=echo 'Task C'</span><br></pre></td></tr></table></figure>
<p><strong>Task-D.job</strong>   : </p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">type=command</span><br><span class="line">command=echo 'Task D'</span><br><span class="line">dependencies=Task-A,Task-B,Task-C</span><br></pre></td></tr></table></figure>
<p><strong>Task-E.job</strong>   :</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">type=command</span><br><span class="line">command=echo 'Task E'</span><br><span class="line">dependencies=Task-D</span><br></pre></td></tr></table></figure>
<h3 id="3-2-压缩上传"><a href="#3-2-压缩上传" class="headerlink" title="3.2 压缩上传"></a>3.2 压缩上传</h3><p>压缩后进行上传，这里需要注意的是一个 Project 只能接收一个压缩包，这里我还沿用上面的 Project，默认后面的压缩包会覆盖前面的压缩包：</p>
<div align="center"> <img src="../pictures/azkaban-task-abcde-zip.png"> </div>

<h3 id="3-3-依赖关系"><a href="#3-3-依赖关系" class="headerlink" title="3.3 依赖关系"></a>3.3 依赖关系</h3><p>多个任务存在依赖时，默认采用最后一个任务的文件名作为 Flow 的名称，其依赖关系如图：</p>
<div align="center"> <img src="../pictures/azkaban-dependencies.png"> </div>

<h3 id="3-4-执行结果"><a href="#3-4-执行结果" class="headerlink" title="3.4 执行结果"></a>3.4 执行结果</h3><div align="center"> <img src="../pictures/azkaban-task-abcde.png"> </div>

<p>从这个案例可以看出，Flow1.0 无法通过一个 job 文件来完成多个任务的配置，但是 Flow 2.0 就很好的解决了这个问题。</p>
<h2 id="四、调度HDFS作业"><a href="#四、调度HDFS作业" class="headerlink" title="四、调度HDFS作业"></a>四、调度HDFS作业</h2><p>步骤与上面的步骤一致，这里以查看 HDFS 上的文件列表为例。命令建议采用完整路径，配置文件如下：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">type=command</span><br><span class="line">command=/usr/app/hadoop-2.6.0-cdh5.15.2/bin/hadoop fs -ls /</span><br></pre></td></tr></table></figure>
<p>执行结果：</p>
<div align="center"> <img src="../pictures/azkaban-hdfs.png"> </div>

<h2 id="五、调度MR作业"><a href="#五、调度MR作业" class="headerlink" title="五、调度MR作业"></a>五、调度MR作业</h2><p>MR 作业配置：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">type=command</span><br><span class="line">command=/usr/app/hadoop-2.6.0-cdh5.15.2/bin/hadoop jar /usr/app/hadoop-2.6.0-cdh5.15.2/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0-cdh5.15.2.jar pi 3 3</span><br></pre></td></tr></table></figure>
<p>执行结果：</p>
<div align="center"> <img src="../pictures/azkaban-mr.png"> </div>

<h2 id="六、调度Hive作业"><a href="#六、调度Hive作业" class="headerlink" title="六、调度Hive作业"></a>六、调度Hive作业</h2><p>作业配置：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">type=command</span><br><span class="line">command=/usr/app/hive-1.1.0-cdh5.15.2/bin/hive -f 'test.sql'</span><br></pre></td></tr></table></figure>
<p>其中 <code>test.sql</code> 内容如下，创建一张雇员表，然后查看其结构：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">DATABASE</span> <span class="keyword">IF</span> <span class="keyword">NOT</span> <span class="keyword">EXISTS</span> hive;</span><br><span class="line"><span class="keyword">use</span> hive;</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> emp;</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> emp(</span><br><span class="line">empno <span class="built_in">int</span>,</span><br><span class="line">ename <span class="keyword">string</span>,</span><br><span class="line">job <span class="keyword">string</span>,</span><br><span class="line">mgr <span class="built_in">int</span>,</span><br><span class="line">hiredate <span class="keyword">string</span>,</span><br><span class="line">sal <span class="keyword">double</span>,</span><br><span class="line">comm <span class="keyword">double</span>,</span><br><span class="line">deptno <span class="built_in">int</span></span><br><span class="line">) <span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">'\t'</span>;</span><br><span class="line"><span class="comment">-- 查看 emp 表的信息</span></span><br><span class="line">desc emp;</span><br></pre></td></tr></table></figure>
<p>打包的时候将 <code>job</code> 文件与 <code>sql</code> 文件一并进行打包：</p>
<div align="center"> <img src="../pictures/azkaban-hive.png"> </div>

<p>执行结果如下：</p>
<div align="center"> <img src="../pictures/azkaban-hive-result.png"> </div>

<h2 id="七、在线修改作业配置"><a href="#七、在线修改作业配置" class="headerlink" title="七、在线修改作业配置"></a>七、在线修改作业配置</h2><p>在测试时，我们可能需要频繁修改配置，如果每次修改都要重新打包上传，这会比较麻烦。所以 Azkaban 支持配置的在线修改，点击需要修改的 Flow，就可以进入详情页面：</p>
<div align="center"> <img src="../pictures/azkaban-project-edit.png"> </div>

<p>在详情页面点击 <code>Eidt</code> 按钮可以进入编辑页面：</p>
<div align="center"> <img src="../pictures/azkaban-edit.png"> </div>

<p>在编辑页面可以新增配置或者修改配置：</p>
<div align="center"> <img src="../pictures/azkaban-click-edit.png"> </div>

<h2 id="附：可能出现的问题"><a href="#附：可能出现的问题" class="headerlink" title="附：可能出现的问题"></a>附：可能出现的问题</h2><p>如果出现以下异常，多半是因为执行主机内存不足，Azkaban 要求执行主机的可用内存必须大于 3G 才能执行任务：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">Cannot request memory (Xms 0 kb, Xmx 0 kb) from system for job</span><br></pre></td></tr></table></figure>
<div align="center"> <img src="../pictures/azkaban-memory.png"> </div>

<p>如果你的执行主机没办法增大内存，那么可以通过修改 <code>plugins/jobtypes/</code> 目录下的 <code>commonprivate.properties</code> 文件来关闭内存检查，配置如下：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">memCheck.enabled=false</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>notes</category>
        <category>Azkaban</category>
      </categories>
      <tags>
        <tag>Azkaban</tag>
      </tags>
  </entry>
  <entry>
    <title>Azkaban Flow 2.0的使用</title>
    <url>/2021/03/17/Azkaban_Flow_2.0_%E7%9A%84%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<h2 id="一、Flow-2-0-简介"><a href="#一、Flow-2-0-简介" class="headerlink" title="一、Flow 2.0 简介"></a>一、Flow 2.0 简介</h2><h3 id="1-1-Flow-2-0-的产生"><a href="#1-1-Flow-2-0-的产生" class="headerlink" title="1.1 Flow 2.0 的产生"></a>1.1 Flow 2.0 的产生</h3><p>Azkaban 目前同时支持 Flow 1.0 和 Flow2.0 ，但是官方文档上更推荐使用 Flow 2.0，因为 Flow 1.0 会在将来的版本被移除。Flow 2.0 的主要设计思想是提供 1.0 所没有的流级定义。用户可以将属于给定流的所有 <code>job / properties</code> 文件合并到单个流定义文件中，其内容采用 YAML 语法进行定义，同时还支持在流中再定义流，称为为嵌入流或子流。</p>
<h3 id="1-2-基本结构"><a href="#1-2-基本结构" class="headerlink" title="1.2 基本结构"></a>1.2 基本结构</h3><p>项目 zip 将包含多个流 YAML 文件，一个项目 YAML 文件以及可选库和源代码。Flow YAML 文件的基本结构如下：</p>
<ul>
<li>每个 Flow 都在单个 YAML 文件中定义；</li>
<li>流文件以流名称命名，如：<code>my-flow-name.flow</code>；</li>
<li>包含 DAG 中的所有节点；</li>
<li>每个节点可以是作业或流程；</li>
<li>每个节点 可以拥有 name, type, config, dependsOn 和 nodes sections 等属性；</li>
<li>通过列出 dependsOn 列表中的父节点来指定节点依赖性；</li>
<li>包含与流相关的其他配置；</li>
<li>当前 properties 文件中流的所有常见属性都将迁移到每个流 YAML 文件中的 config 部分。</li>
</ul>
<p>官方提供了一个比较完善的配置样例，如下：</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">config:</span></span><br><span class="line">  <span class="attr">user.to.proxy:</span> <span class="string">azktest</span></span><br><span class="line">  <span class="attr">param.hadoopOutData:</span> <span class="string">/tmp/wordcounthadoopout</span></span><br><span class="line">  <span class="attr">param.inData:</span> <span class="string">/tmp/wordcountpigin</span></span><br><span class="line">  <span class="attr">param.outData:</span> <span class="string">/tmp/wordcountpigout</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># This section defines the list of jobs</span></span><br><span class="line"><span class="comment"># A node can be a job or a flow</span></span><br><span class="line"><span class="comment"># In this example, all nodes are jobs</span></span><br><span class="line"><span class="attr">nodes:</span></span><br><span class="line"> <span class="comment"># Job definition</span></span><br><span class="line"> <span class="comment"># The job definition is like a YAMLified version of properties file</span></span><br><span class="line"> <span class="comment"># with one major difference. All custom properties are now clubbed together</span></span><br><span class="line"> <span class="comment"># in a config section in the definition.</span></span><br><span class="line"> <span class="comment"># The first line describes the name of the job</span></span><br><span class="line"> <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">AZTest</span></span><br><span class="line">   <span class="attr">type:</span> <span class="string">noop</span></span><br><span class="line">   <span class="comment"># The dependsOn section contains the list of parent nodes the current</span></span><br><span class="line">   <span class="comment"># node depends on</span></span><br><span class="line">   <span class="attr">dependsOn:</span></span><br><span class="line">     <span class="bullet">-</span> <span class="string">hadoopWC1</span></span><br><span class="line">     <span class="bullet">-</span> <span class="string">NoOpTest1</span></span><br><span class="line">     <span class="bullet">-</span> <span class="string">hive2</span></span><br><span class="line">     <span class="bullet">-</span> <span class="string">java1</span></span><br><span class="line">     <span class="bullet">-</span> <span class="string">jobCommand2</span></span><br><span class="line"></span><br><span class="line"> <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">pigWordCount1</span></span><br><span class="line">   <span class="attr">type:</span> <span class="string">pig</span></span><br><span class="line">   <span class="comment"># The config section contains custom arguments or parameters which are</span></span><br><span class="line">   <span class="comment"># required by the job</span></span><br><span class="line">   <span class="attr">config:</span></span><br><span class="line">     <span class="attr">pig.script:</span> <span class="string">src/main/pig/wordCountText.pig</span></span><br><span class="line"></span><br><span class="line"> <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">hadoopWC1</span></span><br><span class="line">   <span class="attr">type:</span> <span class="string">hadoopJava</span></span><br><span class="line">   <span class="attr">dependsOn:</span></span><br><span class="line">     <span class="bullet">-</span> <span class="string">pigWordCount1</span></span><br><span class="line">   <span class="attr">config:</span></span><br><span class="line">     <span class="attr">classpath:</span> <span class="string">./*</span></span><br><span class="line">     <span class="attr">force.output.overwrite:</span> <span class="literal">true</span></span><br><span class="line">     <span class="attr">input.path:</span> <span class="string">$&#123;param.inData&#125;</span></span><br><span class="line">     <span class="attr">job.class:</span> <span class="string">com.linkedin.wordcount.WordCount</span></span><br><span class="line">     <span class="attr">main.args:</span> <span class="string">$&#123;param.inData&#125;</span> <span class="string">$&#123;param.hadoopOutData&#125;</span></span><br><span class="line">     <span class="attr">output.path:</span> <span class="string">$&#123;param.hadoopOutData&#125;</span></span><br><span class="line"></span><br><span class="line"> <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">hive1</span></span><br><span class="line">   <span class="attr">type:</span> <span class="string">hive</span></span><br><span class="line">   <span class="attr">config:</span></span><br><span class="line">     <span class="attr">hive.script:</span> <span class="string">src/main/hive/showdb.q</span></span><br><span class="line"></span><br><span class="line"> <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">NoOpTest1</span></span><br><span class="line">   <span class="attr">type:</span> <span class="string">noop</span></span><br><span class="line"></span><br><span class="line"> <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">hive2</span></span><br><span class="line">   <span class="attr">type:</span> <span class="string">hive</span></span><br><span class="line">   <span class="attr">dependsOn:</span></span><br><span class="line">     <span class="bullet">-</span> <span class="string">hive1</span></span><br><span class="line">   <span class="attr">config:</span></span><br><span class="line">     <span class="attr">hive.script:</span> <span class="string">src/main/hive/showTables.sql</span></span><br><span class="line"></span><br><span class="line"> <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">java1</span></span><br><span class="line">   <span class="attr">type:</span> <span class="string">javaprocess</span></span><br><span class="line">   <span class="attr">config:</span></span><br><span class="line">     <span class="attr">Xms:</span> <span class="string">96M</span></span><br><span class="line">     <span class="attr">java.class:</span> <span class="string">com.linkedin.foo.HelloJavaProcessJob</span></span><br><span class="line"></span><br><span class="line"> <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">jobCommand1</span></span><br><span class="line">   <span class="attr">type:</span> <span class="string">command</span></span><br><span class="line">   <span class="attr">config:</span></span><br><span class="line">     <span class="attr">command:</span> <span class="string">echo</span> <span class="string">"hello world from job_command_1"</span></span><br><span class="line"></span><br><span class="line"> <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">jobCommand2</span></span><br><span class="line">   <span class="attr">type:</span> <span class="string">command</span></span><br><span class="line">   <span class="attr">dependsOn:</span></span><br><span class="line">     <span class="bullet">-</span> <span class="string">jobCommand1</span></span><br><span class="line">   <span class="attr">config:</span></span><br><span class="line">     <span class="attr">command:</span> <span class="string">echo</span> <span class="string">"hello world from job_command_2"</span></span><br></pre></td></tr></table></figure>
<h2 id="二、YAML语法"><a href="#二、YAML语法" class="headerlink" title="二、YAML语法"></a>二、YAML语法</h2><p>想要使用 Flow 2.0 进行工作流的配置，首先需要了解 YAML 。YAML 是一种简洁的非标记语言，有着严格的格式要求的，如果你的格式配置失败，上传到 Azkaban 的时候就会抛出解析异常。</p>
<h3 id="2-1-基本规则"><a href="#2-1-基本规则" class="headerlink" title="2.1 基本规则"></a>2.1 基本规则</h3><ol>
<li>大小写敏感 ；</li>
<li>使用缩进表示层级关系 ；</li>
<li>缩进长度没有限制，只要元素对齐就表示这些元素属于一个层级； </li>
<li>使用#表示注释 ；</li>
<li>字符串默认不用加单双引号，但单引号和双引号都可以使用，双引号表示不需要对特殊字符进行转义；</li>
<li>YAML 中提供了多种常量结构，包括：整数，浮点数，字符串，NULL，日期，布尔，时间。</li>
</ol>
<h3 id="2-2-对象的写法"><a href="#2-2-对象的写法" class="headerlink" title="2.2 对象的写法"></a>2.2 对象的写法</h3><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="comment"># value 与 ： 符号之间必须要有一个空格</span></span><br><span class="line"><span class="attr">key:</span> <span class="string">value</span></span><br></pre></td></tr></table></figure>
<h3 id="2-3-map的写法"><a href="#2-3-map的写法" class="headerlink" title="2.3 map的写法"></a>2.3 map的写法</h3><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 写法一 同一缩进的所有键值对属于一个map</span></span><br><span class="line"><span class="attr">key:</span> </span><br><span class="line">    <span class="attr">key1:</span> <span class="string">value1</span></span><br><span class="line">    <span class="attr">key2:</span> <span class="string">value2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 写法二</span></span><br><span class="line"><span class="string">&#123;key1:</span> <span class="string">value1,</span> <span class="attr">key2:</span> <span class="string">value2&#125;</span></span><br></pre></td></tr></table></figure>
<h3 id="2-3-数组的写法"><a href="#2-3-数组的写法" class="headerlink" title="2.3 数组的写法"></a>2.3 数组的写法</h3><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 写法一 使用一个短横线加一个空格代表一个数组项</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">a</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">b</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">c</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 写法二</span></span><br><span class="line"><span class="string">[a,b,c]</span></span><br></pre></td></tr></table></figure>
<h3 id="2-5-单双引号"><a href="#2-5-单双引号" class="headerlink" title="2.5 单双引号"></a>2.5 单双引号</h3><p>支持单引号和双引号，但双引号不会对特殊字符进行转义：</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">s1:</span> <span class="string">'内容\n 字符串'</span></span><br><span class="line"><span class="attr">s2:</span> <span class="string">"内容\n 字符串"</span></span><br><span class="line"></span><br><span class="line"><span class="string">转换后：</span></span><br><span class="line"><span class="string">&#123;</span> <span class="attr">s1:</span> <span class="string">'内容\\n 字符串'</span><span class="string">,</span> <span class="attr">s2:</span> <span class="string">'内容\n 字符串'</span> <span class="string">&#125;</span></span><br></pre></td></tr></table></figure>
<h3 id="2-6-特殊符号"><a href="#2-6-特殊符号" class="headerlink" title="2.6 特殊符号"></a>2.6 特殊符号</h3><p>一个 YAML 文件中可以包括多个文档，使用 <code>---</code> 进行分割。</p>
<h3 id="2-7-配置引用"><a href="#2-7-配置引用" class="headerlink" title="2.7 配置引用"></a>2.7 配置引用</h3><p>Flow 2.0 建议将公共参数定义在 <code>config</code> 下，并通过 <code>${}</code> 进行引用。</p>
<h2 id="三、简单任务调度"><a href="#三、简单任务调度" class="headerlink" title="三、简单任务调度"></a>三、简单任务调度</h2><h3 id="3-1-任务配置"><a href="#3-1-任务配置" class="headerlink" title="3.1 任务配置"></a>3.1 任务配置</h3><p>新建 <code>flow</code> 配置文件：</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">nodes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">jobA</span></span><br><span class="line">    <span class="attr">type:</span> <span class="string">command</span></span><br><span class="line">    <span class="attr">config:</span></span><br><span class="line">      <span class="attr">command:</span> <span class="string">echo</span> <span class="string">"Hello Azkaban Flow 2.0."</span></span><br></pre></td></tr></table></figure>
<p>在当前的版本中，Azkaban 同时支持 Flow 1.0 和 Flow 2.0，如果你希望以 2.0 的方式运行，则需要新建一个 <code>project</code> 文件，指明是使用的是 Flow 2.0：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">azkaban-flow-version: 2.0</span><br></pre></td></tr></table></figure>
<h3 id="3-2-打包上传"><a href="#3-2-打包上传" class="headerlink" title="3.2 打包上传"></a>3.2 打包上传</h3><div align="center"> <img src="../pictures/azkaban-simple.png"> </div>



<h3 id="3-3-执行结果"><a href="#3-3-执行结果" class="headerlink" title="3.3 执行结果"></a>3.3 执行结果</h3><p>由于在 1.0 版本中已经介绍过 Web UI 的使用，这里就不再赘述。对于 1.0 和 2.0 版本，只有配置方式有所不同，其他上传执行的方式都是相同的。执行结果如下：</p>
<div align="center"> <img src="../pictures/azkaban-simle-result.png"> </div>

<h2 id="四、多任务调度"><a href="#四、多任务调度" class="headerlink" title="四、多任务调度"></a>四、多任务调度</h2><p>和 1.0 给出的案例一样，这里假设我们有五个任务（jobA——jobE）, D 任务需要在 A，B，C 任务执行完成后才能执行，而 E 任务则需要在 D 任务执行完成后才能执行，相关配置文件应如下。可以看到在 1.0 中我们需要分别定义五个配置文件，而在 2.0 中我们只需要一个配置文件即可完成配置。</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">nodes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">jobE</span></span><br><span class="line">    <span class="attr">type:</span> <span class="string">command</span></span><br><span class="line">    <span class="attr">config:</span></span><br><span class="line">      <span class="attr">command:</span> <span class="string">echo</span> <span class="string">"This is job E"</span></span><br><span class="line">    <span class="comment"># jobE depends on jobD</span></span><br><span class="line">    <span class="attr">dependsOn:</span> </span><br><span class="line">      <span class="bullet">-</span> <span class="string">jobD</span></span><br><span class="line">    </span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">jobD</span></span><br><span class="line">    <span class="attr">type:</span> <span class="string">command</span></span><br><span class="line">    <span class="attr">config:</span></span><br><span class="line">      <span class="attr">command:</span> <span class="string">echo</span> <span class="string">"This is job D"</span></span><br><span class="line">    <span class="comment"># jobD depends on jobA、jobB、jobC</span></span><br><span class="line">    <span class="attr">dependsOn:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">jobA</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">jobB</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">jobC</span></span><br><span class="line"></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">jobA</span></span><br><span class="line">    <span class="attr">type:</span> <span class="string">command</span></span><br><span class="line">    <span class="attr">config:</span></span><br><span class="line">      <span class="attr">command:</span> <span class="string">echo</span> <span class="string">"This is job A"</span></span><br><span class="line"></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">jobB</span></span><br><span class="line">    <span class="attr">type:</span> <span class="string">command</span></span><br><span class="line">    <span class="attr">config:</span></span><br><span class="line">      <span class="attr">command:</span> <span class="string">echo</span> <span class="string">"This is job B"</span></span><br><span class="line"></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">jobC</span></span><br><span class="line">    <span class="attr">type:</span> <span class="string">command</span></span><br><span class="line">    <span class="attr">config:</span></span><br><span class="line">      <span class="attr">command:</span> <span class="string">echo</span> <span class="string">"This is job C"</span></span><br></pre></td></tr></table></figure>
<h2 id="五、内嵌流"><a href="#五、内嵌流" class="headerlink" title="五、内嵌流"></a>五、内嵌流</h2><p>Flow2.0 支持在一个 Flow 中定义另一个 Flow，称为内嵌流或者子流。这里给出一个内嵌流的示例，其 <code>Flow</code> 配置如下：</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">nodes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">jobC</span></span><br><span class="line">    <span class="attr">type:</span> <span class="string">command</span></span><br><span class="line">    <span class="attr">config:</span></span><br><span class="line">      <span class="attr">command:</span> <span class="string">echo</span> <span class="string">"This is job C"</span></span><br><span class="line">    <span class="attr">dependsOn:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">embedded_flow</span></span><br><span class="line"></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">embedded_flow</span></span><br><span class="line">    <span class="attr">type:</span> <span class="string">flow</span></span><br><span class="line">    <span class="attr">config:</span></span><br><span class="line">      <span class="attr">prop:</span> <span class="string">value</span></span><br><span class="line">    <span class="attr">nodes:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">jobB</span></span><br><span class="line">        <span class="attr">type:</span> <span class="string">command</span></span><br><span class="line">        <span class="attr">config:</span></span><br><span class="line">          <span class="attr">command:</span> <span class="string">echo</span> <span class="string">"This is job B"</span></span><br><span class="line">        <span class="attr">dependsOn:</span></span><br><span class="line">          <span class="bullet">-</span> <span class="string">jobA</span></span><br><span class="line"></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">jobA</span></span><br><span class="line">        <span class="attr">type:</span> <span class="string">command</span></span><br><span class="line">        <span class="attr">config:</span></span><br><span class="line">          <span class="attr">command:</span> <span class="string">echo</span> <span class="string">"This is job A"</span></span><br></pre></td></tr></table></figure>
<p>内嵌流的 DAG 图如下：</p>
<div align="center"> <img src="../pictures/azkaban-embeded-flow.png"> </div>

<p>执行情况如下：</p>
<div align="center"> <img src="../pictures/azkaban-embeded-success.png"> </div>



<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol>
<li><a href="https://github.com/azkaban/azkaban/wiki/Azkaban-Flow-2.0-Design" target="_blank" rel="noopener">Azkaban Flow 2.0 Design</a></li>
<li><a href="https://github.com/azkaban/azkaban/wiki/Getting-started-with-Azkaban-Flow-2.0" target="_blank" rel="noopener">Getting started with Azkaban Flow 2.0</a></li>
</ol>
]]></content>
      <categories>
        <category>notes</category>
        <category>Azkaban</category>
      </categories>
      <tags>
        <tag>Azkaban</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink Sink</title>
    <url>/2021/03/17/Flink_Data_Sink/</url>
    <content><![CDATA[<h2 id="一、Data-Sinks"><a href="#一、Data-Sinks" class="headerlink" title="一、Data Sinks"></a>一、Data Sinks</h2><p>在使用 Flink 进行数据处理时，数据经 Data Source 流入，然后通过系列 Transformations 的转化，最终可以通过 Sink 将计算结果进行输出，Flink Data Sinks 就是用于定义数据流最终的输出位置。Flink 提供了几个较为简单的 Sink API 用于日常的开发，具体如下：</p>
<h3 id="1-1-writeAsText"><a href="#1-1-writeAsText" class="headerlink" title="1.1 writeAsText"></a>1.1 writeAsText</h3><p><code>writeAsText</code> 用于将计算结果以文本的方式并行地写入到指定文件夹下，除了路径参数是必选外，该方法还可以通过指定第二个参数来定义输出模式，它有以下两个可选值：</p>
<ul>
<li><strong>WriteMode.NO_OVERWRITE</strong>：当指定路径上不存在任何文件时，才执行写出操作；</li>
<li><strong>WriteMode.OVERWRITE</strong>：不论指定路径上是否存在文件，都执行写出操作；如果原来已有文件，则进行覆盖。</li>
</ul>
<p>使用示例如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">streamSource.writeAsText(<span class="string">"D:\\out"</span>, FileSystem.WriteMode.OVERWRITE);</span><br></pre></td></tr></table></figure>
<p>以上写出是以并行的方式写出到多个文件，如果想要将输出结果全部写出到一个文件，需要设置其并行度为 1：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">streamSource.writeAsText(<span class="string">"D:\\out"</span>, FileSystem.WriteMode.OVERWRITE).setParallelism(<span class="number">1</span>);</span><br></pre></td></tr></table></figure>
<h3 id="1-2-writeAsCsv"><a href="#1-2-writeAsCsv" class="headerlink" title="1.2 writeAsCsv"></a>1.2 writeAsCsv</h3><p><code>writeAsCsv</code> 用于将计算结果以 CSV 的文件格式写出到指定目录，除了路径参数是必选外，该方法还支持传入输出模式，行分隔符，和字段分隔符三个额外的参数，其方法定义如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">writeAsCsv(String path, WriteMode writeMode, String rowDelimiter, String fieldDelimiter)</span><br></pre></td></tr></table></figure>
<h3 id="1-3-print-printToErr"><a href="#1-3-print-printToErr" class="headerlink" title="1.3 print \ printToErr"></a>1.3 print \ printToErr</h3><p><code>print \ printToErr</code> 是测试当中最常用的方式，用于将计算结果以标准输出流或错误输出流的方式打印到控制台上。</p>
<h3 id="1-4-writeUsingOutputFormat"><a href="#1-4-writeUsingOutputFormat" class="headerlink" title="1.4 writeUsingOutputFormat"></a>1.4 writeUsingOutputFormat</h3><p>采用自定义的输出格式将计算结果写出，上面介绍的 <code>writeAsText</code> 和 <code>writeAsCsv</code> 其底层调用的都是该方法，源码如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> DataStreamSink&lt;T&gt; <span class="title">writeAsText</span><span class="params">(String path, WriteMode writeMode)</span> </span>&#123;</span><br><span class="line">    TextOutputFormat&lt;T&gt; tof = <span class="keyword">new</span> TextOutputFormat&lt;&gt;(<span class="keyword">new</span> Path(path));</span><br><span class="line">    tof.setWriteMode(writeMode);</span><br><span class="line">    <span class="keyword">return</span> writeUsingOutputFormat(tof);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="1-5-writeToSocket"><a href="#1-5-writeToSocket" class="headerlink" title="1.5 writeToSocket"></a>1.5 writeToSocket</h3><p><code>writeToSocket</code> 用于将计算结果以指定的格式写出到 Socket 中，使用示例如下：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">streamSource.writeToSocket("192.168.0.226", 9999, new SimpleStringSchema());</span><br></pre></td></tr></table></figure>
<h2 id="二、Streaming-Connectors"><a href="#二、Streaming-Connectors" class="headerlink" title="二、Streaming Connectors"></a>二、Streaming Connectors</h2><p>除了上述 API 外，Flink 中还内置了系列的 Connectors 连接器，用于将计算结果输入到常用的存储系统或者消息中间件中，具体如下：</p>
<ul>
<li>Apache Kafka (支持 source 和 sink)</li>
<li>Apache Cassandra (sink)</li>
<li>Amazon Kinesis Streams (source/sink)</li>
<li>Elasticsearch (sink)</li>
<li>Hadoop FileSystem (sink)</li>
<li>RabbitMQ (source/sink)</li>
<li>Apache NiFi (source/sink)</li>
<li>Google PubSub (source/sink)</li>
</ul>
<p>除了内置的连接器外，你还可以通过 Apache Bahir 的连接器扩展 Flink。Apache Bahir 旨在为分布式数据分析系统 (如 Spark，Flink) 等提供功能上的扩展，当前其支持的与 Flink Sink 相关的连接器如下：</p>
<ul>
<li>Apache ActiveMQ (source/sink)</li>
<li>Apache Flume (sink)</li>
<li>Redis (sink)</li>
<li>Akka (sink)</li>
</ul>
<p>这里接着在 Data Sources 章节介绍的整合 Kafka Source 的基础上，将 Kafka Sink 也一并进行整合，具体步骤如下。</p>
<h2 id="三、整合-Kafka-Sink"><a href="#三、整合-Kafka-Sink" class="headerlink" title="三、整合 Kafka Sink"></a>三、整合 Kafka Sink</h2><h3 id="3-1-addSink"><a href="#3-1-addSink" class="headerlink" title="3.1 addSink"></a>3.1 addSink</h3><p>Flink 提供了 addSink 方法用来调用自定义的 Sink 或者第三方的连接器，想要将计算结果写出到 Kafka，需要使用该方法来调用 Kafka 的生产者 FlinkKafkaProducer，具体代码如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 1.指定Kafka的相关配置属性</span></span><br><span class="line">Properties properties = <span class="keyword">new</span> Properties();</span><br><span class="line">properties.setProperty(<span class="string">"bootstrap.servers"</span>, <span class="string">"192.168.200.0:9092"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 2.接收Kafka上的数据</span></span><br><span class="line">DataStream&lt;String&gt; stream = env</span><br><span class="line">    .addSource(<span class="keyword">new</span> FlinkKafkaConsumer&lt;&gt;(<span class="string">"flink-stream-in-topic"</span>, <span class="keyword">new</span> SimpleStringSchema(), properties));</span><br><span class="line"></span><br><span class="line"><span class="comment">// 3.定义计算结果到 Kafka ProducerRecord 的转换</span></span><br><span class="line">KafkaSerializationSchema&lt;String&gt; kafkaSerializationSchema = <span class="keyword">new</span> KafkaSerializationSchema&lt;String&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> ProducerRecord&lt;<span class="keyword">byte</span>[], <span class="keyword">byte</span>[]&gt; serialize(String element, <span class="meta">@Nullable</span> Long timestamp) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> ProducerRecord&lt;&gt;(<span class="string">"flink-stream-out-topic"</span>, element.getBytes());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="comment">// 4. 定义Flink Kafka生产者</span></span><br><span class="line">FlinkKafkaProducer&lt;String&gt; kafkaProducer = <span class="keyword">new</span> FlinkKafkaProducer&lt;&gt;(<span class="string">"flink-stream-out-topic"</span>,</span><br><span class="line">                                                                    kafkaSerializationSchema,</span><br><span class="line">                                                                    properties,</span><br><span class="line">                                               FlinkKafkaProducer.Semantic.AT_LEAST_ONCE, <span class="number">5</span>);</span><br><span class="line"><span class="comment">// 5. 将接收到输入元素*2后写出到Kafka</span></span><br><span class="line">stream.map((MapFunction&lt;String, String&gt;) value -&gt; value + value).addSink(kafkaProducer);</span><br><span class="line">env.execute(<span class="string">"Flink Streaming"</span>);</span><br></pre></td></tr></table></figure>
<h3 id="3-2-创建输出主题"><a href="#3-2-创建输出主题" class="headerlink" title="3.2 创建输出主题"></a>3.2 创建输出主题</h3><p>创建用于输出测试的主题：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">bin/kafka-topics.sh --create \</span><br><span class="line">                    --bootstrap-server hadoop001:9092 \</span><br><span class="line">                    --replication-factor 1 \</span><br><span class="line">                    --partitions 1  \</span><br><span class="line">                    --topic flink-stream-out-topic</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 查看所有主题</span><br><span class="line"> bin/kafka-topics.sh --list --bootstrap-server hadoop001:9092</span><br></pre></td></tr></table></figure>
<h3 id="3-3-启动消费者"><a href="#3-3-启动消费者" class="headerlink" title="3.3 启动消费者"></a>3.3 启动消费者</h3><p>启动一个 Kafka 消费者，用于查看 Flink 程序的输出情况：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">bin/kafka-console-consumer.sh --bootstrap-server hadoop001:<span class="number">9092</span> --topic flink-stream-out-topic</span><br></pre></td></tr></table></figure>
<h3 id="3-4-测试结果"><a href="#3-4-测试结果" class="headerlink" title="3.4 测试结果"></a>3.4 测试结果</h3><p>在 Kafka 生产者上发送消息到 Flink 程序，观察 Flink 程序转换后的输出情况，具体如下：</p>
<div align="center"> <img src="../pictures/flink-kafka-producer-consumer.png"> </div>


<p>可以看到 Kafka 生成者发出的数据已经被 Flink 程序正常接收到，并经过转换后又输出到 Kafka 对应的 Topic 上。</p>
<h2 id="四、自定义-Sink"><a href="#四、自定义-Sink" class="headerlink" title="四、自定义 Sink"></a>四、自定义 Sink</h2><p>除了使用内置的第三方连接器外，Flink 还支持使用自定义的 Sink 来满足多样化的输出需求。想要实现自定义的 Sink ，需要直接或者间接实现 SinkFunction 接口。通常情况下，我们都是实现其抽象类 RichSinkFunction，相比于 SinkFunction ，其提供了更多的与生命周期相关的方法。两者间的关系如下：</p>
<div align="center"> <img src="../pictures/flink-richsink.png"> </div>


<p>这里我们以自定义一个 FlinkToMySQLSink 为例，将计算结果写出到 MySQL 数据库中，具体步骤如下：</p>
<h3 id="4-1-导入依赖"><a href="#4-1-导入依赖" class="headerlink" title="4.1 导入依赖"></a>4.1 导入依赖</h3><p>首先需要导入 MySQL 相关的依赖：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>mysql<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>mysql-connector-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>8.0.16<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h3 id="4-2-自定义-Sink"><a href="#4-2-自定义-Sink" class="headerlink" title="4.2 自定义 Sink"></a>4.2 自定义 Sink</h3><p>继承自 RichSinkFunction，实现自定义的 Sink ：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlinkToMySQLSink</span> <span class="keyword">extends</span> <span class="title">RichSinkFunction</span>&lt;<span class="title">Employee</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> PreparedStatement stmt;</span><br><span class="line">    <span class="keyword">private</span> Connection conn;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        Class.forName(<span class="string">"com.mysql.cj.jdbc.Driver"</span>);</span><br><span class="line">        conn = DriverManager.getConnection(<span class="string">"jdbc:mysql://192.168.0.229:3306/employees"</span> +</span><br><span class="line">                                           <span class="string">"?characterEncoding=UTF-8&amp;serverTimezone=UTC&amp;useSSL=false"</span>, </span><br><span class="line">                                           <span class="string">"root"</span>, </span><br><span class="line">                                           <span class="string">"123456"</span>);</span><br><span class="line">        String sql = <span class="string">"insert into emp(name, age, birthday) values(?, ?, ?)"</span>;</span><br><span class="line">        stmt = conn.prepareStatement(sql);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">invoke</span><span class="params">(Employee value, Context context)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        stmt.setString(<span class="number">1</span>, value.getName());</span><br><span class="line">        stmt.setInt(<span class="number">2</span>, value.getAge());</span><br><span class="line">        stmt.setDate(<span class="number">3</span>, value.getBirthday());</span><br><span class="line">        stmt.executeUpdate();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">super</span>.close();</span><br><span class="line">        <span class="keyword">if</span> (stmt != <span class="keyword">null</span>) &#123;</span><br><span class="line">            stmt.close();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (conn != <span class="keyword">null</span>) &#123;</span><br><span class="line">            conn.close();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="4-3-使用自定义-Sink"><a href="#4-3-使用自定义-Sink" class="headerlink" title="4.3 使用自定义 Sink"></a>4.3 使用自定义 Sink</h3><p>想要使用自定义的 Sink，同样是需要调用 addSink 方法，具体如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">Date date = <span class="keyword">new</span> Date(System.currentTimeMillis());</span><br><span class="line">DataStreamSource&lt;Employee&gt; streamSource = env.fromElements(</span><br><span class="line">    <span class="keyword">new</span> Employee(<span class="string">"hei"</span>, <span class="number">10</span>, date),</span><br><span class="line">    <span class="keyword">new</span> Employee(<span class="string">"bai"</span>, <span class="number">20</span>, date),</span><br><span class="line">    <span class="keyword">new</span> Employee(<span class="string">"ying"</span>, <span class="number">30</span>, date));</span><br><span class="line">streamSource.addSink(<span class="keyword">new</span> FlinkToMySQLSink());</span><br><span class="line">env.execute();</span><br></pre></td></tr></table></figure>
<h3 id="4-4-测试结果"><a href="#4-4-测试结果" class="headerlink" title="4.4 测试结果"></a>4.4 测试结果</h3><p>启动程序，观察数据库写入情况：</p>
<div align="center"> <img src="../pictures/flink-mysql-sink.png"> </div>


<p>数据库成功写入，代表自定义 Sink 整合成功。</p>
<blockquote>
<p>以上所有用例的源码见本仓库：<a href="https://github.com/heibaiying/BigData-Notes/tree/master/code/Flink/flink-kafka-integration" target="_blank" rel="noopener">flink-kafka-integration</a></p>
</blockquote>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol>
<li>data-sinks： <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/datastream_api.html#data-sinks" target="_blank" rel="noopener">https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/datastream_api.html#data-sinks</a> </li>
<li>Streaming Connectors：<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/connectors/index.html" target="_blank" rel="noopener">https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/connectors/index.html</a></li>
<li>Apache Kafka Connector： <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/connectors/kafka.html" target="_blank" rel="noopener">https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/connectors/kafka.html</a> </li>
</ol>
]]></content>
      <categories>
        <category>notes</category>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Azkaban简介</title>
    <url>/2021/03/17/Azkaban%E7%AE%80%E4%BB%8B/</url>
    <content><![CDATA[<h2 id="一、Azkaban-介绍"><a href="#一、Azkaban-介绍" class="headerlink" title="一、Azkaban 介绍"></a>一、Azkaban 介绍</h2><h4 id="1-1-背景"><a href="#1-1-背景" class="headerlink" title="1.1 背景"></a>1.1 背景</h4><p>一个完整的大数据分析系统，必然由很多任务单元 (如数据收集、数据清洗、数据存储、数据分析等) 组成，所有的任务单元及其之间的依赖关系组成了复杂的工作流。复杂的工作流管理涉及到很多问题：</p>
<ul>
<li>如何定时调度某个任务？</li>
<li>如何在某个任务执行完成后再去执行另一个任务？</li>
<li>如何在任务失败时候发出预警？</li>
<li>……</li>
</ul>
<p>面对这些问题，工作流调度系统应运而生。Azkaban 就是其中之一。</p>
<h4 id="1-2-功能"><a href="#1-2-功能" class="headerlink" title="1.2 功能"></a>1.2 功能</h4><p>Azkaban 产生于 LinkedIn，并经过多年生产环境的检验，它具备以下功能：</p>
<ul>
<li>兼容任何版本的 Hadoop</li>
<li>易于使用的 Web UI</li>
<li>可以使用简单的 Web 页面进行工作流上传</li>
<li>支持按项目进行独立管理</li>
<li>定时任务调度</li>
<li>模块化和可插入</li>
<li>身份验证和授权</li>
<li>跟踪用户操作</li>
<li>支持失败和成功的电子邮件提醒</li>
<li>SLA 警报和自动查杀失败任务</li>
<li>重试失败的任务</li>
</ul>
<p>Azkaban 的设计理念是在保证功能实现的基础上兼顾易用性，其页面风格清晰明朗，下面是其 WEB UI 界面：</p>
<div align="center"> <img src="../pictures/azkaban-web.png"> </div>

<h2 id="二、Azkaban-和-Oozie"><a href="#二、Azkaban-和-Oozie" class="headerlink" title="二、Azkaban 和 Oozie"></a>二、Azkaban 和 Oozie</h2><p>Azkaban 和 Oozie 都是目前使用最为广泛的工作流调度程序，其主要区别如下：</p>
<h4 id="功能对比"><a href="#功能对比" class="headerlink" title="功能对比"></a>功能对比</h4><ul>
<li>两者均可以调度 Linux 命令、MapReduce、Spark、Pig、Java、Hive 等工作流任务；</li>
<li>两者均可以定时执行工作流任务。</li>
</ul>
<h4 id="工作流定义"><a href="#工作流定义" class="headerlink" title="工作流定义"></a>工作流定义</h4><ul>
<li>Azkaban 使用 Properties(Flow 1.0) 和 YAML(Flow 2.0) 文件定义工作流；</li>
<li>Oozie 使用 Hadoop 流程定义语言（hadoop process defination language，HPDL）来描述工作流，HPDL 是一种 XML 流程定义语言。</li>
</ul>
<h4 id="资源管理"><a href="#资源管理" class="headerlink" title="资源管理"></a>资源管理</h4><ul>
<li>Azkaban 有较严格的权限控制，如用户对工作流进行读/写/执行等操作；</li>
<li>Oozie 暂无严格的权限控制。</li>
</ul>
<h4 id="运行模式"><a href="#运行模式" class="headerlink" title="运行模式"></a>运行模式</h4><ul>
<li><p>Azkaban 3.x 提供了两种运行模式：</p>
<ul>
<li><strong>solo server model(单服务模式)</strong> ：元数据默认存放在内置的 H2 数据库（可以修改为 MySQL），该模式中 <code>webServer</code>(管理服务器) 和 <code>executorServer</code>(执行服务器) 运行在同一个进程中，进程名是 <code>AzkabanSingleServer</code>。该模式适用于小规模工作流的调度。</li>
<li><strong>multiple-executor(分布式多服务模式)</strong> ：存放元数据的数据库为 MySQL，MySQL 应采用主从模式进行备份和容错。这种模式下 <code>webServer</code> 和 <code>executorServer</code> 在不同进程中运行，彼此之间互不影响，适合用于生产环境。</li>
</ul>
</li>
<li><p>Oozie 使用 Tomcat 等 Web 容器来展示 Web 页面，默认使用 derby 存储工作流的元数据，由于 derby 过于轻量，实际使用中通常用 MySQL 代替。</p>
</li>
</ul>
<h2 id="三、总结"><a href="#三、总结" class="headerlink" title="三、总结"></a>三、总结</h2><p>如果你的工作流不是特别复杂，推荐使用轻量级的 Azkaban，主要有以下原因：</p>
<ul>
<li><strong>安装方面</strong>：Azkaban 3.0 之前都是提供安装包的，直接解压部署即可。Azkaban 3.0 之后的版本需要编译，这个编译是基于 gradle 的，自动化程度比较高；</li>
<li><strong>页面设计</strong>：所有任务的依赖关系、执行结果、执行日志都可以从界面上直观查看到；</li>
<li><strong>配置方面</strong>：Azkaban Flow 1.0 基于 Properties 文件来定义工作流，这个时候的限制可能会多一点。但是在 Flow 2.0 就支持了 YARM。YARM 语法更加灵活简单，著名的微服务框架 Spring Boot 就采用的 YAML 代替了繁重的 XML。</li>
</ul>
]]></content>
      <categories>
        <category>notes</category>
        <category>Azkaban</category>
      </categories>
      <tags>
        <tag>Azkaban</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink Windows</title>
    <url>/2021/03/17/Flink_Windows/</url>
    <content><![CDATA[<h2 id="一、窗口概念"><a href="#一、窗口概念" class="headerlink" title="一、窗口概念"></a>一、窗口概念</h2><p>在大多数场景下，我们需要统计的数据流都是无界的，因此我们无法等待整个数据流终止后才进行统计。通常情况下，我们只需要对某个时间范围或者数量范围内的数据进行统计分析：如每隔五分钟统计一次过去一小时内所有商品的点击量；或者每发生1000次点击后，都去统计一下每个商品点击率的占比。在 Flink 中，我们使用窗口 (Window) 来实现这类功能。按照统计维度的不同，Flink 中的窗口可以分为 时间窗口 (Time Windows) 和 计数窗口 (Count Windows) 。</p>
<h2 id="二、Time-Windows"><a href="#二、Time-Windows" class="headerlink" title="二、Time Windows"></a>二、Time Windows</h2><p>Time Windows 用于以时间为维度来进行数据聚合，具体分为以下四类：</p>
<h3 id="2-1-Tumbling-Windows"><a href="#2-1-Tumbling-Windows" class="headerlink" title="2.1 Tumbling Windows"></a>2.1 Tumbling Windows</h3><p>滚动窗口 (Tumbling Windows) 是指彼此之间没有重叠的窗口。例如：每隔1小时统计过去1小时内的商品点击量，那么 1 天就只能分为 24 个窗口，每个窗口彼此之间是不存在重叠的，具体如下：</p>
<div align="center"> <img width="600px" src="../pictures/flink-tumbling-windows.png"> </div>


<p>这里我们以词频统计为例，给出一个具体的用例，代码如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"><span class="comment">// 接收socket上的数据输入</span></span><br><span class="line">DataStreamSource&lt;String&gt; streamSource = env.socketTextStream(<span class="string">"hadoop001"</span>, <span class="number">9999</span>, <span class="string">"\n"</span>, <span class="number">3</span>);</span><br><span class="line">streamSource.flatMap(<span class="keyword">new</span> FlatMapFunction&lt;String, Tuple2&lt;String, Long&gt;&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(String value, Collector&lt;Tuple2&lt;String, Long&gt;&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        String[] words = value.split(<span class="string">"\t"</span>);</span><br><span class="line">        <span class="keyword">for</span> (String word : words) &#123;</span><br><span class="line">            out.collect(<span class="keyword">new</span> Tuple2&lt;&gt;(word, <span class="number">1L</span>));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;).keyBy(<span class="number">0</span>).timeWindow(Time.seconds(<span class="number">3</span>)).sum(<span class="number">1</span>).print(); <span class="comment">//每隔3秒统计一次每个单词出现的数量</span></span><br><span class="line">env.execute(<span class="string">"Flink Streaming"</span>);</span><br></pre></td></tr></table></figure>
<p>测试结果如下：</p>
<div align="center"> <img src="../pictures/flink-window-word-count.png"> </div>




<h3 id="2-2-Sliding-Windows"><a href="#2-2-Sliding-Windows" class="headerlink" title="2.2 Sliding Windows"></a>2.2 Sliding Windows</h3><p>滑动窗口用于滚动进行聚合分析，例如：每隔 6 分钟统计一次过去一小时内所有商品的点击量，那么统计窗口彼此之间就是存在重叠的，即 1天可以分为 240 个窗口。图示如下：</p>
<div align="center"> <img width="600px" src="../pictures/flink-sliding-windows.png"> </div>


<p>可以看到 window 1 - 4 这四个窗口彼此之间都存在着时间相等的重叠部分。想要实现滑动窗口，只需要在使用 timeWindow 方法时额外传递第二个参数作为滚动时间即可，具体如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 每隔3秒统计一次过去1分钟内的数据</span></span><br><span class="line">timeWindow(Time.minutes(<span class="number">1</span>),Time.seconds(<span class="number">3</span>))</span><br></pre></td></tr></table></figure>
<h3 id="2-3-Session-Windows"><a href="#2-3-Session-Windows" class="headerlink" title="2.3 Session Windows"></a>2.3 Session Windows</h3><p>当用户在进行持续浏览时，可能每时每刻都会有点击数据，例如在活动区间内，用户可能频繁的将某类商品加入和移除购物车，而你只想知道用户本次浏览最终的购物车情况，此时就可以在用户持有的会话结束后再进行统计。想要实现这类统计，可以通过 Session Windows 来进行实现。</p>
<div align="center"> <img width="600px" src="../pictures/flink-session-windows.png"> </div>


<p>具体的实现代码如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 以处理时间为衡量标准，如果10秒内没有任何数据输入，就认为会话已经关闭，此时触发统计</span></span><br><span class="line">window(ProcessingTimeSessionWindows.withGap(Time.seconds(<span class="number">10</span>)))</span><br><span class="line"><span class="comment">// 以事件时间为衡量标准    </span></span><br><span class="line">window(EventTimeSessionWindows.withGap(Time.seconds(<span class="number">10</span>)))</span><br></pre></td></tr></table></figure>
<h3 id="2-4-Global-Windows"><a href="#2-4-Global-Windows" class="headerlink" title="2.4 Global Windows"></a>2.4 Global Windows</h3><p>最后一个窗口是全局窗口， 全局窗口会将所有 key 相同的元素分配到同一个窗口中，其通常配合触发器 (trigger) 进行使用。如果没有相应触发器，则计算将不会被执行。</p>
<div align="center"> <img width="600px" src="../pictures/flink-non-windowed.png"> </div>


<p>这里继续以上面词频统计的案例为例，示例代码如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 当单词累计出现的次数每达到10次时，则触发计算，计算整个窗口内该单词出现的总数</span></span><br><span class="line">window(GlobalWindows.create()).trigger(CountTrigger.of(<span class="number">10</span>)).sum(<span class="number">1</span>).print();</span><br></pre></td></tr></table></figure>
<h2 id="三、Count-Windows"><a href="#三、Count-Windows" class="headerlink" title="三、Count Windows"></a>三、Count Windows</h2><p>Count Windows 用于以数量为维度来进行数据聚合，同样也分为滚动窗口和滑动窗口，实现方式也和时间窗口完全一致，只是调用的 API 不同，具体如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 滚动计数窗口，每1000次点击则计算一次</span></span><br><span class="line">countWindow(<span class="number">1000</span>)</span><br><span class="line"><span class="comment">// 滑动计数窗口，每10次点击发生后，则计算过去1000次点击的情况</span></span><br><span class="line">countWindow(<span class="number">1000</span>,<span class="number">10</span>)</span><br></pre></td></tr></table></figure>
<p>实际上计数窗口内部就是调用的我们上一部分介绍的全局窗口来实现的，其源码如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> WindowedStream&lt;T, KEY, GlobalWindow&gt; <span class="title">countWindow</span><span class="params">(<span class="keyword">long</span> size)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> window(GlobalWindows.create()).trigger(PurgingTrigger.of(CountTrigger.of(size)));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> WindowedStream&lt;T, KEY, GlobalWindow&gt; <span class="title">countWindow</span><span class="params">(<span class="keyword">long</span> size, <span class="keyword">long</span> slide)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> window(GlobalWindows.create())</span><br><span class="line">        .evictor(CountEvictor.of(size))</span><br><span class="line">        .trigger(CountTrigger.of(slide));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>Flink Windows： <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/stream/operators/windows.html" target="_blank" rel="noopener">https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/stream/operators/windows.html</a> </p>
]]></content>
      <categories>
        <category>notes</category>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink Data Source</title>
    <url>/2021/03/17/Flink_Data_Source/</url>
    <content><![CDATA[<h2 id="一、内置-Data-Source"><a href="#一、内置-Data-Source" class="headerlink" title="一、内置 Data Source"></a>一、内置 Data Source</h2><p>Flink Data Source 用于定义 Flink 程序的数据来源，Flink 官方提供了多种数据获取方法，用于帮助开发者简单快速地构建输入流，具体如下：</p>
<h3 id="1-1-基于文件构建"><a href="#1-1-基于文件构建" class="headerlink" title="1.1 基于文件构建"></a>1.1 基于文件构建</h3><p><strong>1. readTextFile(path)</strong>：按照 TextInputFormat 格式读取文本文件，并将其内容以字符串的形式返回。示例如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">env.readTextFile(filePath).print();</span><br></pre></td></tr></table></figure>
<p><strong>2. readFile(fileInputFormat, path)</strong> ：按照指定格式读取文件。</p>
<p><strong>3. readFile(inputFormat, filePath, watchType, interval, typeInformation)</strong>：按照指定格式周期性的读取文件。其中各个参数的含义如下：</p>
<ul>
<li><strong>inputFormat</strong>：数据流的输入格式。</li>
<li><strong>filePath</strong>：文件路径，可以是本地文件系统上的路径，也可以是 HDFS 上的文件路径。</li>
<li><strong>watchType</strong>：读取方式，它有两个可选值，分别是 <code>FileProcessingMode.PROCESS_ONCE</code> 和 <code>FileProcessingMode.PROCESS_CONTINUOUSLY</code>：前者表示对指定路径上的数据只读取一次，然后退出；后者表示对路径进行定期地扫描和读取。需要注意的是如果 watchType 被设置为 <code>PROCESS_CONTINUOUSLY</code>，那么当文件被修改时，其所有的内容 (包含原有的内容和新增的内容) 都将被重新处理，因此这会打破 Flink 的 <em>exactly-once</em> 语义。</li>
<li><strong>interval</strong>：定期扫描的时间间隔。</li>
<li><strong>typeInformation</strong>：输入流中元素的类型。</li>
</ul>
<p>使用示例如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">final</span> String filePath = <span class="string">"D:\\log4j.properties"</span>;</span><br><span class="line"><span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">env.readFile(<span class="keyword">new</span> TextInputFormat(<span class="keyword">new</span> Path(filePath)),</span><br><span class="line">             filePath,</span><br><span class="line">             FileProcessingMode.PROCESS_ONCE,</span><br><span class="line">             <span class="number">1</span>,</span><br><span class="line">             BasicTypeInfo.STRING_TYPE_INFO).print();</span><br><span class="line">env.execute();</span><br></pre></td></tr></table></figure>
<h3 id="1-2-基于集合构建"><a href="#1-2-基于集合构建" class="headerlink" title="1.2 基于集合构建"></a>1.2 基于集合构建</h3><p><strong>1. fromCollection(Collection)</strong>：基于集合构建，集合中的所有元素必须是同一类型。示例如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">env.fromCollection(Arrays.asList(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>)).print();</span><br></pre></td></tr></table></figure>
<p><strong>2. fromElements(T …)</strong>： 基于元素构建，所有元素必须是同一类型。示例如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">env.fromElements(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>).print();</span><br></pre></td></tr></table></figure>
<p><strong>3. generateSequence(from, to)</strong>：基于给定的序列区间进行构建。示例如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">env.generateSequence(<span class="number">0</span>,<span class="number">100</span>);</span><br></pre></td></tr></table></figure>
<p><strong>4. fromCollection(Iterator, Class)</strong>：基于迭代器进行构建。第一个参数用于定义迭代器，第二个参数用于定义输出元素的类型。使用示例如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">env.fromCollection(<span class="keyword">new</span> CustomIterator(), BasicTypeInfo.INT_TYPE_INFO).print();</span><br></pre></td></tr></table></figure>
<p>其中 CustomIterator 为自定义的迭代器，这里以产生 1 到 100 区间内的数据为例，源码如下。需要注意的是自定义迭代器除了要实现 Iterator 接口外，还必须要实现序列化接口 Serializable ，否则会抛出序列化失败的异常：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.Serializable;</span><br><span class="line"><span class="keyword">import</span> java.util.Iterator;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomIterator</span> <span class="keyword">implements</span> <span class="title">Iterator</span>&lt;<span class="title">Integer</span>&gt;, <span class="title">Serializable</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> Integer i = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">hasNext</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> i &lt; <span class="number">100</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Integer <span class="title">next</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        i++;</span><br><span class="line">        <span class="keyword">return</span> i;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>5. fromParallelCollection(SplittableIterator, Class)</strong>：方法接收两个参数，第二个参数用于定义输出元素的类型，第一个参数 SplittableIterator 是迭代器的抽象基类，它用于将原始迭代器的值拆分到多个不相交的迭代器中。</p>
<h3 id="1-3-基于-Socket-构建"><a href="#1-3-基于-Socket-构建" class="headerlink" title="1.3  基于 Socket 构建"></a>1.3  基于 Socket 构建</h3><p>Flink 提供了 socketTextStream 方法用于构建基于 Socket 的数据流，socketTextStream 方法有以下四个主要参数：</p>
<ul>
<li><strong>hostname</strong>：主机名；</li>
<li><strong>port</strong>：端口号，设置为 0 时，表示端口号自动分配；</li>
<li><strong>delimiter</strong>：用于分隔每条记录的分隔符；</li>
<li><strong>maxRetry</strong>：当 Socket 临时关闭时，程序的最大重试间隔，单位为秒。设置为 0 时表示不进行重试；设置为负值则表示一直重试。示例如下：</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">env.socketTextStream("192.168.0.229", 9999, "\n", 3).print();</span><br></pre></td></tr></table></figure>
<h2 id="二、自定义-Data-Source"><a href="#二、自定义-Data-Source" class="headerlink" title="二、自定义 Data Source"></a>二、自定义 Data Source</h2><h3 id="2-1-SourceFunction"><a href="#2-1-SourceFunction" class="headerlink" title="2.1 SourceFunction"></a>2.1 SourceFunction</h3><p>除了内置的数据源外，用户还可以使用 <code>addSource</code> 方法来添加自定义的数据源。自定义的数据源必须要实现 SourceFunction 接口，这里以产生 [0 , 1000) 区间内的数据为例，代码如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">env.addSource(<span class="keyword">new</span> SourceFunction&lt;Long&gt;() &#123;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">long</span> count = <span class="number">0L</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">volatile</span> <span class="keyword">boolean</span> isRunning = <span class="keyword">true</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">(SourceContext&lt;Long&gt; ctx)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">while</span> (isRunning &amp;&amp; count &lt; <span class="number">1000</span>) &#123;</span><br><span class="line">            <span class="comment">// 通过collect将输入发送出去 </span></span><br><span class="line">            ctx.collect(count);</span><br><span class="line">            count++;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">cancel</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        isRunning = <span class="keyword">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;).print();</span><br><span class="line">env.execute();</span><br></pre></td></tr></table></figure>
<h3 id="2-2-ParallelSourceFunction-和-RichParallelSourceFunction"><a href="#2-2-ParallelSourceFunction-和-RichParallelSourceFunction" class="headerlink" title="2.2 ParallelSourceFunction 和 RichParallelSourceFunction"></a>2.2 ParallelSourceFunction 和 RichParallelSourceFunction</h3><p>上面通过 SourceFunction 实现的数据源是不具有并行度的，即不支持在得到的 DataStream 上调用 <code>setParallelism(n)</code> 方法，此时会抛出如下的异常：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">Exception in thread "main" java.lang.IllegalArgumentException: Source: 1 is not a parallel source</span><br></pre></td></tr></table></figure>
<p>如果你想要实现具有并行度的输入流，则需要实现 ParallelSourceFunction 或 RichParallelSourceFunction 接口，其与 SourceFunction 的关系如下图： </p>
<p><div align="center"> <img src="../pictures/flink-RichParallelSourceFunction.png"> </div><br>ParallelSourceFunction 直接继承自 ParallelSourceFunction，具有并行度的功能。RichParallelSourceFunction 则继承自 AbstractRichFunction，同时实现了 ParallelSourceFunction 接口，所以其除了具有并行度的功能外，还提供了额外的与生命周期相关的方法，如 open() ，closen() 。</p>
<h2 id="三、Streaming-Connectors"><a href="#三、Streaming-Connectors" class="headerlink" title="三、Streaming Connectors"></a>三、Streaming Connectors</h2><h3 id="3-1-内置连接器"><a href="#3-1-内置连接器" class="headerlink" title="3.1 内置连接器"></a>3.1 内置连接器</h3><p>除了自定义数据源外， Flink 还内置了多种连接器，用于满足大多数的数据收集场景。当前内置连接器的支持情况如下：</p>
<ul>
<li>Apache Kafka (支持 source 和 sink)</li>
<li>Apache Cassandra (sink)</li>
<li>Amazon Kinesis Streams (source/sink)</li>
<li>Elasticsearch (sink)</li>
<li>Hadoop FileSystem (sink)</li>
<li>RabbitMQ (source/sink)</li>
<li>Apache NiFi (source/sink)</li>
<li>Twitter Streaming API (source)</li>
<li>Google PubSub (source/sink)</li>
</ul>
<p>除了上述的连接器外，你还可以通过 Apache Bahir 的连接器扩展 Flink。Apache Bahir 旨在为分布式数据分析系统 (如 Spark，Flink) 等提供功能上的扩展，当前其支持的与 Flink 相关的连接器如下：</p>
<ul>
<li>Apache ActiveMQ (source/sink)</li>
<li>Apache Flume (sink)</li>
<li>Redis (sink)</li>
<li>Akka (sink)</li>
<li>Netty (source)</li>
</ul>
<p>随着 Flink 的不断发展，可以预见到其会支持越来越多类型的连接器，关于连接器的后续发展情况，可以查看其官方文档：<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/connectors/index.html" target="_blank" rel="noopener">Streaming Connectors</a> 。在所有 DataSource 连接器中，使用的广泛的就是 Kafka，所以这里我们以其为例，来介绍 Connectors 的整合步骤。</p>
<h3 id="3-2-整合-Kakfa"><a href="#3-2-整合-Kakfa" class="headerlink" title="3.2 整合 Kakfa"></a>3.2 整合 Kakfa</h3><h4 id="1-导入依赖"><a href="#1-导入依赖" class="headerlink" title="1. 导入依赖"></a>1. 导入依赖</h4><p>整合 Kafka 时，一定要注意所使用的 Kafka 的版本，不同版本间所需的 Maven 依赖和开发时所调用的类均不相同，具体如下：</p>
<table>
<thead>
<tr>
<th style="text-align:left">Maven 依赖</th>
<th style="text-align:left">Flink 版本</th>
<th style="text-align:left">Consumer and Producer 类的名称</th>
<th style="text-align:left">Kafka 版本</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">flink-connector-kafka-0.8_2.11</td>
<td style="text-align:left">1.0.0 +</td>
<td style="text-align:left">FlinkKafkaConsumer08 <br>FlinkKafkaProducer08</td>
<td style="text-align:left">0.8.x</td>
</tr>
<tr>
<td style="text-align:left">flink-connector-kafka-0.9_2.11</td>
<td style="text-align:left">1.0.0 +</td>
<td style="text-align:left">FlinkKafkaConsumer09<br> FlinkKafkaProducer09</td>
<td style="text-align:left">0.9.x</td>
</tr>
<tr>
<td style="text-align:left">flink-connector-kafka-0.10_2.11</td>
<td style="text-align:left">1.2.0 +</td>
<td style="text-align:left">FlinkKafkaConsumer010 <br>FlinkKafkaProducer010</td>
<td style="text-align:left">0.10.x</td>
</tr>
<tr>
<td style="text-align:left">flink-connector-kafka-0.11_2.11</td>
<td style="text-align:left">1.4.0 +</td>
<td style="text-align:left">FlinkKafkaConsumer011 <br>FlinkKafkaProducer011</td>
<td style="text-align:left">0.11.x</td>
</tr>
<tr>
<td style="text-align:left">flink-connector-kafka_2.11</td>
<td style="text-align:left">1.7.0 +</td>
<td style="text-align:left">FlinkKafkaConsumer <br>FlinkKafkaProducer</td>
<td style="text-align:left">&gt;= 1.0.0</td>
</tr>
</tbody>
</table>
<p>这里我使用的 Kafka 版本为 kafka_2.12-2.2.0，添加的依赖如下：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-kafka_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.9.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h4 id="2-代码开发"><a href="#2-代码开发" class="headerlink" title="2. 代码开发"></a>2. 代码开发</h4><p>这里以最简单的场景为例，接收 Kafka 上的数据并打印，代码如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">Properties properties = <span class="keyword">new</span> Properties();</span><br><span class="line"><span class="comment">// 指定Kafka的连接位置</span></span><br><span class="line">properties.setProperty(<span class="string">"bootstrap.servers"</span>, <span class="string">"hadoop001:9092"</span>);</span><br><span class="line"><span class="comment">// 指定监听的主题，并定义Kafka字节消息到Flink对象之间的转换规则</span></span><br><span class="line">DataStream&lt;String&gt; stream = env</span><br><span class="line">    .addSource(<span class="keyword">new</span> FlinkKafkaConsumer&lt;&gt;(<span class="string">"flink-stream-in-topic"</span>, <span class="keyword">new</span> SimpleStringSchema(), properties));</span><br><span class="line">stream.print();</span><br><span class="line">env.execute(<span class="string">"Flink Streaming"</span>);</span><br></pre></td></tr></table></figure>
<h3 id="3-3-整合测试"><a href="#3-3-整合测试" class="headerlink" title="3.3 整合测试"></a>3.3 整合测试</h3><h4 id="1-启动-Kakfa"><a href="#1-启动-Kakfa" class="headerlink" title="1. 启动 Kakfa"></a>1. 启动 Kakfa</h4><p>Kafka 的运行依赖于 zookeeper，需要预先启动，可以启动 Kafka 内置的 zookeeper，也可以启动自己安装的：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span> zookeeper启动命令</span><br><span class="line">bin/zkServer.sh start</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 内置zookeeper启动命令</span><br><span class="line">bin/zookeeper-server-start.sh config/zookeeper.properties</span><br></pre></td></tr></table></figure>
<p>启动单节点 kafka 用于测试：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span> bin/kafka-server-start.sh config/server.properties</span><br></pre></td></tr></table></figure>
<h4 id="2-创建-Topic"><a href="#2-创建-Topic" class="headerlink" title="2. 创建 Topic"></a>2. 创建 Topic</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span> 创建用于测试主题</span><br><span class="line">bin/kafka-topics.sh --create \</span><br><span class="line">                    --bootstrap-server hadoop001:9092 \</span><br><span class="line">                    --replication-factor 1 \</span><br><span class="line">                    --partitions 1  \</span><br><span class="line">                    --topic flink-stream-in-topic</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 查看所有主题</span><br><span class="line"> bin/kafka-topics.sh --list --bootstrap-server hadoop001:9092</span><br></pre></td></tr></table></figure>
<h4 id="3-启动-Producer"><a href="#3-启动-Producer" class="headerlink" title="3. 启动 Producer"></a>3. 启动 Producer</h4><p>这里 启动一个 Kafka 生产者，用于发送测试数据：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">bin/kafka-console-producer.sh --broker-list hadoop001:9092 --topic flink-stream-in-topic</span><br></pre></td></tr></table></figure>
<h4 id="4-测试结果"><a href="#4-测试结果" class="headerlink" title="4. 测试结果"></a>4. 测试结果</h4><p>在 Producer 上输入任意测试数据，之后观察程序控制台的输出：</p>
<p><div align="center"> <img src="../pictures/flink-kafka-datasource-producer.png"> </div><br>程序控制台的输出如下：</p>
<p><div align="center"> <img src="../pictures/flink-kafka-datasource-console.png"> </div><br>可以看到已经成功接收并打印出相关的数据。</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol>
<li>data-sources：<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/datastream_api.html#data-sources" target="_blank" rel="noopener">https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/datastream_api.html#data-sources</a> </li>
<li>Streaming Connectors：<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/connectors/index.html" target="_blank" rel="noopener">https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/connectors/index.html</a></li>
<li>Apache Kafka Connector： <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/connectors/kafka.html" target="_blank" rel="noopener">https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/connectors/kafka.html</a> </li>
</ol>
]]></content>
      <categories>
        <category>notes</category>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink Transformation</title>
    <url>/2021/03/17/Flink_Data_Transformation/</url>
    <content><![CDATA[<h2 id="一、Transformations-分类"><a href="#一、Transformations-分类" class="headerlink" title="一、Transformations 分类"></a>一、Transformations 分类</h2><p>Flink 的 Transformations 操作主要用于将一个和多个 DataStream 按需转换成新的 DataStream。它主要分为以下三类：</p>
<ul>
<li><strong>DataStream Transformations</strong>：进行数据流相关转换操作；</li>
<li><strong>Physical partitioning</strong>：物理分区。Flink 提供的底层 API ，允许用户定义数据的分区规则；</li>
<li><strong>Task chaining and resource groups</strong>：任务链和资源组。允许用户进行任务链和资源组的细粒度的控制。</li>
</ul>
<p>以下分别对其主要 API 进行介绍：</p>
<h2 id="二、DataStream-Transformations"><a href="#二、DataStream-Transformations" class="headerlink" title="二、DataStream Transformations"></a>二、DataStream Transformations</h2><h3 id="2-1-Map-DataStream-→-DataStream"><a href="#2-1-Map-DataStream-→-DataStream" class="headerlink" title="2.1 Map [DataStream → DataStream]"></a>2.1 Map [DataStream → DataStream]</h3><p>对一个 DataStream 中的每个元素都执行特定的转换操作：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataStream&lt;Integer&gt; integerDataStream = env.fromElements(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>);</span><br><span class="line">integerDataStream.map((MapFunction&lt;Integer, Object&gt;) value -&gt; value * <span class="number">2</span>).print();</span><br><span class="line"><span class="comment">// 输出 2,4,6,8,10</span></span><br></pre></td></tr></table></figure>
<h3 id="2-2-FlatMap-DataStream-→-DataStream"><a href="#2-2-FlatMap-DataStream-→-DataStream" class="headerlink" title="2.2 FlatMap [DataStream → DataStream]"></a>2.2 FlatMap [DataStream → DataStream]</h3><p>FlatMap 与 Map 类似，但是 FlatMap 中的一个输入元素可以被映射成一个或者多个输出元素，示例如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">String string01 = <span class="string">"one one one two two"</span>;</span><br><span class="line">String string02 = <span class="string">"third third third four"</span>;</span><br><span class="line">DataStream&lt;String&gt; stringDataStream = env.fromElements(string01, string02);</span><br><span class="line">stringDataStream.flatMap(<span class="keyword">new</span> FlatMapFunction&lt;String, String&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(String value, Collector&lt;String&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">for</span> (String s : value.split(<span class="string">" "</span>)) &#123;</span><br><span class="line">            out.collect(s);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;).print();</span><br><span class="line"><span class="comment">// 输出每一个独立的单词，为节省排版，这里去掉换行，后文亦同</span></span><br><span class="line">one one one two two third third third four</span><br></pre></td></tr></table></figure>
<h3 id="2-3-Filter-DataStream-→-DataStream"><a href="#2-3-Filter-DataStream-→-DataStream" class="headerlink" title="2.3 Filter [DataStream → DataStream]"></a>2.3 Filter [DataStream → DataStream]</h3><p>用于过滤符合条件的数据：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">env.fromElements(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>).filter(x -&gt; x &gt; <span class="number">3</span>).print();</span><br></pre></td></tr></table></figure>
<h3 id="2-4-KeyBy-和-Reduce"><a href="#2-4-KeyBy-和-Reduce" class="headerlink" title="2.4 KeyBy 和 Reduce"></a>2.4 KeyBy 和 Reduce</h3><ul>
<li><strong>KeyBy [DataStream → KeyedStream]</strong> ：用于将相同 Key 值的数据分到相同的分区中；</li>
<li><strong>Reduce [KeyedStream → DataStream]</strong> ：用于对数据执行归约计算。</li>
</ul>
<p>如下例子将数据按照 key 值分区后，滚动进行求和计算：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; tuple2DataStream = env.fromElements(<span class="keyword">new</span> Tuple2&lt;&gt;(<span class="string">"a"</span>, <span class="number">1</span>),</span><br><span class="line">                                                                        <span class="keyword">new</span> Tuple2&lt;&gt;(<span class="string">"a"</span>, <span class="number">2</span>), </span><br><span class="line">                                                                        <span class="keyword">new</span> Tuple2&lt;&gt;(<span class="string">"b"</span>, <span class="number">3</span>), </span><br><span class="line">                                                                        <span class="keyword">new</span> Tuple2&lt;&gt;(<span class="string">"b"</span>, <span class="number">5</span>));</span><br><span class="line">KeyedStream&lt;Tuple2&lt;String, Integer&gt;, Tuple&gt; keyedStream = tuple2DataStream.keyBy(<span class="number">0</span>);</span><br><span class="line">keyedStream.reduce((ReduceFunction&lt;Tuple2&lt;String, Integer&gt;&gt;) (value1, value2) -&gt;</span><br><span class="line">                   <span class="keyword">new</span> Tuple2&lt;&gt;(value1.f0, value1.f1 + value2.f1)).print();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 持续进行求和计算，输出：</span></span><br><span class="line">(a,<span class="number">1</span>)</span><br><span class="line">(a,<span class="number">3</span>)</span><br><span class="line">(b,<span class="number">3</span>)</span><br><span class="line">(b,<span class="number">8</span>)</span><br></pre></td></tr></table></figure>
<p>KeyBy 操作存在以下两个限制：</p>
<ul>
<li>KeyBy 操作用于用户自定义的 POJOs 类型时，该自定义类型必须重写 hashCode 方法；</li>
<li>KeyBy 操作不能用于数组类型。</li>
</ul>
<h3 id="2-5-Aggregations-KeyedStream-→-DataStream"><a href="#2-5-Aggregations-KeyedStream-→-DataStream" class="headerlink" title="2.5 Aggregations [KeyedStream → DataStream]"></a>2.5 Aggregations [KeyedStream → DataStream]</h3><p>Aggregations 是官方提供的聚合算子，封装了常用的聚合操作，如上利用 Reduce 进行求和的操作也可以利用 Aggregations 中的 sum 算子重写为下面的形式：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">tuple2DataStream.keyBy(<span class="number">0</span>).sum(<span class="number">1</span>).print();</span><br></pre></td></tr></table></figure>
<p>除了 sum 外，Flink 还提供了 min , max , minBy，maxBy 等常用聚合算子：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 滚动计算指定key的最小值，可以通过index或者fieldName来指定key</span></span><br><span class="line">keyedStream.min(<span class="number">0</span>);</span><br><span class="line">keyedStream.min(<span class="string">"key"</span>);</span><br><span class="line"><span class="comment">// 滚动计算指定key的最大值</span></span><br><span class="line">keyedStream.max(<span class="number">0</span>);</span><br><span class="line">keyedStream.max(<span class="string">"key"</span>);</span><br><span class="line"><span class="comment">// 滚动计算指定key的最小值，并返回其对应的元素</span></span><br><span class="line">keyedStream.minBy(<span class="number">0</span>);</span><br><span class="line">keyedStream.minBy(<span class="string">"key"</span>);</span><br><span class="line"><span class="comment">// 滚动计算指定key的最大值，并返回其对应的元素</span></span><br><span class="line">keyedStream.maxBy(<span class="number">0</span>);</span><br><span class="line">keyedStream.maxBy(<span class="string">"key"</span>);</span><br></pre></td></tr></table></figure>
<h3 id="2-6-Union-DataStream-→-DataStream"><a href="#2-6-Union-DataStream-→-DataStream" class="headerlink" title="2.6 Union [DataStream* → DataStream]"></a>2.6 Union [DataStream* → DataStream]</h3><p>用于连接两个或者多个元素类型相同的 DataStream 。当然一个 DataStream 也可以与其本生进行连接，此时该 DataStream 中的每个元素都会被获取两次：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">DataStreamSource&lt;Tuple2&lt;String, Integer&gt;&gt; streamSource01 = env.fromElements(new Tuple2&lt;&gt;("a", 1), </span><br><span class="line">                                                                            new Tuple2&lt;&gt;("a", 2));</span><br><span class="line">DataStreamSource&lt;Tuple2&lt;String, Integer&gt;&gt; streamSource02 = env.fromElements(new Tuple2&lt;&gt;("b", 1), </span><br><span class="line">                                                                            new Tuple2&lt;&gt;("b", 2));</span><br><span class="line">streamSource01.union(streamSource02);</span><br><span class="line">streamSource01.union(streamSource01,streamSource02);</span><br></pre></td></tr></table></figure>
<h3 id="2-7-Connect-DataStream-DataStream-→-ConnectedStreams"><a href="#2-7-Connect-DataStream-DataStream-→-ConnectedStreams" class="headerlink" title="2.7 Connect [DataStream,DataStream → ConnectedStreams]"></a>2.7 Connect [DataStream,DataStream → ConnectedStreams]</h3><p>Connect 操作用于连接两个或者多个类型不同的 DataStream ，其返回的类型是 ConnectedStreams ，此时被连接的多个 DataStreams 可以共享彼此之间的数据状态。但是需要注意的是由于不同 DataStream 之间的数据类型是不同的，如果想要进行后续的计算操作，还需要通过 CoMap 或 CoFlatMap 将 ConnectedStreams  转换回 DataStream：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataStreamSource&lt;Tuple2&lt;String, Integer&gt;&gt; streamSource01 = env.fromElements(<span class="keyword">new</span> Tuple2&lt;&gt;(<span class="string">"a"</span>, <span class="number">3</span>), </span><br><span class="line">                                                                            <span class="keyword">new</span> Tuple2&lt;&gt;(<span class="string">"b"</span>, <span class="number">5</span>));</span><br><span class="line">DataStreamSource&lt;Integer&gt; streamSource02 = env.fromElements(<span class="number">2</span>, <span class="number">3</span>, <span class="number">9</span>);</span><br><span class="line"><span class="comment">// 使用connect进行连接</span></span><br><span class="line">ConnectedStreams&lt;Tuple2&lt;String, Integer&gt;, Integer&gt; connect = streamSource01.connect(streamSource02);</span><br><span class="line">connect.map(<span class="keyword">new</span> CoMapFunction&lt;Tuple2&lt;String, Integer&gt;, Integer, Integer&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Integer <span class="title">map1</span><span class="params">(Tuple2&lt;String, Integer&gt; value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> value.f1;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Integer <span class="title">map2</span><span class="params">(Integer value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> value;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;).map(x -&gt; x * <span class="number">100</span>).print();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出：</span></span><br><span class="line"><span class="number">300</span> <span class="number">500</span> <span class="number">200</span> <span class="number">900</span> <span class="number">300</span></span><br></pre></td></tr></table></figure>
<h3 id="2-8-Split-和-Select"><a href="#2-8-Split-和-Select" class="headerlink" title="2.8 Split 和 Select"></a>2.8 Split 和 Select</h3><ul>
<li><strong>Split [DataStream → SplitStream]</strong>：用于将一个 DataStream 按照指定规则进行拆分为多个 DataStream，需要注意的是这里进行的是逻辑拆分，即 Split 只是将数据贴上不同的类型标签，但最终返回的仍然只是一个 SplitStream；</li>
<li><strong>Select [SplitStream → DataStream]</strong>：想要从逻辑拆分的 SplitStream 中获取真实的不同类型的 DataStream，需要使用 Select 算子，示例如下：</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataStreamSource&lt;Integer&gt; streamSource = env.fromElements(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>);</span><br><span class="line"><span class="comment">// 标记</span></span><br><span class="line">SplitStream&lt;Integer&gt; split = streamSource.split(<span class="keyword">new</span> OutputSelector&lt;Integer&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Iterable&lt;String&gt; <span class="title">select</span><span class="params">(Integer value)</span> </span>&#123;</span><br><span class="line">        List&lt;String&gt; output = <span class="keyword">new</span> ArrayList&lt;String&gt;();</span><br><span class="line">        output.add(value % <span class="number">2</span> == <span class="number">0</span> ? <span class="string">"even"</span> : <span class="string">"odd"</span>);</span><br><span class="line">        <span class="keyword">return</span> output;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br><span class="line"><span class="comment">// 获取偶数数据集</span></span><br><span class="line">split.select(<span class="string">"even"</span>).print();</span><br><span class="line"><span class="comment">// 输出 2,4,6,8</span></span><br></pre></td></tr></table></figure>
<h3 id="2-9-project-DataStream-→-DataStream"><a href="#2-9-project-DataStream-→-DataStream" class="headerlink" title="2.9 project [DataStream → DataStream]"></a>2.9 project [DataStream → DataStream]</h3><p>project 主要用于获取 tuples 中的指定字段集，示例如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataStreamSource&lt;Tuple3&lt;String, Integer, String&gt;&gt; streamSource = env.fromElements(</span><br><span class="line">                                                                         <span class="keyword">new</span> Tuple3&lt;&gt;(<span class="string">"li"</span>, <span class="number">22</span>, <span class="string">"2018-09-23"</span>),</span><br><span class="line">                                                                         <span class="keyword">new</span> Tuple3&lt;&gt;(<span class="string">"ming"</span>, <span class="number">33</span>, <span class="string">"2020-09-23"</span>));</span><br><span class="line">streamSource.project(<span class="number">0</span>,<span class="number">2</span>).print();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出</span></span><br><span class="line">(li,<span class="number">2018</span>-<span class="number">09</span>-<span class="number">23</span>)</span><br><span class="line">(ming,<span class="number">2020</span>-<span class="number">09</span>-<span class="number">23</span>)</span><br></pre></td></tr></table></figure>
<h2 id="三、物理分区"><a href="#三、物理分区" class="headerlink" title="三、物理分区"></a>三、物理分区</h2><p>物理分区 (Physical partitioning) 是 Flink 提供的底层的 API，允许用户采用内置的分区规则或者自定义的分区规则来对数据进行分区，从而避免数据在某些分区上过于倾斜，常用的分区规则如下：</p>
<h3 id="3-1-Random-partitioning-DataStream-→-DataStream"><a href="#3-1-Random-partitioning-DataStream-→-DataStream" class="headerlink" title="3.1 Random partitioning [DataStream → DataStream]"></a>3.1 Random partitioning [DataStream → DataStream]</h3><p>随机分区 (Random partitioning) 用于随机的将数据分布到所有下游分区中，通过 shuffle 方法来进行实现：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">dataStream.shuffle();</span><br></pre></td></tr></table></figure>
<h3 id="3-2-Rebalancing-DataStream-→-DataStream"><a href="#3-2-Rebalancing-DataStream-→-DataStream" class="headerlink" title="3.2 Rebalancing [DataStream → DataStream]"></a>3.2 Rebalancing [DataStream → DataStream]</h3><p>Rebalancing 采用轮询的方式将数据进行分区，其适合于存在数据倾斜的场景下，通过 rebalance 方法进行实现：  </p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">dataStream.rebalance();</span><br></pre></td></tr></table></figure>
<h3 id="3-3-Rescaling-DataStream-→-DataStream"><a href="#3-3-Rescaling-DataStream-→-DataStream" class="headerlink" title="3.3 Rescaling [DataStream → DataStream]"></a>3.3 Rescaling [DataStream → DataStream]</h3><p>当采用 Rebalancing 进行分区平衡时，其实现的是全局性的负载均衡，数据会通过网络传输到其他节点上并完成分区数据的均衡。 而 Rescaling 则是低配版本的 rebalance，它不需要额外的网络开销，它只会对上下游的算子之间进行重新均衡，通过 rescale 方法进行实现：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">dataStream.rescale();</span><br></pre></td></tr></table></figure>
<p>ReScale 这个单词具有重新缩放的意义，其对应的操作也是如此，具体如下：如果上游 operation 并行度为 2，而下游的 operation 并行度为 6，则其中 1 个上游的 operation 会将元素分发到 3 个下游 operation，另 1 个上游 operation 则会将元素分发到另外 3 个下游 operation。反之亦然，如果上游的 operation 并行度为 6，而下游 operation 并行度为 2，则其中 3 个上游 operation 会将元素分发到 1 个下游 operation，另 3 个上游 operation 会将元素分发到另外 1 个下游operation：</p>
<div align="center"> <img src="../pictures/flink-Rescaling.png"> </div>


<h3 id="3-4-Broadcasting-DataStream-→-DataStream"><a href="#3-4-Broadcasting-DataStream-→-DataStream" class="headerlink" title="3.4 Broadcasting [DataStream → DataStream]"></a>3.4 Broadcasting [DataStream → DataStream]</h3><p>将数据分发到所有分区上。通常用于小数据集与大数据集进行关联的情况下，此时可以将小数据集广播到所有分区上，避免频繁的跨分区关联，通过 broadcast 方法进行实现：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">dataStream.broadcast();</span><br></pre></td></tr></table></figure>
<h3 id="3-5-Custom-partitioning-DataStream-→-DataStream"><a href="#3-5-Custom-partitioning-DataStream-→-DataStream" class="headerlink" title="3.5 Custom partitioning [DataStream → DataStream]"></a>3.5 Custom partitioning [DataStream → DataStream]</h3><p>Flink 运行用户采用自定义的分区规则来实现分区，此时需要通过实现 Partitioner 接口来自定义分区规则，并指定对应的分区键，示例如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"> DataStreamSource&lt;Tuple2&lt;String, Integer&gt;&gt; streamSource = env.fromElements(<span class="keyword">new</span> Tuple2&lt;&gt;(<span class="string">"Hadoop"</span>, <span class="number">1</span>),</span><br><span class="line">                <span class="keyword">new</span> Tuple2&lt;&gt;(<span class="string">"Spark"</span>, <span class="number">1</span>),</span><br><span class="line">                <span class="keyword">new</span> Tuple2&lt;&gt;(<span class="string">"Flink-streaming"</span>, <span class="number">2</span>),</span><br><span class="line">                <span class="keyword">new</span> Tuple2&lt;&gt;(<span class="string">"Flink-batch"</span>, <span class="number">4</span>),</span><br><span class="line">                <span class="keyword">new</span> Tuple2&lt;&gt;(<span class="string">"Storm"</span>, <span class="number">4</span>),</span><br><span class="line">                <span class="keyword">new</span> Tuple2&lt;&gt;(<span class="string">"HBase"</span>, <span class="number">3</span>));</span><br><span class="line">streamSource.partitionCustom(<span class="keyword">new</span> Partitioner&lt;String&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">partition</span><span class="params">(String key, <span class="keyword">int</span> numPartitions)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 将第一个字段包含flink的Tuple2分配到同一个分区</span></span><br><span class="line">        <span class="keyword">return</span> key.toLowerCase().contains(<span class="string">"flink"</span>) ? <span class="number">0</span> : <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;, <span class="number">0</span>).print();</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出如下：</span></span><br><span class="line"><span class="number">1</span>&gt; (Flink-streaming,<span class="number">2</span>)</span><br><span class="line"><span class="number">1</span>&gt; (Flink-batch,<span class="number">4</span>)</span><br><span class="line"><span class="number">2</span>&gt; (Hadoop,<span class="number">1</span>)</span><br><span class="line"><span class="number">2</span>&gt; (Spark,<span class="number">1</span>)</span><br><span class="line"><span class="number">2</span>&gt; (Storm,<span class="number">4</span>)</span><br><span class="line"><span class="number">2</span>&gt; (HBase,<span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<h2 id="四、任务链和资源组"><a href="#四、任务链和资源组" class="headerlink" title="四、任务链和资源组"></a>四、任务链和资源组</h2><p>任务链和资源组 ( Task chaining and resource groups ) 也是 Flink 提供的底层 API，用于控制任务链和资源分配。默认情况下，如果操作允许 (例如相邻的两次 map 操作) ，则 Flink 会尝试将它们在同一个线程内进行，从而可以获取更好的性能。但是 Flink 也允许用户自己来控制这些行为，这就是任务链和资源组 API：</p>
<h3 id="4-1-startNewChain"><a href="#4-1-startNewChain" class="headerlink" title="4.1 startNewChain"></a>4.1 startNewChain</h3><p>startNewChain 用于基于当前 operation 开启一个新的任务链。如下所示，基于第一个 map 开启一个新的任务链，此时前一个 map 和 后一个 map 将处于同一个新的任务链中，但它们与 filter 操作则分别处于不同的任务链中：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">someStream.filter(...).map(...).startNewChain().map(...);</span><br></pre></td></tr></table></figure>
<h3 id="4-2-disableChaining"><a href="#4-2-disableChaining" class="headerlink" title="4.2 disableChaining"></a>4.2 disableChaining</h3><p> disableChaining 操作用于禁止将其他操作与当前操作放置于同一个任务链中，示例如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">someStream.map(...).disableChaining();</span><br></pre></td></tr></table></figure>
<h3 id="4-3-slotSharingGroup"><a href="#4-3-slotSharingGroup" class="headerlink" title="4.3 slotSharingGroup"></a>4.3 slotSharingGroup</h3><p>slot 是任务管理器  (TaskManager) 所拥有资源的固定子集，每个操作 (operation) 的子任务 (sub task) 都需要获取 slot 来执行计算，但每个操作所需要资源的大小都是不相同的，为了更好地利用资源，Flink 允许不同操作的子任务被部署到同一 slot 中。slotSharingGroup 用于设置操作的 slot 共享组 (slot sharing group) ，Flink 会将具有相同 slot 共享组的操作放到同一个 slot 中 。示例如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">someStream.filter(...).slotSharingGroup(<span class="string">"slotSharingGroupName"</span>);</span><br></pre></td></tr></table></figure>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>Flink Operators： <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/stream/operators/" target="_blank" rel="noopener">https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/stream/operators/</a> </p>
]]></content>
      <categories>
        <category>notes</category>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink 开发环境搭建</title>
    <url>/2021/03/17/Flink%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/</url>
    <content><![CDATA[<h2 id="一、安装-Scala-插件"><a href="#一、安装-Scala-插件" class="headerlink" title="一、安装 Scala 插件"></a>一、安装 Scala 插件</h2><p>Flink 分别提供了基于 Java 语言和 Scala 语言的 API ，如果想要使用 Scala 语言来开发 Flink 程序，可以通过在 IDEA 中安装 Scala 插件来提供语法提示，代码高亮等功能。打开 IDEA , 依次点击 <code>File =&gt; settings =&gt; plugins</code> 打开插件安装页面，搜索 Scala 插件并进行安装，安装完成后，重启 IDEA 即可生效。  </p>
<div align="center"> <img src="../pictures/scala-plugin.png"> </div>

<h2 id="二、Flink-项目初始化"><a href="#二、Flink-项目初始化" class="headerlink" title="二、Flink 项目初始化"></a>二、Flink 项目初始化</h2><h3 id="2-1-使用官方脚本构建"><a href="#2-1-使用官方脚本构建" class="headerlink" title="2.1 使用官方脚本构建"></a>2.1 使用官方脚本构建</h3><p>Flink 官方支持使用 Maven 和 Gradle 两种构建工具来构建基于 Java 语言的 Flink 项目；支持使用 SBT 和 Maven 两种构建工具来构建基于 Scala 语言的 Flink 项目。 这里以 Maven 为例进行说明，因为其可以同时支持 Java 语言和 Scala 语言项目的构建。需要注意的是 Flink 1.9 只支持 Maven 3.0.4 以上的版本，Maven 安装完成后，可以通过以下两种方式来构建项目：</p>
<p><strong>1. 直接基于 Maven Archetype 构建</strong></p>
<p>直接使用下面的 mvn 语句来进行构建，然后根据交互信息的提示，依次输入 groupId , artifactId 以及包名等信息后等待初始化的完成： </p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ mvn archetype:generate                               \</span><br><span class="line">      -DarchetypeGroupId=org.apache.flink              \</span><br><span class="line">      -DarchetypeArtifactId=flink-quickstart-java      \</span><br><span class="line">      -DarchetypeVersion=1.9.0</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注：如果想要创建基于 Scala 语言的项目，只需要将 flink-quickstart-java 换成 flink-quickstart-scala 即可，后文亦同。</p>
</blockquote>
<p><strong>2. 使用官方脚本快速构建</strong></p>
<p>为了更方便的初始化项目，官方提供了快速构建脚本，可以直接通过以下命令来进行调用：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> curl https://flink.apache.org/q/quickstart.sh | bash -s 1.9.0</span></span><br></pre></td></tr></table></figure>
<p>该方式其实也是通过执行 maven archetype 命令来进行初始化，其脚本内容如下：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">PACKAGE=quickstart</span><br><span class="line"></span><br><span class="line">mvn archetype:generate \</span><br><span class="line">  -DarchetypeGroupId=org.apache.flink \</span><br><span class="line">  -DarchetypeArtifactId=flink-quickstart-java \</span><br><span class="line">  -DarchetypeVersion=$&#123;1:-1.8.0&#125; \</span><br><span class="line">  -DgroupId=org.myorg.quickstart \</span><br><span class="line">  -DartifactId=$PACKAGE	\</span><br><span class="line">  -Dversion=0.1 \</span><br><span class="line">  -Dpackage=org.myorg.quickstart \</span><br><span class="line">  -DinteractiveMode=false</span><br></pre></td></tr></table></figure>
<p>可以看到相比于第一种方式，该种方式只是直接指定好了 groupId ，artifactId ，version 等信息而已。</p>
<h3 id="2-2-使用-IDEA-构建"><a href="#2-2-使用-IDEA-构建" class="headerlink" title="2.2 使用 IDEA 构建"></a>2.2 使用 IDEA 构建</h3><p>如果你使用的是开发工具是 IDEA ，可以直接在项目创建页面选择 Maven Flink Archetype 进行项目初始化：</p>
<div align="center"> <img src="../pictures/flink-maven.png"> </div>

<p>如果你的 IDEA 没有上述 Archetype， 可以通过点击右上角的 <code>ADD ARCHETYPE</code> ，来进行添加，依次填入所需信息，这些信息都可以从上述的 <code>archetype:generate</code> 语句中获取。点击  <code>OK</code> 保存后，该 Archetype 就会一直存在于你的 IDEA 中，之后每次创建项目时，只需要直接选择该 Archetype 即可：</p>
<div align="center"> <img src="../pictures/flink-maven-new.png"> </div>

<p>选中 Flink Archetype ，然后点击 <code>NEXT</code> 按钮，之后的所有步骤都和正常的 Maven 工程相同。</p>
<h2 id="三、项目结构"><a href="#三、项目结构" class="headerlink" title="三、项目结构"></a>三、项目结构</h2><h3 id="3-1-项目结构"><a href="#3-1-项目结构" class="headerlink" title="3.1 项目结构"></a>3.1 项目结构</h3><p>创建完成后的自动生成的项目结构如下：</p>
<div align="center"> <img src="../pictures/flink-basis-project.png"> </div>

<p>其中 BatchJob 为批处理的样例代码，源码如下：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">BatchJob</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">ExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">      ....</span><br><span class="line">    env.execute(<span class="string">"Flink Batch Scala API Skeleton"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>getExecutionEnvironment 代表获取批处理的执行环境，如果是本地运行则获取到的就是本地的执行环境；如果在集群上运行，得到的就是集群的执行环境。如果想要获取流处理的执行环境，则只需要将 <code>ExecutionEnvironment</code> 替换为 <code>StreamExecutionEnvironment</code>， 对应的代码样例在 StreamingJob 中：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StreamingJob</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">      ...</span><br><span class="line">    env.execute(<span class="string">"Flink Streaming Scala API Skeleton"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>需要注意的是对于流处理项目 <code>env.execute()</code> 这句代码是必须的，否则流处理程序就不会被执行，但是对于批处理项目则是可选的。</p>
<h3 id="3-2-主要依赖"><a href="#3-2-主要依赖" class="headerlink" title="3.2 主要依赖"></a>3.2 主要依赖</h3><p>基于 Maven 骨架创建的项目主要提供了以下核心依赖：其中 <code>flink-scala</code> 用于支持开发批处理程序 ；<code>flink-streaming-scala</code> 用于支持开发流处理程序 ；<code>scala-library</code> 用于提供 Scala 语言所需要的类库。如果在使用 Maven 骨架创建时选择的是 Java 语言，则默认提供的则是 <code>flink-java</code> 和 <code>flink-streaming-java</code> 依赖。</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- Apache Flink dependencies --&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- These dependencies are provided, because they should not be packaged into the JAR file. --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-scala_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">scope</span>&gt;</span>provided<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-streaming-scala_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">scope</span>&gt;</span>provided<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- Scala Library, provided by Flink as well. --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.scala-lang<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>scala-library<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;scala.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">scope</span>&gt;</span>provided<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>需要特别注意的以上依赖的 <code>scope</code> 标签全部被标识为 provided ，这意味着这些依赖都不会被打入最终的 JAR 包。因为 Flink 的安装包中已经提供了这些依赖，位于其 lib 目录下，名为  <code>flink-dist_*.jar</code>  ，它包含了 Flink 的所有核心类和依赖：</p>
<div align="center"> <img src="../pictures/flink-lib.png"> </div>

<p> <code>scope</code> 标签被标识为 provided 会导致你在 IDEA 中启动项目时会抛出 ClassNotFoundException 异常。基于这个原因，在使用 IDEA 创建项目时还自动生成了以下 profile 配置：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- This profile helps to make things run out of the box in IntelliJ --&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- Its adds Flink's core classes to the runtime class path. --&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- Otherwise they are missing in IntelliJ, because the dependency is 'provided' --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">profiles</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">profile</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">id</span>&gt;</span>add-dependencies-for-IDEA<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">activation</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>idea.version<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">activation</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-scala_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">scope</span>&gt;</span>compile<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-streaming-scala_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">scope</span>&gt;</span>compile<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.scala-lang<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>scala-library<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;scala.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">scope</span>&gt;</span>compile<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">profile</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">profiles</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>在 id 为 <code>add-dependencies-for-IDEA</code> 的 profile 中，所有的核心依赖都被标识为 compile，此时你可以无需改动任何代码，只需要在 IDEA 的 Maven 面板中勾选该 profile，即可直接在 IDEA 中运行 Flink 项目：</p>
<div align="center"> <img src="../pictures/flink-maven-profile.png"> </div>

<h2 id="四、词频统计案例"><a href="#四、词频统计案例" class="headerlink" title="四、词频统计案例"></a>四、词频统计案例</h2><p>项目创建完成后，可以先书写一个简单的词频统计的案例来尝试运行 Flink 项目，以下以 Scala 语言为例，分别介绍流处理程序和批处理程序的编程示例：</p>
<h3 id="4-1-批处理示例"><a href="#4-1-批处理示例" class="headerlink" title="4.1 批处理示例"></a>4.1 批处理示例</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WordCountBatch</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> benv = <span class="type">ExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    <span class="keyword">val</span> dataSet = benv.readTextFile(<span class="string">"D:\\wordcount.txt"</span>)</span><br><span class="line">    dataSet.flatMap &#123; _.toLowerCase.split(<span class="string">","</span>)&#125;</span><br><span class="line">            .filter (_.nonEmpty)</span><br><span class="line">            .map &#123; (_, <span class="number">1</span>) &#125;</span><br><span class="line">            .groupBy(<span class="number">0</span>)</span><br><span class="line">            .sum(<span class="number">1</span>)</span><br><span class="line">            .print()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>其中 <code>wordcount.txt</code> 中的内容如下：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">a,a,a,a,a</span><br><span class="line">b,b,b</span><br><span class="line">c,c</span><br><span class="line">d,d</span><br></pre></td></tr></table></figure>
<p>本机不需要配置其他任何的 Flink 环境，直接运行 Main 方法即可，结果如下：</p>
<div align="center"> <img src="../pictures/flink-word-count.png"> </div>

<h3 id="4-2-流处理示例"><a href="#4-2-流处理示例" class="headerlink" title="4.2 流处理示例"></a>4.2 流处理示例</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.time.<span class="type">Time</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WordCountStreaming</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> senv = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> dataStream: <span class="type">DataStream</span>[<span class="type">String</span>] = senv.socketTextStream(<span class="string">"192.168.0.229"</span>, <span class="number">9999</span>, '\n')</span><br><span class="line">    dataStream.flatMap &#123; line =&gt; line.toLowerCase.split(<span class="string">","</span>) &#125;</span><br><span class="line">              .filter(_.nonEmpty)</span><br><span class="line">              .map &#123; word =&gt; (word, <span class="number">1</span>) &#125;</span><br><span class="line">              .keyBy(<span class="number">0</span>)</span><br><span class="line">              .timeWindow(<span class="type">Time</span>.seconds(<span class="number">3</span>))</span><br><span class="line">              .sum(<span class="number">1</span>)</span><br><span class="line">              .print()</span><br><span class="line">    senv.execute(<span class="string">"Streaming WordCount"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里以监听指定端口号上的内容为例，使用以下命令来开启端口服务：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">nc -lk 9999</span><br></pre></td></tr></table></figure>
<p>之后输入测试数据即可观察到流处理程序的处理情况。</p>
<h2 id="五、使用-Scala-Shell"><a href="#五、使用-Scala-Shell" class="headerlink" title="五、使用 Scala Shell"></a>五、使用 Scala Shell</h2><p>对于日常的 Demo 项目，如果你不想频繁地启动 IDEA 来观察测试结果，可以像 Spark 一样，直接使用 Scala Shell 来运行程序，这对于日常的学习来说，效果更加直观，也更省时。Flink 安装包的下载地址如下：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">https://flink.apache.org/downloads.html</span><br></pre></td></tr></table></figure>
<p>Flink 大多数版本都提供有 Scala 2.11 和 Scala 2.12 两个版本的安装包可供下载：</p>
<div align="center"> <img src="../pictures/flink-download.png"> </div>

<p>下载完成后进行解压即可，Scala Shell 位于安装目录的 bin 目录下，直接使用以下命令即可以本地模式启动：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">./start-scala-shell.sh local</span><br></pre></td></tr></table></figure>
<p>命令行启动完成后，其已经提供了批处理 （benv 和 btenv）和流处理（senv 和 stenv）的运行环境，可以直接运行 Scala Flink 程序，示例如下：</p>
<div align="center"> <img src="../pictures/flink-scala-shell.png"> </div>

<p>最后解释一个常见的异常：这里我使用的 Flink 版本为 1.9.1，启动时会抛出如下异常。这里因为按照官方的说明，目前所有 Scala 2.12 版本的安装包暂时都不支持 Scala Shell，所以如果想要使用 Scala Shell，只能选择 Scala 2.11 版本的安装包。 </p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@hadoop001 bin]# ./start-scala-shell.sh local</span><br><span class="line">错误: 找不到或无法加载主类 org.apache.flink.api.scala.FlinkShell</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>notes</category>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink 核心概念综述</title>
    <url>/2021/03/17/Flink%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5%E7%BB%BC%E8%BF%B0/</url>
    <content><![CDATA[<h2 id="一、Flink-简介"><a href="#一、Flink-简介" class="headerlink" title="一、Flink 简介"></a>一、Flink 简介</h2><p>Apache Flink 诞生于柏林工业大学的一个研究性项目，原名 StratoSphere 。2014 年，由 StratoSphere 项目孵化出 Flink，并于同年捐赠 Apache，之后成为 Apache 的顶级项目。2019 年 1 年，阿里巴巴收购了 Flink 的母公司 Data Artisans，并宣布开源内部的 Blink，Blink 是阿里巴巴基于 Flink 优化后的版本，增加了大量的新功能，并在性能和稳定性上进行了各种优化，经历过阿里内部多种复杂业务的挑战和检验。同时阿里巴巴也表示会逐步将这些新功能和特性 Merge 回社区版本的 Flink 中，因此 Flink 成为目前最为火热的大数据处理框架。</p>
<p>简单来说，Flink 是一个分布式的流处理框架，它能够对有界和无界的数据流进行高效的处理。Flink 的核心是流处理，当然它也能支持批处理，Flink 将批处理看成是流处理的一种特殊情况，即数据流是有明确界限的。这和 Spark Streaming 的思想是完全相反的，Spark Streaming 的核心是批处理，它将流处理看成是批处理的一种特殊情况， 即把数据流进行极小粒度的拆分，拆分为多个微批处理。</p>
<p>Flink 有界数据流和无界数据流：</p>
<div align="center"> <img width="600px" src="../pictures/flink-bounded-unbounded.png"> </div>




<p>Spark Streaming 数据流的拆分：</p>
<div align="center"> <img width="600px" src="../pictures/streaming-flow.png"> </div>




<h2 id="二、Flink-核心架构"><a href="#二、Flink-核心架构" class="headerlink" title="二、Flink 核心架构"></a>二、Flink 核心架构</h2><p>Flink 采用分层的架构设计，从而保证各层在功能和职责上的清晰。如下图所示，由上而下分别是 API &amp; Libraries 层、Runtime 核心层以及物理部署层：</p>
<div align="center"> <img width="600px" src="../pictures/flink-stack.png"> </div>




<h3 id="2-1-API-amp-Libraries-层"><a href="#2-1-API-amp-Libraries-层" class="headerlink" title="2.1 API &amp; Libraries 层"></a>2.1 API &amp; Libraries 层</h3><p>这一层主要提供了编程 API 和 顶层类库：</p>
<ul>
<li>编程 API : 用于进行流处理的 DataStream API 和用于进行批处理的 DataSet API；</li>
<li>顶层类库：包括用于复杂事件处理的 CEP 库；用于结构化数据查询的 SQL &amp; Table 库，以及基于批处理的机器学习库 FlinkML 和 图形处理库 Gelly。</li>
</ul>
<h3 id="2-2-Runtime-核心层"><a href="#2-2-Runtime-核心层" class="headerlink" title="2.2 Runtime 核心层"></a>2.2 Runtime 核心层</h3><p>这一层是 Flink 分布式计算框架的核心实现层，包括作业转换，任务调度，资源分配，任务执行等功能，基于这一层的实现，可以在流式引擎下同时运行流处理程序和批处理程序。</p>
<h3 id="2-3-物理部署层"><a href="#2-3-物理部署层" class="headerlink" title="2.3 物理部署层"></a>2.3 物理部署层</h3><p>Flink 的物理部署层，用于支持在不同平台上部署运行 Flink 应用。</p>
<h2 id="三、Flink-分层-API"><a href="#三、Flink-分层-API" class="headerlink" title="三、Flink 分层 API"></a>三、Flink 分层 API</h2><p>在上面介绍的 API &amp; Libraries 这一层，Flink 又进行了更为具体的划分。具体如下：</p>
<div align="center"> <img src="../pictures/flink-api-stack.png"> </div>




<p>按照如上的层次结构，API 的一致性由下至上依次递增，接口的表现能力由下至上依次递减，各层的核心功能如下：</p>
<h3 id="3-1-SQL-amp-Table-API"><a href="#3-1-SQL-amp-Table-API" class="headerlink" title="3.1 SQL &amp; Table API"></a>3.1 SQL &amp; Table API</h3><p>SQL &amp; Table API 同时适用于批处理和流处理，这意味着你可以对有界数据流和无界数据流以相同的语义进行查询，并产生相同的结果。除了基本查询外， 它还支持自定义的标量函数，聚合函数以及表值函数，可以满足多样化的查询需求。 </p>
<h3 id="3-2-DataStream-amp-DataSet-API"><a href="#3-2-DataStream-amp-DataSet-API" class="headerlink" title="3.2 DataStream &amp; DataSet API"></a>3.2 DataStream &amp; DataSet API</h3><p>DataStream &amp;  DataSet API 是 Flink 数据处理的核心 API，支持使用 Java 语言或 Scala 语言进行调用，提供了数据读取，数据转换和数据输出等一系列常用操作的封装。</p>
<h3 id="3-3-Stateful-Stream-Processing"><a href="#3-3-Stateful-Stream-Processing" class="headerlink" title="3.3 Stateful Stream Processing"></a>3.3 Stateful Stream Processing</h3><p>Stateful Stream Processing 是最低级别的抽象，它通过 Process Function 函数内嵌到 DataStream API 中。 Process Function 是 Flink 提供的最底层 API，具有最大的灵活性，允许开发者对于时间和状态进行细粒度的控制。</p>
<h2 id="四、Flink-集群架构"><a href="#四、Flink-集群架构" class="headerlink" title="四、Flink 集群架构"></a>四、Flink 集群架构</h2><h3 id="4-1-核心组件"><a href="#4-1-核心组件" class="headerlink" title="4.1  核心组件"></a>4.1  核心组件</h3><p>按照上面的介绍，Flink 核心架构的第二层是 Runtime 层， 该层采用标准的 Master - Slave 结构， 其中，Master 部分又包含了三个核心组件：Dispatcher、ResourceManager 和 JobManager，而 Slave 则主要是 TaskManager 进程。它们的功能分别如下：</p>
<ul>
<li><strong>JobManagers</strong> (也称为 <em>masters</em>) ：JobManagers 接收由 Dispatcher 传递过来的执行程序，该执行程序包含了作业图 (JobGraph)，逻辑数据流图 (logical dataflow graph) 及其所有的 classes 文件以及第三方类库 (libraries) 等等 。紧接着 JobManagers 会将 JobGraph 转换为执行图 (ExecutionGraph)，然后向 ResourceManager 申请资源来执行该任务，一旦申请到资源，就将执行图分发给对应的 TaskManagers 。因此每个作业 (Job) 至少有一个 JobManager；高可用部署下可以有多个 JobManagers，其中一个作为 <em>leader</em>，其余的则处于 <em>standby</em> 状态。</li>
<li><strong>TaskManagers</strong> (也称为 <em>workers</em>) : TaskManagers 负责实际的子任务 (subtasks) 的执行，每个 TaskManagers 都拥有一定数量的 slots。Slot 是一组固定大小的资源的合集 (如计算能力，存储空间)。TaskManagers 启动后，会将其所拥有的 slots 注册到 ResourceManager 上，由 ResourceManager 进行统一管理。</li>
<li><strong>Dispatcher</strong>：负责接收客户端提交的执行程序，并传递给 JobManager 。除此之外，它还提供了一个 WEB UI 界面，用于监控作业的执行情况。</li>
<li><strong>ResourceManager</strong> ：负责管理 slots 并协调集群资源。ResourceManager 接收来自 JobManager 的资源请求，并将存在空闲 slots 的 TaskManagers 分配给 JobManager 执行任务。Flink 基于不同的部署平台，如 YARN , Mesos，K8s 等提供了不同的资源管理器，当 TaskManagers 没有足够的 slots 来执行任务时，它会向第三方平台发起会话来请求额外的资源。</li>
</ul>
<div align="center"> <img src="../pictures/flink-application-submission.png"> </div>


<h3 id="4-2-Task-amp-SubTask"><a href="#4-2-Task-amp-SubTask" class="headerlink" title="4.2  Task &amp; SubTask"></a>4.2  Task &amp; SubTask</h3><p>上面我们提到：TaskManagers 实际执行的是 SubTask，而不是 Task，这里解释一下两者的区别：</p>
<p>在执行分布式计算时，Flink 将可以链接的操作 (operators) 链接到一起，这就是 Task。之所以这样做， 是为了减少线程间切换和缓冲而导致的开销，在降低延迟的同时可以提高整体的吞吐量。 但不是所有的 operator 都可以被链接，如下 keyBy 等操作会导致网络 shuffle 和重分区，因此其就不能被链接，只能被单独作为一个 Task。  简单来说，一个 Task 就是一个可以链接的最小的操作链 (Operator Chains) 。如下图，source 和 map 算子被链接到一块，因此整个作业就只有三个 Task：</p>
<div align="center"> <img src="../pictures/flink-task-subtask.png"> </div>


<p>解释完 Task ，我们在解释一下什么是 SubTask，其准确的翻译是： <em>A subtask is one parallel slice of a task</em>，即一个 Task 可以按照其并行度拆分为多个 SubTask。如上图，source &amp; map 具有两个并行度，KeyBy 具有两个并行度，Sink 具有一个并行度，因此整个虽然只有 3 个 Task，但是却有 5 个 SubTask。Jobmanager 负责定义和拆分这些 SubTask，并将其交给 Taskmanagers 来执行，每个 SubTask 都是一个单独的线程。</p>
<h3 id="4-3-资源管理"><a href="#4-3-资源管理" class="headerlink" title="4.3  资源管理"></a>4.3  资源管理</h3><p>理解了 SubTasks ，我们再来看看其与 Slots 的对应情况。一种可能的分配情况如下：</p>
<div align="center"> <img src="../pictures/flink-tasks-slots.png"> </div>




<p>这时每个 SubTask 线程运行在一个独立的 TaskSlot， 它们共享所属的 TaskManager 进程的TCP 连接（通过多路复用技术）和心跳信息 (heartbeat messages)，从而可以降低整体的性能开销。此时看似是最好的情况，但是每个操作需要的资源都是不尽相同的，这里假设该作业 keyBy 操作所需资源的数量比 Sink 多很多 ，那么此时 Sink 所在 Slot 的资源就没有得到有效的利用。</p>
<p>基于这个原因，Flink 允许多个 subtasks 共享 slots，即使它们是不同 tasks 的 subtasks，但只要它们来自同一个 Job 就可以。假设上面 souce &amp; map 和 keyBy 的并行度调整为 6，而 Slot 的数量不变，此时情况如下：</p>
<div align="center"> <img src="../pictures/flink-subtask-slots.png"> </div>




<p>可以看到一个 Task Slot 中运行了多个 SubTask 子任务，此时每个子任务仍然在一个独立的线程中执行，只不过共享一组 Sot 资源而已。那么 Flink 到底如何确定一个 Job 至少需要多少个 Slot 呢？Flink 对于这个问题的处理很简单，默认情况一个 Job 所需要的 Slot 的数量就等于其 Operation 操作的最高并行度。如下， A，B，D 操作的并行度为 4，而 C，E 操作的并行度为 2，那么此时整个 Job 就需要至少四个 Slots 来完成。通过这个机制，Flink 就可以不必去关心一个 Job 到底会被拆分为多少个 Tasks 和 SubTasks。</p>
<div align="center"> <img src="../pictures/flink-task-parallelism.png"> </div>






<h3 id="4-4-组件通讯"><a href="#4-4-组件通讯" class="headerlink" title="4.4 组件通讯"></a>4.4 组件通讯</h3><p>Flink 的所有组件都基于 Actor System 来进行通讯。Actor system是多种角色的 actor 的容器，它提供调度，配置，日志记录等多种服务，并包含一个可以启动所有 actor 的线程池，如果 actor 是本地的，则消息通过共享内存进行共享，但如果 actor 是远程的，则通过 RPC 的调用来传递消息。</p>
<div align="center"> <img src="../pictures/flink-process.png"> </div>




<h2 id="五、Flink-的优点"><a href="#五、Flink-的优点" class="headerlink" title="五、Flink 的优点"></a>五、Flink 的优点</h2><p>最后基于上面的介绍，来总结一下 Flink 的优点：</p>
<ul>
<li>Flink 是基于事件驱动 (Event-driven) 的应用，能够同时支持流处理和批处理；</li>
<li>基于内存的计算，能够保证高吞吐和低延迟，具有优越的性能表现；</li>
<li>支持精确一次 (Exactly-once) 语意，能够完美地保证一致性和正确性；</li>
<li>分层 API ，能够满足各个层次的开发需求；</li>
<li>支持高可用配置，支持保存点机制，能够提供安全性和稳定性上的保证；</li>
<li>多样化的部署方式，支持本地，远端，云端等多种部署方案；</li>
<li>具有横向扩展架构，能够按照用户的需求进行动态扩容；</li>
<li>活跃度极高的社区和完善的生态圈的支持。</li>
</ul>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.9/concepts/programming-model.html" target="_blank" rel="noopener">Dataflow Programming Model</a></li>
<li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.9/concepts/runtime.html" target="_blank" rel="noopener">Distributed Runtime Environment</a></li>
<li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.9/internals/components.html" target="_blank" rel="noopener">Component Stack</a></li>
<li>Fabian Hueske , Vasiliki Kalavri . 《Stream Processing with Apache Flink》.  O’Reilly Media .  2019-4-30 </li>
</ul>
]]></content>
      <categories>
        <category>notes</category>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink 状态管理</title>
    <url>/2021/03/17/Flink%E7%8A%B6%E6%80%81%E7%AE%A1%E7%90%86%E4%B8%8E%E6%A3%80%E6%9F%A5%E7%82%B9%E6%9C%BA%E5%88%B6/</url>
    <content><![CDATA[<h2 id="一、状态分类"><a href="#一、状态分类" class="headerlink" title="一、状态分类"></a>一、状态分类</h2><p>相对于其他流计算框架，Flink 一个比较重要的特性就是其支持有状态计算。即你可以将中间的计算结果进行保存，并提供给后续的计算使用：</p>
<div align="center"> <img width="500px" src="../pictures/flink-stateful-stream.png"> </div>



<p>具体而言，Flink 又将状态 (State) 分为 Keyed State 与 Operator State：</p>
<h3 id="2-1-算子状态"><a href="#2-1-算子状态" class="headerlink" title="2.1 算子状态"></a>2.1 算子状态</h3><p>算子状态 (Operator State)：顾名思义，状态是和算子进行绑定的，一个算子的状态不能被其他算子所访问到。官方文档上对 Operator State 的解释是：<em>each operator state is bound to one parallel operator instance</em>，所以更为确切的说一个算子状态是与一个并发的算子实例所绑定的，即假设算子的并行度是 2，那么其应有两个对应的算子状态：</p>
<div align="center"> <img width="500px" src="../pictures/flink-operator-state.png"> </div>



<h3 id="2-2-键控状态"><a href="#2-2-键控状态" class="headerlink" title="2.2 键控状态"></a>2.2 键控状态</h3><p>键控状态 (Keyed State) ：是一种特殊的算子状态，即状态是根据 key 值进行区分的，Flink 会为每类键值维护一个状态实例。如下图所示，每个颜色代表不同 key 值，对应四个不同的状态实例。需要注意的是键控状态只能在 <code>KeyedStream</code> 上进行使用，我们可以通过 <code>stream.keyBy(...)</code> 来得到 <code>KeyedStream</code> 。</p>
<div align="center"> <img src="../pictures/flink-keyed-state.png"> </div>



<h2 id="二、状态编程"><a href="#二、状态编程" class="headerlink" title="二、状态编程"></a>二、状态编程</h2><h3 id="2-1-键控状态"><a href="#2-1-键控状态" class="headerlink" title="2.1 键控状态"></a>2.1 键控状态</h3><p>Flink 提供了以下数据格式来管理和存储键控状态 (Keyed State)：</p>
<ul>
<li><strong>ValueState</strong>：存储单值类型的状态。可以使用  <code>update(T)</code> 进行更新，并通过 <code>T value()</code> 进行检索。 </li>
<li><strong>ListState</strong>：存储列表类型的状态。可以使用 <code>add(T)</code> 或 <code>addAll(List)</code> 添加元素；并通过 <code>get()</code> 获得整个列表。</li>
<li><strong>ReducingState</strong>：用于存储经过 ReduceFunction 计算后的结果，使用 <code>add(T)</code> 增加元素。</li>
<li><strong>AggregatingState</strong>：用于存储经过 AggregatingState 计算后的结果，使用 <code>add(IN)</code> 添加元素。</li>
<li><strong>FoldingState</strong>：已被标识为废弃，会在未来版本中移除，官方推荐使用 <code>AggregatingState</code> 代替。</li>
<li><strong>MapState</strong>：维护 Map 类型的状态。</li>
</ul>
<p>以上所有增删改查方法不必硬记，在使用时通过语法提示来调用即可。这里给出一个具体的使用示例：假设我们正在开发一个监控系统，当监控数据超过阈值一定次数后，需要发出报警信息。这里之所以要达到一定次数，是因为由于偶发原因，偶尔一次超过阈值并不能代表什么，故需要达到一定次数后才触发报警，这就需要使用到 Flink 的状态编程。相关代码如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ThresholdWarning</span> <span class="keyword">extends</span> </span></span><br><span class="line"><span class="class">    <span class="title">RichFlatMapFunction</span>&lt;<span class="title">Tuple2</span>&lt;<span class="title">String</span>, <span class="title">Long</span>&gt;, <span class="title">Tuple2</span>&lt;<span class="title">String</span>, <span class="title">List</span>&lt;<span class="title">Long</span>&gt;&gt;&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 通过ListState来存储非正常数据的状态</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">transient</span> ListState&lt;Long&gt; abnormalData;</span><br><span class="line">    <span class="comment">// 需要监控的阈值</span></span><br><span class="line">    <span class="keyword">private</span> Long threshold;</span><br><span class="line">    <span class="comment">// 触发报警的次数</span></span><br><span class="line">    <span class="keyword">private</span> Integer numberOfTimes;</span><br><span class="line"></span><br><span class="line">    ThresholdWarning(Long threshold, Integer numberOfTimes) &#123;</span><br><span class="line">        <span class="keyword">this</span>.threshold = threshold;</span><br><span class="line">        <span class="keyword">this</span>.numberOfTimes = numberOfTimes;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 通过状态名称(句柄)获取状态实例，如果不存在则会自动创建</span></span><br><span class="line">        abnormalData = getRuntimeContext().getListState(</span><br><span class="line">            <span class="keyword">new</span> ListStateDescriptor&lt;&gt;(<span class="string">"abnormalData"</span>, Long<span class="class">.<span class="keyword">class</span>))</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(Tuple2&lt;String, Long&gt; value, Collector&lt;Tuple2&lt;String, List&lt;Long&gt;&gt;&gt; out)</span></span></span><br><span class="line"><span class="function">        <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        Long inputValue = value.f1;</span><br><span class="line">        <span class="comment">// 如果输入值超过阈值，则记录该次不正常的数据信息</span></span><br><span class="line">        <span class="keyword">if</span> (inputValue &gt;= threshold) &#123;</span><br><span class="line">            abnormalData.add(inputValue);</span><br><span class="line">        &#125;</span><br><span class="line">        ArrayList&lt;Long&gt; list = Lists.newArrayList(abnormalData.get().iterator());</span><br><span class="line">        <span class="comment">// 如果不正常的数据出现达到一定次数，则输出报警信息</span></span><br><span class="line">        <span class="keyword">if</span> (list.size() &gt;= numberOfTimes) &#123;</span><br><span class="line">            out.collect(Tuple2.of(value.f0 + <span class="string">" 超过指定阈值 "</span>, list));</span><br><span class="line">            <span class="comment">// 报警信息输出后，清空状态</span></span><br><span class="line">            abnormalData.clear();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>调用自定义的状态监控，这里我们使用 a，b 来代表不同类型的监控数据，分别对其数据进行监控：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">DataStreamSource&lt;Tuple2&lt;String, Long&gt;&gt; tuple2DataStreamSource = env.fromElements(</span><br><span class="line">    Tuple2.of(<span class="string">"a"</span>, <span class="number">50L</span>), Tuple2.of(<span class="string">"a"</span>, <span class="number">80L</span>), Tuple2.of(<span class="string">"a"</span>, <span class="number">400L</span>),</span><br><span class="line">    Tuple2.of(<span class="string">"a"</span>, <span class="number">100L</span>), Tuple2.of(<span class="string">"a"</span>, <span class="number">200L</span>), Tuple2.of(<span class="string">"a"</span>, <span class="number">200L</span>),</span><br><span class="line">    Tuple2.of(<span class="string">"b"</span>, <span class="number">100L</span>), Tuple2.of(<span class="string">"b"</span>, <span class="number">200L</span>), Tuple2.of(<span class="string">"b"</span>, <span class="number">200L</span>),</span><br><span class="line">    Tuple2.of(<span class="string">"b"</span>, <span class="number">500L</span>), Tuple2.of(<span class="string">"b"</span>, <span class="number">600L</span>), Tuple2.of(<span class="string">"b"</span>, <span class="number">700L</span>));</span><br><span class="line">tuple2DataStreamSource</span><br><span class="line">    .keyBy(<span class="number">0</span>)</span><br><span class="line">    .flatMap(<span class="keyword">new</span> ThresholdWarning(<span class="number">100L</span>, <span class="number">3</span>))  <span class="comment">// 超过100的阈值3次后就进行报警</span></span><br><span class="line">    .printToErr();</span><br><span class="line">env.execute(<span class="string">"Managed Keyed State"</span>);</span><br></pre></td></tr></table></figure>
<p>输出如下结果如下：</p>
<div align="center"> <img src="../pictures/flink-state-management.png"> </div>



<h3 id="2-2-状态有效期"><a href="#2-2-状态有效期" class="headerlink" title="2.2 状态有效期"></a>2.2 状态有效期</h3><p>以上任何类型的 keyed state 都支持配置有效期 (TTL) ，示例如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">StateTtlConfig ttlConfig = StateTtlConfig</span><br><span class="line">    <span class="comment">// 设置有效期为 10 秒</span></span><br><span class="line">    .newBuilder(Time.seconds(<span class="number">10</span>))  </span><br><span class="line">    <span class="comment">// 设置有效期更新规则，这里设置为当创建和写入时，都重置其有效期到规定的10秒</span></span><br><span class="line">    .setUpdateType(StateTtlConfig.UpdateType.OnCreateAndWrite) </span><br><span class="line">    <span class="comment">/*设置只要值过期就不可见，另外一个可选值是ReturnExpiredIfNotCleanedUp，</span></span><br><span class="line"><span class="comment">     代表即使值过期了，但如果还没有被物理删除，就是可见的*/</span></span><br><span class="line">    .setStateVisibility(StateTtlConfig.StateVisibility.NeverReturnExpired)</span><br><span class="line">    .build();</span><br><span class="line">ListStateDescriptor&lt;Long&gt; descriptor = <span class="keyword">new</span> ListStateDescriptor&lt;&gt;(<span class="string">"abnormalData"</span>, Long<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">descriptor.enableTimeToLive(ttlConfig);</span><br></pre></td></tr></table></figure>
<h3 id="2-3-算子状态"><a href="#2-3-算子状态" class="headerlink" title="2.3 算子状态"></a>2.3 算子状态</h3><p>相比于键控状态，算子状态目前支持的存储类型只有以下三种：</p>
<ul>
<li><strong>ListState</strong>：存储列表类型的状态。</li>
<li><strong>UnionListState</strong>：存储列表类型的状态，与 ListState 的区别在于：如果并行度发生变化，ListState 会将该算子的所有并发的状态实例进行汇总，然后均分给新的 Task；而 UnionListState 只是将所有并发的状态实例汇总起来，具体的划分行为则由用户进行定义。</li>
<li><strong>BroadcastState</strong>：用于广播的算子状态。</li>
</ul>
<p>这里我们继续沿用上面的例子，假设此时我们不需要区分监控数据的类型，只要有监控数据超过阈值并达到指定的次数后，就进行报警，代码如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ThresholdWarning</span> <span class="keyword">extends</span> <span class="title">RichFlatMapFunction</span>&lt;<span class="title">Tuple2</span>&lt;<span class="title">String</span>, <span class="title">Long</span>&gt;, </span></span><br><span class="line"><span class="class"><span class="title">Tuple2</span>&lt;<span class="title">String</span>, <span class="title">List</span>&lt;<span class="title">Tuple2</span>&lt;<span class="title">String</span>, <span class="title">Long</span>&gt;&gt;&gt;&gt; <span class="keyword">implements</span> <span class="title">CheckpointedFunction</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 非正常数据</span></span><br><span class="line">    <span class="keyword">private</span> List&lt;Tuple2&lt;String, Long&gt;&gt; bufferedData;</span><br><span class="line">    <span class="comment">// checkPointedState</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">transient</span> ListState&lt;Tuple2&lt;String, Long&gt;&gt; checkPointedState;</span><br><span class="line">    <span class="comment">// 需要监控的阈值</span></span><br><span class="line">    <span class="keyword">private</span> Long threshold;</span><br><span class="line">    <span class="comment">// 次数</span></span><br><span class="line">    <span class="keyword">private</span> Integer numberOfTimes;</span><br><span class="line"></span><br><span class="line">    ThresholdWarning(Long threshold, Integer numberOfTimes) &#123;</span><br><span class="line">        <span class="keyword">this</span>.threshold = threshold;</span><br><span class="line">        <span class="keyword">this</span>.numberOfTimes = numberOfTimes;</span><br><span class="line">        <span class="keyword">this</span>.bufferedData = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">initializeState</span><span class="params">(FunctionInitializationContext context)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">// 注意这里获取的是OperatorStateStore</span></span><br><span class="line">        checkPointedState = context.getOperatorStateStore().</span><br><span class="line">            getListState(<span class="keyword">new</span> ListStateDescriptor&lt;&gt;(<span class="string">"abnormalData"</span>,</span><br><span class="line">                TypeInformation.of(<span class="keyword">new</span> TypeHint&lt;Tuple2&lt;String, Long&gt;&gt;() &#123;</span><br><span class="line">                &#125;)));</span><br><span class="line">        <span class="comment">// 如果发生重启，则需要从快照中将状态进行恢复</span></span><br><span class="line">        <span class="keyword">if</span> (context.isRestored()) &#123;</span><br><span class="line">            <span class="keyword">for</span> (Tuple2&lt;String, Long&gt; element : checkPointedState.get()) &#123;</span><br><span class="line">                bufferedData.add(element);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(Tuple2&lt;String, Long&gt; value, </span></span></span><br><span class="line"><span class="function"><span class="params">                        Collector&lt;Tuple2&lt;String, List&lt;Tuple2&lt;String, Long&gt;&gt;&gt;&gt; out)</span> </span>&#123;</span><br><span class="line">        Long inputValue = value.f1;</span><br><span class="line">        <span class="comment">// 超过阈值则进行记录</span></span><br><span class="line">        <span class="keyword">if</span> (inputValue &gt;= threshold) &#123;</span><br><span class="line">            bufferedData.add(value);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 超过指定次数则输出报警信息</span></span><br><span class="line">        <span class="keyword">if</span> (bufferedData.size() &gt;= numberOfTimes) &#123;</span><br><span class="line">             <span class="comment">// 顺便输出状态实例的hashcode</span></span><br><span class="line">             out.collect(Tuple2.of(checkPointedState.hashCode() + <span class="string">"阈值警报！"</span>, bufferedData));</span><br><span class="line">            bufferedData.clear();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">snapshotState</span><span class="params">(FunctionSnapshotContext context)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">// 在进行快照时，将数据存储到checkPointedState</span></span><br><span class="line">        checkPointedState.clear();</span><br><span class="line">        <span class="keyword">for</span> (Tuple2&lt;String, Long&gt; element : bufferedData) &#123;</span><br><span class="line">            checkPointedState.add(element);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>调用自定义算子状态，这里需要将并行度设置为 1：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"><span class="comment">// 开启检查点机制</span></span><br><span class="line">env.enableCheckpointing(<span class="number">1000</span>);</span><br><span class="line"><span class="comment">// 设置并行度为1</span></span><br><span class="line">DataStreamSource&lt;Tuple2&lt;String, Long&gt;&gt; tuple2DataStreamSource = env.setParallelism(<span class="number">1</span>).fromElements(</span><br><span class="line">    Tuple2.of(<span class="string">"a"</span>, <span class="number">50L</span>), Tuple2.of(<span class="string">"a"</span>, <span class="number">80L</span>), Tuple2.of(<span class="string">"a"</span>, <span class="number">400L</span>),</span><br><span class="line">    Tuple2.of(<span class="string">"a"</span>, <span class="number">100L</span>), Tuple2.of(<span class="string">"a"</span>, <span class="number">200L</span>), Tuple2.of(<span class="string">"a"</span>, <span class="number">200L</span>),</span><br><span class="line">    Tuple2.of(<span class="string">"b"</span>, <span class="number">100L</span>), Tuple2.of(<span class="string">"b"</span>, <span class="number">200L</span>), Tuple2.of(<span class="string">"b"</span>, <span class="number">200L</span>),</span><br><span class="line">    Tuple2.of(<span class="string">"b"</span>, <span class="number">500L</span>), Tuple2.of(<span class="string">"b"</span>, <span class="number">600L</span>), Tuple2.of(<span class="string">"b"</span>, <span class="number">700L</span>));</span><br><span class="line">tuple2DataStreamSource</span><br><span class="line">    .flatMap(<span class="keyword">new</span> ThresholdWarning(<span class="number">100L</span>, <span class="number">3</span>))</span><br><span class="line">    .printToErr();</span><br><span class="line">env.execute(<span class="string">"Managed Keyed State"</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>此时输出如下：</p>
<div align="center"> <img src="../pictures/flink-operator-state-para1.png"> </div>



<p>在上面的调用代码中，我们将程序的并行度设置为 1，可以看到三次输出中状态实例的 hashcode 全是一致的，证明它们都同一个状态实例。假设将并行度设置为 2，此时输出如下：</p>
<div align="center"> <img src="../pictures/flink-operator-state-para2.png"> </div>



<p>可以看到此时两次输出中状态实例的 hashcode 是不一致的，代表它们不是同一个状态实例，这也就是上文提到的，一个算子状态是与一个并发的算子实例所绑定的。同时这里只输出两次，是因为在并发处理的情况下，线程 1 可能拿到 5 个非正常值，线程 2 可能拿到 4 个非正常值，因为要大于 3 次才能输出，所以在这种情况下就会出现只输出两条记录的情况，所以需要将程序的并行度设置为 1。</p>
<h2 id="三、检查点机制"><a href="#三、检查点机制" class="headerlink" title="三、检查点机制"></a>三、检查点机制</h2><h3 id="3-1-CheckPoints"><a href="#3-1-CheckPoints" class="headerlink" title="3.1 CheckPoints"></a>3.1 CheckPoints</h3><p>为了使 Flink 的状态具有良好的容错性，Flink 提供了检查点机制 (CheckPoints)  。通过检查点机制，Flink 定期在数据流上生成 checkpoint barrier ，当某个算子收到 barrier 时，即会基于当前状态生成一份快照，然后再将该 barrier 传递到下游算子，下游算子接收到该 barrier 后，也基于当前状态生成一份快照，依次传递直至到最后的 Sink 算子上。当出现异常后，Flink 就可以根据最近的一次的快照数据将所有算子恢复到先前的状态。</p>
<div align="center"> <img src="../pictures/flink-stream-barriers.png"> </div>





<h3 id="3-2-开启检查点"><a href="#3-2-开启检查点" class="headerlink" title="3.2 开启检查点"></a>3.2 开启检查点</h3><p>默认情况下，检查点机制是关闭的，需要在程序中进行开启：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 开启检查点机制，并指定状态检查点之间的时间间隔</span></span><br><span class="line">env.enableCheckpointing(<span class="number">1000</span>); </span><br><span class="line"></span><br><span class="line"><span class="comment">// 其他可选配置如下：</span></span><br><span class="line"><span class="comment">// 设置语义</span></span><br><span class="line">env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);</span><br><span class="line"><span class="comment">// 设置两个检查点之间的最小时间间隔</span></span><br><span class="line">env.getCheckpointConfig().setMinPauseBetweenCheckpoints(<span class="number">500</span>);</span><br><span class="line"><span class="comment">// 设置执行Checkpoint操作时的超时时间</span></span><br><span class="line">env.getCheckpointConfig().setCheckpointTimeout(<span class="number">60000</span>);</span><br><span class="line"><span class="comment">// 设置最大并发执行的检查点的数量</span></span><br><span class="line">env.getCheckpointConfig().setMaxConcurrentCheckpoints(<span class="number">1</span>);</span><br><span class="line"><span class="comment">// 将检查点持久化到外部存储</span></span><br><span class="line">env.getCheckpointConfig().enableExternalizedCheckpoints(</span><br><span class="line">    ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);</span><br><span class="line"><span class="comment">// 如果有更近的保存点时，是否将作业回退到该检查点</span></span><br><span class="line">env.getCheckpointConfig().setPreferCheckpointForRecovery(<span class="keyword">true</span>);</span><br></pre></td></tr></table></figure>
<h3 id="3-3-保存点机制"><a href="#3-3-保存点机制" class="headerlink" title="3.3 保存点机制"></a>3.3 保存点机制</h3><p>保存点机制 (Savepoints) 是检查点机制的一种特殊的实现，它允许你通过手工的方式来触发 Checkpoint，并将结果持久化存储到指定路径中，主要用于避免 Flink 集群在重启或升级时导致状态丢失。示例如下：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 触发指定id的作业的Savepoint，并将结果存储到指定目录下</span></span><br><span class="line">bin/flink savepoint :jobId [:targetDirectory]</span><br></pre></td></tr></table></figure>
<p>更多命令和配置可以参考官方文档：<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.9/zh/ops/state/savepoints.html" target="_blank" rel="noopener">savepoints</a></p>
<h2 id="四、状态后端"><a href="#四、状态后端" class="headerlink" title="四、状态后端"></a>四、状态后端</h2><h3 id="4-1-状态管理器分类"><a href="#4-1-状态管理器分类" class="headerlink" title="4.1 状态管理器分类"></a>4.1 状态管理器分类</h3><p>默认情况下，所有的状态都存储在 JVM 的堆内存中，在状态数据过多的情况下，这种方式很有可能导致内存溢出，因此 Flink 该提供了其它方式来存储状态数据，这些存储方式统一称为状态后端 (或状态管理器)：</p>
<div align="center"> <img src="../pictures/flink-checkpoints-backend.png"> </div>



<p>主要有以下三种：</p>
<h4 id="1-MemoryStateBackend"><a href="#1-MemoryStateBackend" class="headerlink" title="1. MemoryStateBackend"></a>1. MemoryStateBackend</h4><p>默认的方式，即基于 JVM 的堆内存进行存储，主要适用于本地开发和调试。</p>
<h4 id="2-FsStateBackend"><a href="#2-FsStateBackend" class="headerlink" title="2. FsStateBackend"></a>2. FsStateBackend</h4><p>基于文件系统进行存储，可以是本地文件系统，也可以是 HDFS 等分布式文件系统。 需要注意而是虽然选择使用了 FsStateBackend ，但正在进行的数据仍然是存储在 TaskManager 的内存中的，只有在 checkpoint 时，才会将状态快照写入到指定文件系统上。</p>
<h4 id="3-RocksDBStateBackend"><a href="#3-RocksDBStateBackend" class="headerlink" title="3. RocksDBStateBackend"></a>3. RocksDBStateBackend</h4><p>RocksDBStateBackend 是 Flink 内置的第三方状态管理器，采用嵌入式的 key-value 型数据库 RocksDB 来存储正在进行的数据。等到 checkpoint 时，再将其中的数据持久化到指定的文件系统中，所以采用 RocksDBStateBackend 时也需要配置持久化存储的文件系统。之所以这样做是因为 RocksDB 作为嵌入式数据库安全性比较低，但比起全文件系统的方式，其读取速率更快；比起全内存的方式，其存储空间更大，因此它是一种比较均衡的方案。</p>
<h3 id="4-2-配置方式"><a href="#4-2-配置方式" class="headerlink" title="4.2 配置方式"></a>4.2 配置方式</h3><p>Flink 支持使用两种方式来配置后端管理器：</p>
<p><strong>第一种方式</strong>：基于代码方式进行配置，只对当前作业生效：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 配置 FsStateBackend</span></span><br><span class="line">env.setStateBackend(<span class="keyword">new</span> FsStateBackend(<span class="string">"hdfs://namenode:40010/flink/checkpoints"</span>));</span><br><span class="line"><span class="comment">// 配置 RocksDBStateBackend</span></span><br><span class="line">env.setStateBackend(<span class="keyword">new</span> RocksDBStateBackend(<span class="string">"hdfs://namenode:40010/flink/checkpoints"</span>));</span><br></pre></td></tr></table></figure>
<p>配置 RocksDBStateBackend 时，需要额外导入下面的依赖：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-statebackend-rocksdb_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.9.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p><strong>第二种方式</strong>：基于 <code>flink-conf.yaml</code> 配置文件的方式进行配置，对所有部署在该集群上的作业都生效：</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">state.backend:</span> <span class="string">filesystem</span></span><br><span class="line"><span class="attr">state.checkpoints.dir:</span> <span class="string">hdfs://namenode:40010/flink/checkpoints</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>注：本篇文章所有示例代码下载地址：<a href="https://github.com/heibaiying/BigData-Notes/tree/master/code/Flink/flink-state-management" target="_blank" rel="noopener">flink-state-management</a></p>
</blockquote>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/stream/state/state.html" target="_blank" rel="noopener">Working with State</a></li>
<li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/stream/state/checkpointing.html" target="_blank" rel="noopener">Checkpointing</a></li>
<li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.9/ops/state/savepoints.html#savepoints" target="_blank" rel="noopener">Savepoints</a></li>
<li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.9/ops/state/state_backends.html" target="_blank" rel="noopener">State Backends</a></li>
<li>Fabian Hueske , Vasiliki Kalavri . 《Stream Processing with Apache Flink》.  O’Reilly Media .  2019-4-30 </li>
</ul>
]]></content>
      <categories>
        <category>notes</category>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Flume 简介及基本使用</title>
    <url>/2021/03/17/Flume%E7%AE%80%E4%BB%8B%E5%8F%8A%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<h2 id="一、Flume简介"><a href="#一、Flume简介" class="headerlink" title="一、Flume简介"></a>一、Flume简介</h2><p>Apache Flume 是一个分布式，高可用的数据收集系统。它可以从不同的数据源收集数据，经过聚合后发送到存储系统中，通常用于日志数据的收集。Flume 分为 NG 和 OG (1.0 之前) 两个版本，NG 在 OG 的基础上进行了完全的重构，是目前使用最为广泛的版本。下面的介绍均以 NG 为基础。</p>
<h2 id="二、Flume架构和基本概念"><a href="#二、Flume架构和基本概念" class="headerlink" title="二、Flume架构和基本概念"></a>二、Flume架构和基本概念</h2><p>下图为 Flume 的基本架构图：</p>
<div align="center"> <img src="../pictures/flume-architecture.png"> </div>

<h3 id="2-1-基本架构"><a href="#2-1-基本架构" class="headerlink" title="2.1 基本架构"></a>2.1 基本架构</h3><p>外部数据源以特定格式向 Flume 发送 <code>events</code> (事件)，当 <code>source</code> 接收到 <code>events</code> 时，它将其存储到一个或多个 <code>channel</code>，<code>channe</code> 会一直保存 <code>events</code> 直到它被 <code>sink</code> 所消费。<code>sink</code> 的主要功能从 <code>channel</code> 中读取 <code>events</code>，并将其存入外部存储系统或转发到下一个 <code>source</code>，成功后再从 <code>channel</code> 中移除 <code>events</code>。</p>
<h3 id="2-2-基本概念"><a href="#2-2-基本概念" class="headerlink" title="2.2 基本概念"></a>2.2 基本概念</h3><p><strong>1. Event</strong></p>
<p><code>Event</code> 是 Flume NG 数据传输的基本单元。类似于 JMS 和消息系统中的消息。一个 <code>Event</code> 由标题和正文组成：前者是键/值映射，后者是任意字节数组。</p>
<p><strong>2. Source</strong> </p>
<p>数据收集组件，从外部数据源收集数据，并存储到 Channel 中。</p>
<p><strong>3. Channel</strong></p>
<p><code>Channel</code> 是源和接收器之间的管道，用于临时存储数据。可以是内存或持久化的文件系统：</p>
<ul>
<li><code>Memory Channel</code> : 使用内存，优点是速度快，但数据可能会丢失 (如突然宕机)；</li>
<li><code>File Channel</code> : 使用持久化的文件系统，优点是能保证数据不丢失，但是速度慢。</li>
</ul>
<p><strong>4. Sink</strong></p>
<p><code>Sink</code> 的主要功能从 <code>Channel</code> 中读取 <code>Event</code>，并将其存入外部存储系统或将其转发到下一个 <code>Source</code>，成功后再从 <code>Channel</code> 中移除 <code>Event</code>。</p>
<p><strong>5. Agent</strong></p>
<p>是一个独立的 (JVM) 进程，包含 <code>Source</code>、 <code>Channel</code>、 <code>Sink</code> 等组件。</p>
<h3 id="2-3-组件种类"><a href="#2-3-组件种类" class="headerlink" title="2.3 组件种类"></a>2.3 组件种类</h3><p>Flume 中的每一个组件都提供了丰富的类型，适用于不同场景：</p>
<ul>
<li><p>Source 类型 ：内置了几十种类型，如 <code>Avro Source</code>，<code>Thrift Source</code>，<code>Kafka Source</code>，<code>JMS Source</code>；</p>
</li>
<li><p>Sink 类型 ：<code>HDFS Sink</code>，<code>Hive Sink</code>，<code>HBaseSinks</code>，<code>Avro Sink</code> 等；</p>
</li>
<li><p>Channel 类型 ：<code>Memory Channel</code>，<code>JDBC Channel</code>，<code>Kafka Channel</code>，<code>File Channel</code> 等。</p>
</li>
</ul>
<p>对于 Flume 的使用，除非有特别的需求，否则通过组合内置的各种类型的 Source，Sink 和 Channel 就能满足大多数的需求。在 <a href="http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html" target="_blank" rel="noopener">Flume 官网</a> 上对所有类型组件的配置参数均以表格的方式做了详尽的介绍，并附有配置样例；同时不同版本的参数可能略有所不同，所以使用时建议选取官网对应版本的 User Guide 作为主要参考资料。</p>
<h2 id="三、Flume架构模式"><a href="#三、Flume架构模式" class="headerlink" title="三、Flume架构模式"></a>三、Flume架构模式</h2><p>Flume 支持多种架构模式，分别介绍如下</p>
<h3 id="3-1-multi-agent-flow"><a href="#3-1-multi-agent-flow" class="headerlink" title="3.1 multi-agent flow"></a>3.1 multi-agent flow</h3><div align="center"> <img src="../pictures/flume-multi-agent-flow.png"> </div>

<p><br></p>
<p>Flume 支持跨越多个 Agent 的数据传递，这要求前一个 Agent 的 Sink 和下一个 Agent 的 Source 都必须是 <code>Avro</code> 类型，Sink 指向 Source 所在主机名 (或 IP 地址) 和端口（详细配置见下文案例三）。</p>
<h3 id="3-2-Consolidation"><a href="#3-2-Consolidation" class="headerlink" title="3.2 Consolidation"></a>3.2 Consolidation</h3><div align="center"> <img src="../pictures/flume-consolidation.png"> </div>



<p><br></p>
<p>日志收集中常常存在大量的客户端（比如分布式 web 服务），Flume 支持使用多个 Agent 分别收集日志，然后通过一个或者多个 Agent 聚合后再存储到文件系统中。</p>
<h3 id="3-3-Multiplexing-the-flow"><a href="#3-3-Multiplexing-the-flow" class="headerlink" title="3.3 Multiplexing the flow"></a>3.3 Multiplexing the flow</h3><div align="center"> <img src="../pictures/flume-multiplexing-the-flow.png"> </div>

<p>Flume 支持从一个 Source 向多个 Channel，也就是向多个 Sink 传递事件，这个操作称之为 <code>Fan Out</code>(扇出)。默认情况下 <code>Fan Out</code> 是向所有的 Channel 复制 <code>Event</code>，即所有 Channel 收到的数据都是相同的。同时 Flume 也支持在 <code>Source</code> 上自定义一个复用选择器 (multiplexing selector) 来实现自定义的路由规则。</p>
<h2 id="四、Flume配置格式"><a href="#四、Flume配置格式" class="headerlink" title="四、Flume配置格式"></a>四、Flume配置格式</h2><p>Flume 配置通常需要以下两个步骤：</p>
<ol>
<li>分别定义好 Agent 的 Sources，Sinks，Channels，然后将 Sources 和 Sinks 与通道进行绑定。需要注意的是一个 Source 可以配置多个 Channel，但一个 Sink 只能配置一个 Channel。基本格式如下：</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">&lt;Agent&gt;.sources = &lt;Source&gt;</span><br><span class="line">&lt;Agent&gt;.sinks = &lt;Sink&gt;</span><br><span class="line">&lt;Agent&gt;.channels = &lt;Channel1&gt; &lt;Channel2&gt;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">set</span> channel <span class="keyword">for</span> <span class="built_in">source</span></span></span><br><span class="line">&lt;Agent&gt;.sources.&lt;Source&gt;.channels = &lt;Channel1&gt; &lt;Channel2&gt; ...</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">set</span> channel <span class="keyword">for</span> sink</span></span><br><span class="line">&lt;Agent&gt;.sinks.&lt;Sink&gt;.channel = &lt;Channel1&gt;</span><br></pre></td></tr></table></figure>
<ol start="2">
<li>分别定义 Source，Sink，Channel 的具体属性。基本格式如下：</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">&lt;Agent&gt;.sources.&lt;Source&gt;.&lt;someProperty&gt; = &lt;someValue&gt;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> properties <span class="keyword">for</span> channels</span></span><br><span class="line">&lt;Agent&gt;.channel.&lt;Channel&gt;.&lt;someProperty&gt; = &lt;someValue&gt;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> properties <span class="keyword">for</span> sinks</span></span><br><span class="line">&lt;Agent&gt;.sources.&lt;Sink&gt;.&lt;someProperty&gt; = &lt;someValue&gt;</span><br></pre></td></tr></table></figure>
<h2 id="五、Flume的安装部署"><a href="#五、Flume的安装部署" class="headerlink" title="五、Flume的安装部署"></a>五、Flume的安装部署</h2><p>为方便大家后期查阅，本仓库中所有软件的安装均单独成篇，Flume 的安装见：</p>
<p><a href="https://github.com/heibaiying/BigData-Notes/blob/master/notes/installation/Linux%E4%B8%8BFlume%E7%9A%84%E5%AE%89%E8%A3%85.md" target="_blank" rel="noopener">Linux 环境下 Flume 的安装部署</a></p>
<h2 id="六、Flume使用案例"><a href="#六、Flume使用案例" class="headerlink" title="六、Flume使用案例"></a>六、Flume使用案例</h2><p>介绍几个 Flume 的使用案例：</p>
<ul>
<li>案例一：使用 Flume 监听文件内容变动，将新增加的内容输出到控制台。</li>
<li>案例二：使用 Flume 监听指定目录，将目录下新增加的文件存储到 HDFS。</li>
<li>案例三：使用 Avro 将本服务器收集到的日志数据发送到另外一台服务器。</li>
</ul>
<h3 id="6-1-案例一"><a href="#6-1-案例一" class="headerlink" title="6.1 案例一"></a>6.1 案例一</h3><p>需求： 监听文件内容变动，将新增加的内容输出到控制台。</p>
<p>实现： 主要使用 <code>Exec Source</code> 配合 <code>tail</code> 命令实现。</p>
<h4 id="1-配置"><a href="#1-配置" class="headerlink" title="1. 配置"></a>1. 配置</h4><p>新建配置文件 <code>exec-memory-logger.properties</code>,其内容如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#指定agent的sources,sinks,channels</span><br><span class="line">a1.sources = s1  </span><br><span class="line">a1.sinks = k1  </span><br><span class="line">a1.channels = c1  </span><br><span class="line">   </span><br><span class="line">#配置sources属性</span><br><span class="line">a1.sources.s1.type = exec</span><br><span class="line">a1.sources.s1.command = tail -F /tmp/log.txt</span><br><span class="line">a1.sources.s1.shell = /bin/bash -c</span><br><span class="line"></span><br><span class="line">#将sources与channels进行绑定</span><br><span class="line">a1.sources.s1.channels = c1</span><br><span class="line">   </span><br><span class="line">#配置sink </span><br><span class="line">a1.sinks.k1.type = logger</span><br><span class="line"></span><br><span class="line">#将sinks与channels进行绑定  </span><br><span class="line">a1.sinks.k1.channel = c1  </span><br><span class="line">   </span><br><span class="line">#配置channel类型</span><br><span class="line">a1.channels.c1.type = memory</span><br></pre></td></tr></table></figure>
<h4 id="2-启动"><a href="#2-启动" class="headerlink" title="2. 启动　"></a>2. 启动　</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">flume-ng agent \</span><br><span class="line">--conf conf \</span><br><span class="line">--conf-file /usr/app/apache-flume-1.6.0-cdh5.15.2-bin/examples/exec-memory-logger.properties \</span><br><span class="line">--name a1 \</span><br><span class="line">-Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure>
<h4 id="3-测试"><a href="#3-测试" class="headerlink" title="3. 测试"></a>3. 测试</h4><p>向文件中追加数据：</p>
<div align="center"> <img src="../pictures/flume-example-1.png"> </div>

<p>控制台的显示：</p>
<div align="center"> <img src="../pictures/flume-example-2.png"> </div>



<h3 id="6-2-案例二"><a href="#6-2-案例二" class="headerlink" title="6.2 案例二"></a>6.2 案例二</h3><p>需求： 监听指定目录，将目录下新增加的文件存储到 HDFS。</p>
<p>实现：使用 <code>Spooling Directory Source</code> 和 <code>HDFS Sink</code>。</p>
<h4 id="1-配置-1"><a href="#1-配置-1" class="headerlink" title="1. 配置"></a>1. 配置</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#指定agent的sources,sinks,channels</span><br><span class="line">a1.sources = s1  </span><br><span class="line">a1.sinks = k1  </span><br><span class="line">a1.channels = c1  </span><br><span class="line">   </span><br><span class="line">#配置sources属性</span><br><span class="line">a1.sources.s1.type =spooldir  </span><br><span class="line">a1.sources.s1.spoolDir =/tmp/logs</span><br><span class="line">a1.sources.s1.basenameHeader = true</span><br><span class="line">a1.sources.s1.basenameHeaderKey = fileName </span><br><span class="line">#将sources与channels进行绑定  </span><br><span class="line">a1.sources.s1.channels =c1 </span><br><span class="line"></span><br><span class="line">   </span><br><span class="line">#配置sink </span><br><span class="line">a1.sinks.k1.type = hdfs</span><br><span class="line">a1.sinks.k1.hdfs.path = /flume/events/%y-%m-%d/%H/</span><br><span class="line">a1.sinks.k1.hdfs.filePrefix = %&#123;fileName&#125;</span><br><span class="line">#生成的文件类型，默认是Sequencefile，可用DataStream，则为普通文本</span><br><span class="line">a1.sinks.k1.hdfs.fileType = DataStream  </span><br><span class="line">a1.sinks.k1.hdfs.useLocalTimeStamp = true</span><br><span class="line">#将sinks与channels进行绑定  </span><br><span class="line">a1.sinks.k1.channel = c1</span><br><span class="line">   </span><br><span class="line">#配置channel类型</span><br><span class="line">a1.channels.c1.type = memory</span><br></pre></td></tr></table></figure>
<h4 id="2-启动-1"><a href="#2-启动-1" class="headerlink" title="2. 启动"></a>2. 启动</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">flume-ng agent \</span><br><span class="line">--conf conf \</span><br><span class="line">--conf-file /usr/app/apache-flume-1.6.0-cdh5.15.2-bin/examples/spooling-memory-hdfs.properties \</span><br><span class="line">--name a1 -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure>
<h4 id="3-测试-1"><a href="#3-测试-1" class="headerlink" title="3. 测试"></a>3. 测试</h4><p>拷贝任意文件到监听目录下，可以从日志看到文件上传到 HDFS 的路径：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> cp log.txt logs/</span></span><br></pre></td></tr></table></figure>
<div align="center"> <img src="../pictures/flume-example-3.png"> </div>

<p>查看上传到 HDFS 上的文件内容与本地是否一致：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> hdfs dfs -cat /flume/events/19-04-09/13/log.txt.1554788567801</span></span><br></pre></td></tr></table></figure>
<div align="center"> <img src="../pictures/flume-example-4.png"> </div>



<h3 id="6-3-案例三"><a href="#6-3-案例三" class="headerlink" title="6.3 案例三"></a>6.3 案例三</h3><p>需求： 将本服务器收集到的数据发送到另外一台服务器。</p>
<p>实现：使用 <code>avro sources</code> 和 <code>avro Sink</code> 实现。</p>
<h4 id="1-配置日志收集Flume"><a href="#1-配置日志收集Flume" class="headerlink" title="1. 配置日志收集Flume"></a>1. 配置日志收集Flume</h4><p>新建配置 <code>netcat-memory-avro.properties</code>，监听文件内容变化，然后将新的文件内容通过 <code>avro sink</code> 发送到 hadoop001 这台服务器的 8888 端口：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#指定agent的sources,sinks,channels</span><br><span class="line">a1.sources = s1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line">#配置sources属性</span><br><span class="line">a1.sources.s1.type = exec</span><br><span class="line">a1.sources.s1.command = tail -F /tmp/log.txt</span><br><span class="line">a1.sources.s1.shell = /bin/bash -c</span><br><span class="line">a1.sources.s1.channels = c1</span><br><span class="line"></span><br><span class="line">#配置sink</span><br><span class="line">a1.sinks.k1.type = avro</span><br><span class="line">a1.sinks.k1.hostname = hadoop001</span><br><span class="line">a1.sinks.k1.port = 8888</span><br><span class="line">a1.sinks.k1.batch-size = 1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br><span class="line"></span><br><span class="line">#配置channel类型</span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br></pre></td></tr></table></figure>
<h4 id="2-配置日志聚合Flume"><a href="#2-配置日志聚合Flume" class="headerlink" title="2. 配置日志聚合Flume"></a>2. 配置日志聚合Flume</h4><p>使用 <code>avro source</code> 监听 hadoop001 服务器的 8888 端口，将获取到内容输出到控制台：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#指定agent的sources,sinks,channels</span><br><span class="line">a2.sources = s2</span><br><span class="line">a2.sinks = k2</span><br><span class="line">a2.channels = c2</span><br><span class="line"></span><br><span class="line">#配置sources属性</span><br><span class="line">a2.sources.s2.type = avro</span><br><span class="line">a2.sources.s2.bind = hadoop001</span><br><span class="line">a2.sources.s2.port = 8888</span><br><span class="line"></span><br><span class="line">#将sources与channels进行绑定</span><br><span class="line">a2.sources.s2.channels = c2</span><br><span class="line"></span><br><span class="line">#配置sink</span><br><span class="line">a2.sinks.k2.type = logger</span><br><span class="line"></span><br><span class="line">#将sinks与channels进行绑定</span><br><span class="line">a2.sinks.k2.channel = c2</span><br><span class="line"></span><br><span class="line">#配置channel类型</span><br><span class="line">a2.channels.c2.type = memory</span><br><span class="line">a2.channels.c2.capacity = 1000</span><br><span class="line">a2.channels.c2.transactionCapacity = 100</span><br></pre></td></tr></table></figure>
<h4 id="3-启动"><a href="#3-启动" class="headerlink" title="3. 启动"></a>3. 启动</h4><p>启动日志聚集 Flume：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">flume-ng agent \</span><br><span class="line">--conf conf \</span><br><span class="line">--conf-file /usr/app/apache-flume-1.6.0-cdh5.15.2-bin/examples/avro-memory-logger.properties \</span><br><span class="line">--name a2 -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure>
<p>在启动日志收集 Flume:</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">flume-ng agent \</span><br><span class="line">--conf conf \</span><br><span class="line">--conf-file /usr/app/apache-flume-1.6.0-cdh5.15.2-bin/examples/netcat-memory-avro.properties \</span><br><span class="line">--name a1 -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure>
<p>这里建议按以上顺序启动，原因是 <code>avro.source</code> 会先与端口进行绑定，这样 <code>avro sink</code> 连接时才不会报无法连接的异常。但是即使不按顺序启动也是没关系的，<code>sink</code> 会一直重试，直至建立好连接。</p>
<div align="center"> <img src="../pictures/flume-retry.png"> </div>

<h4 id="4-测试"><a href="#4-测试" class="headerlink" title="4.测试"></a>4.测试</h4><p>向文件 <code>tmp/log.txt</code> 中追加内容：</p>
<div align="center"> <img src="../pictures/flume-example-8.png"> </div>

<p>可以看到已经从 8888 端口监听到内容，并成功输出到控制台：</p>
<div align="center"> <img src="../pictures/flume-example-9.png"> </div>
]]></content>
      <categories>
        <category>notes</category>
        <category>Flume</category>
      </categories>
      <tags>
        <tag>Flume</tag>
      </tags>
  </entry>
  <entry>
    <title>HDFS Java API</title>
    <url>/2021/03/17/HDFS-Java-API/</url>
    <content><![CDATA[<h2 id="一、-简介"><a href="#一、-简介" class="headerlink" title="一、 简介"></a>一、 简介</h2><p>想要使用 HDFS API，需要导入依赖 <code>hadoop-client</code>。如果是 CDH 版本的 Hadoop，还需要额外指明其仓库地址：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version="1.0" encoding="UTF-8"?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">project</span> <span class="attr">xmlns</span>=<span class="string">"http://maven.apache.org/POM/4.0.0"</span></span></span><br><span class="line"><span class="tag">         <span class="attr">xmlns:xsi</span>=<span class="string">"http://www.w3.org/2001/XMLSchema-instance"</span></span></span><br><span class="line"><span class="tag">         <span class="attr">xsi:schemaLocation</span>=<span class="string">"http://maven.apache.org/POM/4.0.0 </span></span></span><br><span class="line"><span class="tag"><span class="string">         http://maven.apache.org/xsd/maven-4.0.0.xsd"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">modelVersion</span>&gt;</span>4.0.0<span class="tag">&lt;/<span class="name">modelVersion</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.heibaiying<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hdfs-java-api<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">properties</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">project.build.sourceEncoding</span>&gt;</span>UTF-8<span class="tag">&lt;/<span class="name">project.build.sourceEncoding</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">hadoop.version</span>&gt;</span>2.6.0-cdh5.15.2<span class="tag">&lt;/<span class="name">hadoop.version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">properties</span>&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!---配置 CDH 仓库地址--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">repositories</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">repository</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">id</span>&gt;</span>cloudera<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">url</span>&gt;</span>https://repository.cloudera.com/artifactory/cloudera-repos/<span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">repository</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">repositories</span>&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--Hadoop-client--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;hadoop.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>4.12<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">scope</span>&gt;</span>test<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">project</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h2 id="二、API的使用"><a href="#二、API的使用" class="headerlink" title="二、API的使用"></a>二、API的使用</h2><h3 id="2-1-FileSystem"><a href="#2-1-FileSystem" class="headerlink" title="2.1 FileSystem"></a>2.1 FileSystem</h3><p>FileSystem 是所有 HDFS 操作的主入口。由于之后的每个单元测试都需要用到它，这里使用 <code>@Before</code> 注解进行标注。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String HDFS_PATH = <span class="string">"hdfs://192.168.0.106:8020"</span>;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String HDFS_USER = <span class="string">"root"</span>;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> FileSystem fileSystem;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Before</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">prepare</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">        <span class="comment">// 这里我启动的是单节点的 Hadoop,所以副本系数设置为 1,默认值为 3</span></span><br><span class="line">        configuration.set(<span class="string">"dfs.replication"</span>, <span class="string">"1"</span>);</span><br><span class="line">        fileSystem = FileSystem.get(<span class="keyword">new</span> URI(HDFS_PATH), configuration, HDFS_USER);</span><br><span class="line">    &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    &#125; <span class="keyword">catch</span> (URISyntaxException e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@After</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">destroy</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    fileSystem = <span class="keyword">null</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="2-2-创建目录"><a href="#2-2-创建目录" class="headerlink" title="2.2 创建目录"></a>2.2 创建目录</h3><p>支持递归创建目录：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">mkDir</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    fileSystem.mkdirs(<span class="keyword">new</span> Path(<span class="string">"/hdfs-api/test0/"</span>));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="2-3-创建指定权限的目录"><a href="#2-3-创建指定权限的目录" class="headerlink" title="2.3 创建指定权限的目录"></a>2.3 创建指定权限的目录</h3><p><code>FsPermission(FsAction u, FsAction g, FsAction o)</code> 的三个参数分别对应：创建者权限，同组其他用户权限，其他用户权限，权限值定义在 <code>FsAction</code> 枚举类中。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">mkDirWithPermission</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    fileSystem.mkdirs(<span class="keyword">new</span> Path(<span class="string">"/hdfs-api/test1/"</span>),</span><br><span class="line">            <span class="keyword">new</span> FsPermission(FsAction.READ_WRITE, FsAction.READ, FsAction.READ));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="2-4-创建文件，并写入内容"><a href="#2-4-创建文件，并写入内容" class="headerlink" title="2.4 创建文件，并写入内容"></a>2.4 创建文件，并写入内容</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">create</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    <span class="comment">// 如果文件存在，默认会覆盖, 可以通过第二个参数进行控制。第三个参数可以控制使用缓冲区的大小</span></span><br><span class="line">    FSDataOutputStream out = fileSystem.create(<span class="keyword">new</span> Path(<span class="string">"/hdfs-api/test/a.txt"</span>),</span><br><span class="line">                                               <span class="keyword">true</span>, <span class="number">4096</span>);</span><br><span class="line">    out.write(<span class="string">"hello hadoop!"</span>.getBytes());</span><br><span class="line">    out.write(<span class="string">"hello spark!"</span>.getBytes());</span><br><span class="line">    out.write(<span class="string">"hello flink!"</span>.getBytes());</span><br><span class="line">    <span class="comment">// 强制将缓冲区中内容刷出</span></span><br><span class="line">    out.flush();</span><br><span class="line">    out.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="2-5-判断文件是否存在"><a href="#2-5-判断文件是否存在" class="headerlink" title="2.5 判断文件是否存在"></a>2.5 判断文件是否存在</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">exist</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    <span class="keyword">boolean</span> exists = fileSystem.exists(<span class="keyword">new</span> Path(<span class="string">"/hdfs-api/test/a.txt"</span>));</span><br><span class="line">    System.out.println(exists);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="2-6-查看文件内容"><a href="#2-6-查看文件内容" class="headerlink" title="2.6 查看文件内容"></a>2.6 查看文件内容</h3><p>查看小文本文件的内容，直接转换成字符串后输出：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readToString</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    FSDataInputStream inputStream = fileSystem.open(<span class="keyword">new</span> Path(<span class="string">"/hdfs-api/test/a.txt"</span>));</span><br><span class="line">    String context = inputStreamToString(inputStream, <span class="string">"utf-8"</span>);</span><br><span class="line">    System.out.println(context);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>inputStreamToString</code> 是一个自定义方法，代码如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 把输入流转换为指定编码的字符</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> inputStream 输入流</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> encode      指定编码类型</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> String <span class="title">inputStreamToString</span><span class="params">(InputStream inputStream, String encode)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="keyword">if</span> (encode == <span class="keyword">null</span> || (<span class="string">""</span>.equals(encode))) &#123;</span><br><span class="line">            encode = <span class="string">"utf-8"</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        BufferedReader reader = <span class="keyword">new</span> BufferedReader(<span class="keyword">new</span> InputStreamReader(inputStream, encode));</span><br><span class="line">        StringBuilder builder = <span class="keyword">new</span> StringBuilder();</span><br><span class="line">        String str = <span class="string">""</span>;</span><br><span class="line">        <span class="keyword">while</span> ((str = reader.readLine()) != <span class="keyword">null</span>) &#123;</span><br><span class="line">            builder.append(str).append(<span class="string">"\n"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> builder.toString();</span><br><span class="line">    &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="2-7-文件重命名"><a href="#2-7-文件重命名" class="headerlink" title="2.7 文件重命名"></a>2.7 文件重命名</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">rename</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    Path oldPath = <span class="keyword">new</span> Path(<span class="string">"/hdfs-api/test/a.txt"</span>);</span><br><span class="line">    Path newPath = <span class="keyword">new</span> Path(<span class="string">"/hdfs-api/test/b.txt"</span>);</span><br><span class="line">    <span class="keyword">boolean</span> result = fileSystem.rename(oldPath, newPath);</span><br><span class="line">    System.out.println(result);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="2-8-删除目录或文件"><a href="#2-8-删除目录或文件" class="headerlink" title="2.8 删除目录或文件"></a>2.8 删除目录或文件</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">delete</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">     *  第二个参数代表是否递归删除</span></span><br><span class="line"><span class="comment">     *    +  如果 path 是一个目录且递归删除为 true, 则删除该目录及其中所有文件;</span></span><br><span class="line"><span class="comment">     *    +  如果 path 是一个目录但递归删除为 false,则会则抛出异常。</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">boolean</span> result = fileSystem.delete(<span class="keyword">new</span> Path(<span class="string">"/hdfs-api/test/b.txt"</span>), <span class="keyword">true</span>);</span><br><span class="line">    System.out.println(result);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="2-9-上传文件到HDFS"><a href="#2-9-上传文件到HDFS" class="headerlink" title="2.9 上传文件到HDFS"></a>2.9 上传文件到HDFS</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">copyFromLocalFile</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    <span class="comment">// 如果指定的是目录，则会把目录及其中的文件都复制到指定目录下</span></span><br><span class="line">    Path src = <span class="keyword">new</span> Path(<span class="string">"D:\\BigData-Notes\\notes\\installation"</span>);</span><br><span class="line">    Path dst = <span class="keyword">new</span> Path(<span class="string">"/hdfs-api/test/"</span>);</span><br><span class="line">    fileSystem.copyFromLocalFile(src, dst);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="2-10-上传大文件并显示上传进度"><a href="#2-10-上传大文件并显示上传进度" class="headerlink" title="2.10 上传大文件并显示上传进度"></a>2.10 上传大文件并显示上传进度</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">copyFromLocalBigFile</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        File file = <span class="keyword">new</span> File(<span class="string">"D:\\kafka.tgz"</span>);</span><br><span class="line">        <span class="keyword">final</span> <span class="keyword">float</span> fileSize = file.length();</span><br><span class="line">        InputStream in = <span class="keyword">new</span> BufferedInputStream(<span class="keyword">new</span> FileInputStream(file));</span><br><span class="line"></span><br><span class="line">        FSDataOutputStream out = fileSystem.create(<span class="keyword">new</span> Path(<span class="string">"/hdfs-api/test/kafka5.tgz"</span>),</span><br><span class="line">                <span class="keyword">new</span> Progressable() &#123;</span><br><span class="line">                  <span class="keyword">long</span> fileCount = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">                  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">progress</span><span class="params">()</span> </span>&#123;</span><br><span class="line">                     fileCount++;</span><br><span class="line">                     <span class="comment">// progress 方法每上传大约 64KB 的数据后就会被调用一次</span></span><br><span class="line">                     System.out.println(<span class="string">"上传进度："</span> + (fileCount * <span class="number">64</span> * <span class="number">1024</span> / fileSize) * <span class="number">100</span> + <span class="string">" %"</span>);</span><br><span class="line">                   &#125;</span><br><span class="line">                &#125;);</span><br><span class="line"></span><br><span class="line">        IOUtils.copyBytes(in, out, <span class="number">4096</span>);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<h3 id="2-11-从HDFS上下载文件"><a href="#2-11-从HDFS上下载文件" class="headerlink" title="2.11 从HDFS上下载文件"></a>2.11 从HDFS上下载文件</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">copyToLocalFile</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    Path src = <span class="keyword">new</span> Path(<span class="string">"/hdfs-api/test/kafka.tgz"</span>);</span><br><span class="line">    Path dst = <span class="keyword">new</span> Path(<span class="string">"D:\\app\\"</span>);</span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">     * 第一个参数控制下载完成后是否删除源文件,默认是 true,即删除;</span></span><br><span class="line"><span class="comment">     * 最后一个参数表示是否将 RawLocalFileSystem 用作本地文件系统;</span></span><br><span class="line"><span class="comment">     * RawLocalFileSystem 默认为 false,通常情况下可以不设置,</span></span><br><span class="line"><span class="comment">     * 但如果你在执行时候抛出 NullPointerException 异常,则代表你的文件系统与程序可能存在不兼容的情况 (window 下常见),</span></span><br><span class="line"><span class="comment">     * 此时可以将 RawLocalFileSystem 设置为 true</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    fileSystem.copyToLocalFile(<span class="keyword">false</span>, src, dst, <span class="keyword">true</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="2-12-查看指定目录下所有文件的信息"><a href="#2-12-查看指定目录下所有文件的信息" class="headerlink" title="2.12 查看指定目录下所有文件的信息"></a>2.12 查看指定目录下所有文件的信息</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">listFiles</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    FileStatus[] statuses = fileSystem.listStatus(<span class="keyword">new</span> Path(<span class="string">"/hdfs-api"</span>));</span><br><span class="line">    <span class="keyword">for</span> (FileStatus fileStatus : statuses) &#123;</span><br><span class="line">        <span class="comment">//fileStatus 的 toString 方法被重写过，直接打印可以看到所有信息</span></span><br><span class="line">        System.out.println(fileStatus.toString());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>FileStatus</code> 中包含了文件的基本信息，比如文件路径，是否是文件夹，修改时间，访问时间，所有者，所属组，文件权限，是否是符号链接等，输出内容示例如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">FileStatus&#123;</span><br><span class="line">path=hdfs://192.168.0.106:8020/hdfs-api/test; </span><br><span class="line">isDirectory=true; </span><br><span class="line">modification_time=1556680796191; </span><br><span class="line">access_time=0; </span><br><span class="line">owner=root; </span><br><span class="line">group=supergroup; </span><br><span class="line">permission=rwxr-xr-x; </span><br><span class="line">isSymlink=false</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="2-13-递归查看指定目录下所有文件的信息"><a href="#2-13-递归查看指定目录下所有文件的信息" class="headerlink" title="2.13 递归查看指定目录下所有文件的信息"></a>2.13 递归查看指定目录下所有文件的信息</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">listFilesRecursive</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    RemoteIterator&lt;LocatedFileStatus&gt; files = fileSystem.listFiles(<span class="keyword">new</span> Path(<span class="string">"/hbase"</span>), <span class="keyword">true</span>);</span><br><span class="line">    <span class="keyword">while</span> (files.hasNext()) &#123;</span><br><span class="line">        System.out.println(files.next());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>和上面输出类似，只是多了文本大小，副本系数，块大小信息。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">LocatedFileStatus&#123;</span><br><span class="line">path=hdfs://192.168.0.106:8020/hbase/hbase.version; </span><br><span class="line">isDirectory=false; </span><br><span class="line">length=7; </span><br><span class="line">replication=1; </span><br><span class="line">blocksize=134217728; </span><br><span class="line">modification_time=1554129052916; </span><br><span class="line">access_time=1554902661455; </span><br><span class="line">owner=root; group=supergroup;</span><br><span class="line">permission=rw-r--r--; </span><br><span class="line">isSymlink=false&#125;</span><br></pre></td></tr></table></figure>
<h3 id="2-14-查看文件的块信息"><a href="#2-14-查看文件的块信息" class="headerlink" title="2.14 查看文件的块信息"></a>2.14 查看文件的块信息</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">getFileBlockLocations</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">    FileStatus fileStatus = fileSystem.getFileStatus(<span class="keyword">new</span> Path(<span class="string">"/hdfs-api/test/kafka.tgz"</span>));</span><br><span class="line">    BlockLocation[] blocks = fileSystem.getFileBlockLocations(fileStatus, <span class="number">0</span>, fileStatus.getLen());</span><br><span class="line">    <span class="keyword">for</span> (BlockLocation block : blocks) &#123;</span><br><span class="line">        System.out.println(block);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>块输出信息有三个值，分别是文件的起始偏移量 (offset)，文件大小 (length)，块所在的主机名 (hosts)。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">0,57028557,hadoop001</span><br></pre></td></tr></table></figure>
<p>这里我上传的文件只有 57M(小于 128M)，且程序中设置了副本系数为 1，所有只有一个块信息。</p>
<p><br></p>
<p><br></p>
<p><strong>以上所有测试用例下载地址</strong>：<a href="https://github.com/heibaiying/BigData-Notes/tree/master/code/Hadoop/hdfs-java-api" target="_blank" rel="noopener">HDFS Java API</a></p>
]]></content>
      <categories>
        <category>notes</category>
        <category>HDFS</category>
      </categories>
      <tags>
        <tag>HDFS</tag>
      </tags>
  </entry>
  <entry>
    <title>Flume 整合 Kafka</title>
    <url>/2021/03/17/Flume%E6%95%B4%E5%90%88Kafka/</url>
    <content><![CDATA[<h2 id="一、背景"><a href="#一、背景" class="headerlink" title="一、背景"></a>一、背景</h2><p>先说一下，为什么要使用 Flume + Kafka？</p>
<p>以实时流处理项目为例，由于采集的数据量可能存在峰值和峰谷，假设是一个电商项目，那么峰值通常出现在秒杀时，这时如果直接将 Flume 聚合后的数据输入到 Storm 等分布式计算框架中，可能就会超过集群的处理能力，这时采用 Kafka 就可以起到削峰的作用。Kafka 天生为大数据场景而设计，具有高吞吐的特性，能很好地抗住峰值数据的冲击。</p>
<div align="center"> <img src="../pictures/flume-kafka.png"> </div>



<h2 id="二、整合流程"><a href="#二、整合流程" class="headerlink" title="二、整合流程"></a>二、整合流程</h2><p>Flume 发送数据到 Kafka 上主要是通过 <code>KafkaSink</code> 来实现的，主要步骤如下：</p>
<h3 id="1-启动Zookeeper和Kafka"><a href="#1-启动Zookeeper和Kafka" class="headerlink" title="1. 启动Zookeeper和Kafka"></a>1. 启动Zookeeper和Kafka</h3><p>这里启动一个单节点的 Kafka 作为测试：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 启动Zookeeper</span></span><br><span class="line">zkServer.sh start</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 启动kafka</span></span><br><span class="line">bin/kafka-server-start.sh config/server.properties</span><br></pre></td></tr></table></figure>
<h3 id="2-创建主题"><a href="#2-创建主题" class="headerlink" title="2. 创建主题"></a>2. 创建主题</h3><p>创建一个主题 <code>flume-kafka</code>，之后 Flume 收集到的数据都会发到这个主题上：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 创建主题</span></span><br><span class="line">bin/kafka-topics.sh --create \</span><br><span class="line">--zookeeper hadoop001:2181 \</span><br><span class="line">--replication-factor 1   \</span><br><span class="line">--partitions 1 --topic flume-kafka</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看创建的主题</span></span><br><span class="line">bin/kafka-topics.sh --zookeeper hadoop001:2181 --list</span><br></pre></td></tr></table></figure>
<h3 id="3-启动kafka消费者"><a href="#3-启动kafka消费者" class="headerlink" title="3. 启动kafka消费者"></a>3. 启动kafka消费者</h3><p>启动一个消费者，监听我们刚才创建的 <code>flume-kafka</code> 主题：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> bin/kafka-console-consumer.sh --bootstrap-server hadoop001:9092 --topic flume-kafka</span></span><br></pre></td></tr></table></figure>
<h3 id="4-配置Flume"><a href="#4-配置Flume" class="headerlink" title="4. 配置Flume"></a>4. 配置Flume</h3><p>新建配置文件 <code>exec-memory-kafka.properties</code>，文件内容如下。这里我们监听一个名为 <code>kafka.log</code> 的文件，当文件内容有变化时，将新增加的内容发送到 Kafka 的 <code>flume-kafka</code> 主题上。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a1.sources = s1</span><br><span class="line">a1.channels = c1</span><br><span class="line">a1.sinks = k1                                                                                         </span><br><span class="line"></span><br><span class="line">a1.sources.s1.type=exec</span><br><span class="line">a1.sources.s1.command=tail -F /tmp/kafka.log</span><br><span class="line">a1.sources.s1.channels=c1 </span><br><span class="line"></span><br><span class="line">#设置Kafka接收器</span><br><span class="line">a1.sinks.k1.type= org.apache.flume.sink.kafka.KafkaSink</span><br><span class="line">#设置Kafka地址</span><br><span class="line">a1.sinks.k1.brokerList=hadoop001:9092</span><br><span class="line">#设置发送到Kafka上的主题</span><br><span class="line">a1.sinks.k1.topic=flume-kafka</span><br><span class="line">#设置序列化方式</span><br><span class="line">a1.sinks.k1.serializer.class=kafka.serializer.StringEncoder</span><br><span class="line">a1.sinks.k1.channel=c1     </span><br><span class="line"></span><br><span class="line">a1.channels.c1.type=memory</span><br><span class="line">a1.channels.c1.capacity=10000</span><br><span class="line">a1.channels.c1.transactionCapacity=100</span><br></pre></td></tr></table></figure>
<h3 id="5-启动Flume"><a href="#5-启动Flume" class="headerlink" title="5. 启动Flume"></a>5. 启动Flume</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">flume-ng agent \</span><br><span class="line">--conf conf \</span><br><span class="line">--conf-file /usr/app/apache-flume-1.6.0-cdh5.15.2-bin/examples/exec-memory-kafka.properties \</span><br><span class="line">--name a1 -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure>
<h3 id="6-测试"><a href="#6-测试" class="headerlink" title="6. 测试"></a>6. 测试</h3><p>向监听的 <code>/tmp/kafka.log</code> 文件中追加内容，查看 Kafka 消费者的输出：</p>
<div align="center"> <img src="../pictures/flume-kafka-01.png"> </div>

<p>可以看到 <code>flume-kafka</code> 主题的消费端已经收到了对应的消息：</p>
<div align="center"> <img src="../pictures/flume-kafka-2.png"> </div>
]]></content>
      <categories>
        <category>notes</category>
        <category>Flume</category>
      </categories>
      <tags>
        <tag>Flume</tag>
      </tags>
  </entry>
  <entry>
    <title>HDFS 常用 shell 命令</title>
    <url>/2021/03/17/HDFS%E5%B8%B8%E7%94%A8Shell%E5%91%BD%E4%BB%A4/</url>
    <content><![CDATA[<p><strong>1. 显示当前目录结构</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 显示当前目录结构</span></span><br><span class="line">hadoop fs -ls  &lt;path&gt;</span><br><span class="line"><span class="meta">#</span><span class="bash"> 递归显示当前目录结构</span></span><br><span class="line">hadoop fs -ls  -R  &lt;path&gt;</span><br><span class="line"><span class="meta">#</span><span class="bash"> 显示根目录下内容</span></span><br><span class="line">hadoop fs -ls  /</span><br></pre></td></tr></table></figure>
<p><strong>2. 创建目录</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 创建目录</span></span><br><span class="line">hadoop fs -mkdir  &lt;path&gt; </span><br><span class="line"><span class="meta">#</span><span class="bash"> 递归创建目录</span></span><br><span class="line">hadoop fs -mkdir -p  &lt;path&gt;</span><br></pre></td></tr></table></figure>
<p><strong>3. 删除操作</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 删除文件</span></span><br><span class="line">hadoop fs -rm  &lt;path&gt;</span><br><span class="line"><span class="meta">#</span><span class="bash"> 递归删除目录和文件</span></span><br><span class="line">hadoop fs -rm -R  &lt;path&gt;</span><br></pre></td></tr></table></figure>
<p><strong>4. 从本地加载文件到 HDFS</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 二选一执行即可</span></span><br><span class="line">hadoop fs -put  [localsrc] [dst] </span><br><span class="line">hadoop fs - copyFromLocal [localsrc] [dst]</span><br></pre></td></tr></table></figure>
<p><strong>5. 从 HDFS 导出文件到本地</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 二选一执行即可</span></span><br><span class="line">hadoop fs -get  [dst] [localsrc] </span><br><span class="line">hadoop fs -copyToLocal [dst] [localsrc]</span><br></pre></td></tr></table></figure>
<p><strong>6. 查看文件内容</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 二选一执行即可</span></span><br><span class="line">hadoop fs -text  &lt;path&gt; </span><br><span class="line">hadoop fs -cat  &lt;path&gt;</span><br></pre></td></tr></table></figure>
<p><strong>7. 显示文件的最后一千字节</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop fs -tail  &lt;path&gt; </span><br><span class="line"><span class="meta">#</span><span class="bash"> 和Linux下一样，会持续监听文件内容变化 并显示文件的最后一千字节</span></span><br><span class="line">hadoop fs -tail -f  &lt;path&gt;</span><br></pre></td></tr></table></figure>
<p><strong>8. 拷贝文件</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop fs -cp [src] [dst]</span><br></pre></td></tr></table></figure>
<p><strong>9. 移动文件</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop fs -mv [src] [dst]</span><br></pre></td></tr></table></figure>
<p><strong>10. 统计当前目录下各文件大小</strong>  </p>
<ul>
<li>默认单位字节  </li>
<li>-s : 显示所有文件大小总和，</li>
<li>-h : 将以更友好的方式显示文件大小（例如 64.0m 而不是 67108864）<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop fs -du  &lt;path&gt;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p><strong>11. 合并下载多个文件</strong></p>
<ul>
<li>-nl  在每个文件的末尾添加换行符（LF）</li>
<li>-skip-empty-file 跳过空文件</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop fs -getmerge</span><br><span class="line"><span class="meta">#</span><span class="bash"> 示例 将HDFS上的hbase-policy.xml和hbase-site.xml文件合并后下载到本地的/usr/test.xml</span></span><br><span class="line">hadoop fs -getmerge -nl  /test/hbase-policy.xml /test/hbase-site.xml /usr/test.xml</span><br></pre></td></tr></table></figure>
<p><strong>12. 统计文件系统的可用空间信息</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop fs -df -h /</span><br></pre></td></tr></table></figure>
<p><strong>13. 更改文件复制因子</strong><br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop fs -setrep [-R] [-w] &lt;numReplicas&gt; &lt;path&gt;</span><br></pre></td></tr></table></figure></p>
<ul>
<li>更改文件的复制因子。如果 path 是目录，则更改其下所有文件的复制因子</li>
<li>-w : 请求命令是否等待复制完成</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 示例</span></span><br><span class="line">hadoop fs -setrep -w 3 /user/hadoop/dir1</span><br></pre></td></tr></table></figure>
<p><strong>14. 权限控制</strong><br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 权限控制和Linux上使用方式一致</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 变更文件或目录的所属群组。 用户必须是文件的所有者或超级用户。</span></span><br><span class="line">hadoop fs -chgrp [-R] GROUP URI [URI ...]</span><br><span class="line"><span class="meta">#</span><span class="bash"> 修改文件或目录的访问权限  用户必须是文件的所有者或超级用户。</span></span><br><span class="line">hadoop fs -chmod [-R] &lt;MODE[,MODE]... | OCTALMODE&gt; URI [URI ...]</span><br><span class="line"><span class="meta">#</span><span class="bash"> 修改文件的拥有者  用户必须是超级用户。</span></span><br><span class="line">hadoop fs -chown [-R] [OWNER][:[GROUP]] URI [URI ]</span><br></pre></td></tr></table></figure></p>
<p><strong>15. 文件检测</strong><br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop fs -test - [defsz]  URI</span><br></pre></td></tr></table></figure></p>
<p>可选选项：</p>
<ul>
<li>-d：如果路径是目录，返回 0。</li>
<li>-e：如果路径存在，则返回 0。</li>
<li>-f：如果路径是文件，则返回 0。</li>
<li>-s：如果路径不为空，则返回 0。</li>
<li>-r：如果路径存在且授予读权限，则返回 0。</li>
<li>-w：如果路径存在且授予写入权限，则返回 0。</li>
<li>-z：如果文件长度为零，则返回 0。</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 示例</span></span><br><span class="line">hadoop fs -test -e filename</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>notes</category>
        <category>HDFS</category>
      </categories>
      <tags>
        <tag>HDFS</tag>
      </tags>
  </entry>
  <entry>
    <title>Hadoop分布式文件系统——HDFS</title>
    <url>/2021/03/17/Hadoop-HDFS/</url>
    <content><![CDATA[<h2 id="一、介绍"><a href="#一、介绍" class="headerlink" title="一、介绍"></a>一、介绍</h2><p><strong>HDFS</strong> （<strong>Hadoop Distributed File System</strong>）是 Hadoop 下的分布式文件系统，具有高容错、高吞吐量等特性，可以部署在低成本的硬件上。</p>
<h2 id="二、HDFS-设计原理"><a href="#二、HDFS-设计原理" class="headerlink" title="二、HDFS 设计原理"></a>二、HDFS 设计原理</h2><div align="center"> <img width="600px" src="../pictures/hdfsarchitecture.png"> </div>

<h3 id="2-1-HDFS-架构"><a href="#2-1-HDFS-架构" class="headerlink" title="2.1 HDFS 架构"></a>2.1 HDFS 架构</h3><p>HDFS 遵循主/从架构，由单个 NameNode(NN) 和多个 DataNode(DN) 组成：</p>
<ul>
<li><strong>NameNode</strong> : 负责执行有关 <code>文件系统命名空间</code> 的操作，例如打开，关闭、重命名文件和目录等。它同时还负责集群元数据的存储，记录着文件中各个数据块的位置信息。</li>
<li><strong>DataNode</strong>：负责提供来自文件系统客户端的读写请求，执行块的创建，删除等操作。</li>
</ul>
<h3 id="2-2-文件系统命名空间"><a href="#2-2-文件系统命名空间" class="headerlink" title="2.2 文件系统命名空间"></a>2.2 文件系统命名空间</h3><p>HDFS 的 <code>文件系统命名空间</code> 的层次结构与大多数文件系统类似 (如 Linux)， 支持目录和文件的创建、移动、删除和重命名等操作，支持配置用户和访问权限，但不支持硬链接和软连接。<code>NameNode</code> 负责维护文件系统名称空间，记录对名称空间或其属性的任何更改。</p>
<h3 id="2-3-数据复制"><a href="#2-3-数据复制" class="headerlink" title="2.3 数据复制"></a>2.3 数据复制</h3><p>由于 Hadoop 被设计运行在廉价的机器上，这意味着硬件是不可靠的，为了保证容错性，HDFS 提供了数据复制机制。HDFS 将每一个文件存储为一系列<strong>块</strong>，每个块由多个副本来保证容错，块的大小和复制因子可以自行配置（默认情况下，块大小是 128M，默认复制因子是 3）。</p>
<div align="center"> <img width="600px" src="../pictures/hdfsdatanodes.png"> </div>

<h3 id="2-4-数据复制的实现原理"><a href="#2-4-数据复制的实现原理" class="headerlink" title="2.4 数据复制的实现原理"></a>2.4 数据复制的实现原理</h3><p>大型的 HDFS 实例在通常分布在多个机架的多台服务器上，不同机架上的两台服务器之间通过交换机进行通讯。在大多数情况下，同一机架中的服务器间的网络带宽大于不同机架中的服务器之间的带宽。因此 HDFS 采用机架感知副本放置策略，对于常见情况，当复制因子为 3 时，HDFS 的放置策略是：</p>
<p>在写入程序位于 <code>datanode</code> 上时，就优先将写入文件的一个副本放置在该 <code>datanode</code> 上，否则放在随机 <code>datanode</code> 上。之后在另一个远程机架上的任意一个节点上放置另一个副本，并在该机架上的另一个节点上放置最后一个副本。此策略可以减少机架间的写入流量，从而提高写入性能。</p>
<div align="center"> <img src="../pictures/hdfs-机架.png"> </div>

<p>如果复制因子大于 3，则随机确定第 4 个和之后副本的放置位置，同时保持每个机架的副本数量低于上限，上限值通常为 <code>（复制系数 - 1）/机架数量 + 2</code>，需要注意的是不允许同一个 <code>dataNode</code> 上具有同一个块的多个副本。</p>
<h3 id="2-5-副本的选择"><a href="#2-5-副本的选择" class="headerlink" title="2.5  副本的选择"></a>2.5  副本的选择</h3><p>为了最大限度地减少带宽消耗和读取延迟，HDFS 在执行读取请求时，优先读取距离读取器最近的副本。如果在与读取器节点相同的机架上存在副本，则优先选择该副本。如果 HDFS 群集跨越多个数据中心，则优先选择本地数据中心上的副本。</p>
<h3 id="2-6-架构的稳定性"><a href="#2-6-架构的稳定性" class="headerlink" title="2.6 架构的稳定性"></a>2.6 架构的稳定性</h3><h4 id="1-心跳机制和重新复制"><a href="#1-心跳机制和重新复制" class="headerlink" title="1. 心跳机制和重新复制"></a>1. 心跳机制和重新复制</h4><p>每个 DataNode 定期向 NameNode 发送心跳消息，如果超过指定时间没有收到心跳消息，则将 DataNode 标记为死亡。NameNode 不会将任何新的 IO 请求转发给标记为死亡的 DataNode，也不会再使用这些 DataNode 上的数据。 由于数据不再可用，可能会导致某些块的复制因子小于其指定值，NameNode 会跟踪这些块，并在必要的时候进行重新复制。</p>
<h4 id="2-数据的完整性"><a href="#2-数据的完整性" class="headerlink" title="2. 数据的完整性"></a>2. 数据的完整性</h4><p>由于存储设备故障等原因，存储在 DataNode 上的数据块也会发生损坏。为了避免读取到已经损坏的数据而导致错误，HDFS 提供了数据完整性校验机制来保证数据的完整性，具体操作如下：</p>
<p>当客户端创建 HDFS 文件时，它会计算文件的每个块的 <code>校验和</code>，并将 <code>校验和</code> 存储在同一 HDFS 命名空间下的单独的隐藏文件中。当客户端检索文件内容时，它会验证从每个 DataNode 接收的数据是否与存储在关联校验和文件中的 <code>校验和</code> 匹配。如果匹配失败，则证明数据已经损坏，此时客户端会选择从其他 DataNode 获取该块的其他可用副本。</p>
<h4 id="3-元数据的磁盘故障"><a href="#3-元数据的磁盘故障" class="headerlink" title="3.元数据的磁盘故障"></a>3.元数据的磁盘故障</h4><p><code>FsImage</code> 和 <code>EditLog</code> 是 HDFS 的核心数据，这些数据的意外丢失可能会导致整个 HDFS 服务不可用。为了避免这个问题，可以配置 NameNode 使其支持 <code>FsImage</code> 和 <code>EditLog</code> 多副本同步，这样 <code>FsImage</code> 或 <code>EditLog</code> 的任何改变都会引起每个副本 <code>FsImage</code> 和 <code>EditLog</code> 的同步更新。</p>
<h4 id="4-支持快照"><a href="#4-支持快照" class="headerlink" title="4.支持快照"></a>4.支持快照</h4><p>快照支持在特定时刻存储数据副本，在数据意外损坏时，可以通过回滚操作恢复到健康的数据状态。</p>
<h2 id="三、HDFS-的特点"><a href="#三、HDFS-的特点" class="headerlink" title="三、HDFS 的特点"></a>三、HDFS 的特点</h2><h3 id="3-1-高容错"><a href="#3-1-高容错" class="headerlink" title="3.1 高容错"></a>3.1 高容错</h3><p>由于 HDFS 采用数据的多副本方案，所以部分硬件的损坏不会导致全部数据的丢失。</p>
<h3 id="3-2-高吞吐量"><a href="#3-2-高吞吐量" class="headerlink" title="3.2 高吞吐量"></a>3.2 高吞吐量</h3><p>HDFS 设计的重点是支持高吞吐量的数据访问，而不是低延迟的数据访问。</p>
<h3 id="3-3-大文件支持"><a href="#3-3-大文件支持" class="headerlink" title="3.3  大文件支持"></a>3.3  大文件支持</h3><p>HDFS 适合于大文件的存储，文档的大小应该是是 GB 到 TB 级别的。</p>
<h3 id="3-3-简单一致性模型"><a href="#3-3-简单一致性模型" class="headerlink" title="3.3 简单一致性模型"></a>3.3 简单一致性模型</h3><p>HDFS 更适合于一次写入多次读取 (write-once-read-many) 的访问模型。支持将内容追加到文件末尾，但不支持数据的随机访问，不能从文件任意位置新增数据。</p>
<h3 id="3-4-跨平台移植性"><a href="#3-4-跨平台移植性" class="headerlink" title="3.4 跨平台移植性"></a>3.4 跨平台移植性</h3><p>HDFS 具有良好的跨平台移植性，这使得其他大数据计算框架都将其作为数据持久化存储的首选方案。</p>
<h2 id="附：图解HDFS存储原理"><a href="#附：图解HDFS存储原理" class="headerlink" title="附：图解HDFS存储原理"></a>附：图解HDFS存储原理</h2><blockquote>
<p>说明：以下图片引用自博客：<a href="https://blog.csdn.net/hudiefenmu/article/details/37655491" target="_blank" rel="noopener">翻译经典 HDFS 原理讲解漫画</a></p>
</blockquote>
<h3 id="1-HDFS写数据原理"><a href="#1-HDFS写数据原理" class="headerlink" title="1. HDFS写数据原理"></a>1. HDFS写数据原理</h3><div align="center"> <img src="../pictures/hdfs-write-1.jpg"> </div>

<div align="center"> <img src="../pictures/hdfs-write-2.jpg"> </div>

<div align="center"> <img src="../pictures/hdfs-write-3.jpg"> </div>



<h3 id="2-HDFS读数据原理"><a href="#2-HDFS读数据原理" class="headerlink" title="2. HDFS读数据原理"></a>2. HDFS读数据原理</h3><div align="center"> <img src="../pictures/hdfs-read-1.jpg"> </div>



<h3 id="3-HDFS故障类型和其检测方法"><a href="#3-HDFS故障类型和其检测方法" class="headerlink" title="3. HDFS故障类型和其检测方法"></a>3. HDFS故障类型和其检测方法</h3><div align="center"> <img src="../pictures/hdfs-tolerance-1.jpg"> </div>

<div align="center"> <img src="../pictures/hdfs-tolerance-2.jpg"> </div>



<p><strong>第二部分：读写故障的处理</strong></p>
<div align="center"> <img src="../pictures/hdfs-tolerance-3.jpg"> </div>



<p><strong>第三部分：DataNode 故障处理</strong></p>
<div align="center"> <img src="../pictures/hdfs-tolerance-4.jpg"> </div>



<p><strong>副本布局策略</strong>：</p>
<div align="center"> <img src="../pictures/hdfs-tolerance-5.jpg"> </div>



<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol>
<li><a href="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html" target="_blank" rel="noopener">Apache Hadoop 2.9.2 &gt; HDFS Architecture</a></li>
<li>Tom White . hadoop 权威指南 [M] . 清华大学出版社 . 2017.</li>
<li><a href="https://blog.csdn.net/hudiefenmu/article/details/37655491" target="_blank" rel="noopener">翻译经典 HDFS 原理讲解漫画</a></li>
</ol>
]]></content>
      <categories>
        <category>notes</category>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title>HBase Java API 的基本使用</title>
    <url>/2021/03/17/Hbase_Java_API/</url>
    <content><![CDATA[<h2 id="一、简述"><a href="#一、简述" class="headerlink" title="一、简述"></a>一、简述</h2><p>截至到目前 (2019.04)，HBase 有两个主要的版本，分别是 1.x 和 2.x ，两个版本的 Java API 有所不同，1.x 中某些方法在 2.x 中被标识为 <code>@deprecated</code> 过时。所以下面关于 API 的样例，我会分别给出 1.x 和 2.x 两个版本。完整的代码见本仓库：</p>
<blockquote>
<ul>
<li><p><a href="https://github.com/heibaiying/BigData-Notes/tree/master/code/Hbase/hbase-java-api-1.x" target="_blank" rel="noopener">Java API 1.x Examples</a></p>
</li>
<li><p><a href="https://github.com/heibaiying/BigData-Notes/tree/master/code/Hbase/hbase-java-api-2.x" target="_blank" rel="noopener">Java API 2.x Examples</a></p>
</li>
</ul>
</blockquote>
<p>同时你使用的客户端的版本必须与服务端版本保持一致，如果用 2.x 版本的客户端代码去连接 1.x 版本的服务端，会抛出 <code>NoSuchColumnFamilyException</code> 等异常。</p>
<h2 id="二、Java-API-1-x-基本使用"><a href="#二、Java-API-1-x-基本使用" class="headerlink" title="二、Java API 1.x 基本使用"></a>二、Java API 1.x 基本使用</h2><h4 id="2-1-新建Maven工程，导入项目依赖"><a href="#2-1-新建Maven工程，导入项目依赖" class="headerlink" title="2.1 新建Maven工程，导入项目依赖"></a>2.1 新建Maven工程，导入项目依赖</h4><p>要使用 Java API 操作 HBase，需要引入 <code>hbase-client</code>。这里选取的 <code>HBase Client</code> 的版本为 <code>1.2.0</code>。</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hbase<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hbase-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h4 id="2-2-API-基本使用"><a href="#2-2-API-基本使用" class="headerlink" title="2.2 API 基本使用"></a>2.2 API 基本使用</h4><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HBaseUtils</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> Connection connection;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">static</span> &#123;</span><br><span class="line">        Configuration configuration = HBaseConfiguration.create();</span><br><span class="line">        configuration.set(<span class="string">"hbase.zookeeper.property.clientPort"</span>, <span class="string">"2181"</span>);</span><br><span class="line">        <span class="comment">// 如果是集群 则主机名用逗号分隔</span></span><br><span class="line">        configuration.set(<span class="string">"hbase.zookeeper.quorum"</span>, <span class="string">"hadoop001"</span>);</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            connection = ConnectionFactory.createConnection(configuration);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 创建 HBase 表</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> tableName      表名</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> columnFamilies 列族的数组</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">boolean</span> <span class="title">createTable</span><span class="params">(String tableName, List&lt;String&gt; columnFamilies)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            HBaseAdmin admin = (HBaseAdmin) connection.getAdmin();</span><br><span class="line">            <span class="keyword">if</span> (admin.tableExists(tableName)) &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            HTableDescriptor tableDescriptor = <span class="keyword">new</span> HTableDescriptor(TableName.valueOf(tableName));</span><br><span class="line">            columnFamilies.forEach(columnFamily -&gt; &#123;</span><br><span class="line">                HColumnDescriptor columnDescriptor = <span class="keyword">new</span> HColumnDescriptor(columnFamily);</span><br><span class="line">                columnDescriptor.setMaxVersions(<span class="number">1</span>);</span><br><span class="line">                tableDescriptor.addFamily(columnDescriptor);</span><br><span class="line">            &#125;);</span><br><span class="line">            admin.createTable(tableDescriptor);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 删除 hBase 表</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> tableName 表名</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">boolean</span> <span class="title">deleteTable</span><span class="params">(String tableName)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            HBaseAdmin admin = (HBaseAdmin) connection.getAdmin();</span><br><span class="line">            <span class="comment">// 删除表前需要先禁用表</span></span><br><span class="line">            admin.disableTable(tableName);</span><br><span class="line">            admin.deleteTable(tableName);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 插入数据</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> tableName        表名</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> rowKey           唯一标识</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> columnFamilyName 列族名</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> qualifier        列标识</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> value            数据</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">boolean</span> <span class="title">putRow</span><span class="params">(String tableName, String rowKey, String columnFamilyName, String qualifier,</span></span></span><br><span class="line"><span class="function"><span class="params">                                 String value)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            Table table = connection.getTable(TableName.valueOf(tableName));</span><br><span class="line">            Put put = <span class="keyword">new</span> Put(Bytes.toBytes(rowKey));</span><br><span class="line">            put.addColumn(Bytes.toBytes(columnFamilyName), Bytes.toBytes(qualifier), Bytes.toBytes(value));</span><br><span class="line">            table.put(put);</span><br><span class="line">            table.close();</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 插入数据</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> tableName        表名</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> rowKey           唯一标识</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> columnFamilyName 列族名</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> pairList         列标识和值的集合</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">boolean</span> <span class="title">putRow</span><span class="params">(String tableName, String rowKey, String columnFamilyName, List&lt;Pair&lt;String, String&gt;&gt; pairList)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            Table table = connection.getTable(TableName.valueOf(tableName));</span><br><span class="line">            Put put = <span class="keyword">new</span> Put(Bytes.toBytes(rowKey));</span><br><span class="line">            pairList.forEach(pair -&gt; put.addColumn(Bytes.toBytes(columnFamilyName), Bytes.toBytes(pair.getKey()), Bytes.toBytes(pair.getValue())));</span><br><span class="line">            table.put(put);</span><br><span class="line">            table.close();</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 根据 rowKey 获取指定行的数据</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> tableName 表名</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> rowKey    唯一标识</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> Result <span class="title">getRow</span><span class="params">(String tableName, String rowKey)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            Table table = connection.getTable(TableName.valueOf(tableName));</span><br><span class="line">            Get get = <span class="keyword">new</span> Get(Bytes.toBytes(rowKey));</span><br><span class="line">            <span class="keyword">return</span> table.get(get);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 获取指定行指定列 (cell) 的最新版本的数据</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> tableName    表名</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> rowKey       唯一标识</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> columnFamily 列族</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> qualifier    列标识</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> String <span class="title">getCell</span><span class="params">(String tableName, String rowKey, String columnFamily, String qualifier)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            Table table = connection.getTable(TableName.valueOf(tableName));</span><br><span class="line">            Get get = <span class="keyword">new</span> Get(Bytes.toBytes(rowKey));</span><br><span class="line">            <span class="keyword">if</span> (!get.isCheckExistenceOnly()) &#123;</span><br><span class="line">                get.addColumn(Bytes.toBytes(columnFamily), Bytes.toBytes(qualifier));</span><br><span class="line">                Result result = table.get(get);</span><br><span class="line">                <span class="keyword">byte</span>[] resultValue = result.getValue(Bytes.toBytes(columnFamily), Bytes.toBytes(qualifier));</span><br><span class="line">                <span class="keyword">return</span> Bytes.toString(resultValue);</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 检索全表</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> tableName 表名</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> ResultScanner <span class="title">getScanner</span><span class="params">(String tableName)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            Table table = connection.getTable(TableName.valueOf(tableName));</span><br><span class="line">            Scan scan = <span class="keyword">new</span> Scan();</span><br><span class="line">            <span class="keyword">return</span> table.getScanner(scan);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 检索表中指定数据</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> tableName  表名</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> filterList 过滤器</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> ResultScanner <span class="title">getScanner</span><span class="params">(String tableName, FilterList filterList)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            Table table = connection.getTable(TableName.valueOf(tableName));</span><br><span class="line">            Scan scan = <span class="keyword">new</span> Scan();</span><br><span class="line">            scan.setFilter(filterList);</span><br><span class="line">            <span class="keyword">return</span> table.getScanner(scan);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 检索表中指定数据</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> tableName   表名</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> startRowKey 起始 RowKey</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> endRowKey   终止 RowKey</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> filterList  过滤器</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> ResultScanner <span class="title">getScanner</span><span class="params">(String tableName, String startRowKey, String endRowKey,</span></span></span><br><span class="line"><span class="function"><span class="params">                                           FilterList filterList)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            Table table = connection.getTable(TableName.valueOf(tableName));</span><br><span class="line">            Scan scan = <span class="keyword">new</span> Scan();</span><br><span class="line">            scan.setStartRow(Bytes.toBytes(startRowKey));</span><br><span class="line">            scan.setStopRow(Bytes.toBytes(endRowKey));</span><br><span class="line">            scan.setFilter(filterList);</span><br><span class="line">            <span class="keyword">return</span> table.getScanner(scan);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 删除指定行记录</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> tableName 表名</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> rowKey    唯一标识</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">boolean</span> <span class="title">deleteRow</span><span class="params">(String tableName, String rowKey)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            Table table = connection.getTable(TableName.valueOf(tableName));</span><br><span class="line">            Delete delete = <span class="keyword">new</span> Delete(Bytes.toBytes(rowKey));</span><br><span class="line">            table.delete(delete);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 删除指定行的指定列</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> tableName  表名</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> rowKey     唯一标识</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> familyName 列族</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> qualifier  列标识</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">boolean</span> <span class="title">deleteColumn</span><span class="params">(String tableName, String rowKey, String familyName,</span></span></span><br><span class="line"><span class="function"><span class="params">                                          String qualifier)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            Table table = connection.getTable(TableName.valueOf(tableName));</span><br><span class="line">            Delete delete = <span class="keyword">new</span> Delete(Bytes.toBytes(rowKey));</span><br><span class="line">            delete.addColumn(Bytes.toBytes(familyName), Bytes.toBytes(qualifier));</span><br><span class="line">            table.delete(delete);</span><br><span class="line">            table.close();</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="2-3-单元测试"><a href="#2-3-单元测试" class="headerlink" title="2.3 单元测试"></a>2.3 单元测试</h3><p>以单元测试的方式对上面封装的 API 进行测试。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HBaseUtilsTest</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String TABLE_NAME = <span class="string">"class"</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String TEACHER = <span class="string">"teacher"</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String STUDENT = <span class="string">"student"</span>;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">createTable</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 新建表</span></span><br><span class="line">        List&lt;String&gt; columnFamilies = Arrays.asList(TEACHER, STUDENT);</span><br><span class="line">        <span class="keyword">boolean</span> table = HBaseUtils.createTable(TABLE_NAME, columnFamilies);</span><br><span class="line">        System.out.println(<span class="string">"表创建结果:"</span> + table);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">insertData</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        List&lt;Pair&lt;String, String&gt;&gt; pairs1 = Arrays.asList(<span class="keyword">new</span> Pair&lt;&gt;(<span class="string">"name"</span>, <span class="string">"Tom"</span>),</span><br><span class="line">                <span class="keyword">new</span> Pair&lt;&gt;(<span class="string">"age"</span>, <span class="string">"22"</span>),</span><br><span class="line">                <span class="keyword">new</span> Pair&lt;&gt;(<span class="string">"gender"</span>, <span class="string">"1"</span>));</span><br><span class="line">        HBaseUtils.putRow(TABLE_NAME, <span class="string">"rowKey1"</span>, STUDENT, pairs1);</span><br><span class="line"></span><br><span class="line">        List&lt;Pair&lt;String, String&gt;&gt; pairs2 = Arrays.asList(<span class="keyword">new</span> Pair&lt;&gt;(<span class="string">"name"</span>, <span class="string">"Jack"</span>),</span><br><span class="line">                <span class="keyword">new</span> Pair&lt;&gt;(<span class="string">"age"</span>, <span class="string">"33"</span>),</span><br><span class="line">                <span class="keyword">new</span> Pair&lt;&gt;(<span class="string">"gender"</span>, <span class="string">"2"</span>));</span><br><span class="line">        HBaseUtils.putRow(TABLE_NAME, <span class="string">"rowKey2"</span>, STUDENT, pairs2);</span><br><span class="line"></span><br><span class="line">        List&lt;Pair&lt;String, String&gt;&gt; pairs3 = Arrays.asList(<span class="keyword">new</span> Pair&lt;&gt;(<span class="string">"name"</span>, <span class="string">"Mike"</span>),</span><br><span class="line">                <span class="keyword">new</span> Pair&lt;&gt;(<span class="string">"age"</span>, <span class="string">"44"</span>),</span><br><span class="line">                <span class="keyword">new</span> Pair&lt;&gt;(<span class="string">"gender"</span>, <span class="string">"1"</span>));</span><br><span class="line">        HBaseUtils.putRow(TABLE_NAME, <span class="string">"rowKey3"</span>, STUDENT, pairs3);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">getRow</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        Result result = HBaseUtils.getRow(TABLE_NAME, <span class="string">"rowKey1"</span>);</span><br><span class="line">        <span class="keyword">if</span> (result != <span class="keyword">null</span>) &#123;</span><br><span class="line">            System.out.println(Bytes</span><br><span class="line">                    .toString(result.getValue(Bytes.toBytes(STUDENT), Bytes.toBytes(<span class="string">"name"</span>))));</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">getCell</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        String cell = HBaseUtils.getCell(TABLE_NAME, <span class="string">"rowKey2"</span>, STUDENT, <span class="string">"age"</span>);</span><br><span class="line">        System.out.println(<span class="string">"cell age :"</span> + cell);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">getScanner</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        ResultScanner scanner = HBaseUtils.getScanner(TABLE_NAME);</span><br><span class="line">        <span class="keyword">if</span> (scanner != <span class="keyword">null</span>) &#123;</span><br><span class="line">            scanner.forEach(result -&gt; System.out.println(Bytes.toString(result.getRow()) + <span class="string">"-&gt;"</span> + Bytes</span><br><span class="line">                    .toString(result.getValue(Bytes.toBytes(STUDENT), Bytes.toBytes(<span class="string">"name"</span>)))));</span><br><span class="line">            scanner.close();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">getScannerWithFilter</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        FilterList filterList = <span class="keyword">new</span> FilterList(FilterList.Operator.MUST_PASS_ALL);</span><br><span class="line">        SingleColumnValueFilter nameFilter = <span class="keyword">new</span> SingleColumnValueFilter(Bytes.toBytes(STUDENT),</span><br><span class="line">                Bytes.toBytes(<span class="string">"name"</span>), CompareOperator.EQUAL, Bytes.toBytes(<span class="string">"Jack"</span>));</span><br><span class="line">        filterList.addFilter(nameFilter);</span><br><span class="line">        ResultScanner scanner = HBaseUtils.getScanner(TABLE_NAME, filterList);</span><br><span class="line">        <span class="keyword">if</span> (scanner != <span class="keyword">null</span>) &#123;</span><br><span class="line">            scanner.forEach(result -&gt; System.out.println(Bytes.toString(result.getRow()) + <span class="string">"-&gt;"</span> + Bytes</span><br><span class="line">                    .toString(result.getValue(Bytes.toBytes(STUDENT), Bytes.toBytes(<span class="string">"name"</span>)))));</span><br><span class="line">            scanner.close();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">deleteColumn</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">boolean</span> b = HBaseUtils.deleteColumn(TABLE_NAME, <span class="string">"rowKey2"</span>, STUDENT, <span class="string">"age"</span>);</span><br><span class="line">        System.out.println(<span class="string">"删除结果: "</span> + b);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">deleteRow</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">boolean</span> b = HBaseUtils.deleteRow(TABLE_NAME, <span class="string">"rowKey2"</span>);</span><br><span class="line">        System.out.println(<span class="string">"删除结果: "</span> + b);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">deleteTable</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">boolean</span> b = HBaseUtils.deleteTable(TABLE_NAME);</span><br><span class="line">        System.out.println(<span class="string">"删除结果: "</span> + b);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="三、Java-API-2-x-基本使用"><a href="#三、Java-API-2-x-基本使用" class="headerlink" title="三、Java API 2.x 基本使用"></a>三、Java API 2.x 基本使用</h2><h4 id="3-1-新建Maven工程，导入项目依赖"><a href="#3-1-新建Maven工程，导入项目依赖" class="headerlink" title="3.1 新建Maven工程，导入项目依赖"></a>3.1 新建Maven工程，导入项目依赖</h4><p>这里选取的 <code>HBase Client</code> 的版本为最新的 <code>2.1.4</code>。</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hbase<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hbase-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.1.4<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h4 id="3-2-API-的基本使用"><a href="#3-2-API-的基本使用" class="headerlink" title="3.2 API 的基本使用"></a>3.2 API 的基本使用</h4><p>2.x 版本相比于 1.x 废弃了一部分方法，关于废弃的方法在源码中都会指明新的替代方法，比如，在 2.x 中创建表时：<code>HTableDescriptor</code> 和 <code>HColumnDescriptor</code> 等类都标识为废弃，取而代之的是使用 <code>TableDescriptorBuilder</code> 和 <code>ColumnFamilyDescriptorBuilder</code> 来定义表和列族。</p>
<div align="center"> <img width="700px" src="../pictures/deprecated.png"> </div>



<p>以下为 HBase  2.x 版本 Java API 的使用示例：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HBaseUtils</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> Connection connection;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">static</span> &#123;</span><br><span class="line">        Configuration configuration = HBaseConfiguration.create();</span><br><span class="line">        configuration.set(<span class="string">"hbase.zookeeper.property.clientPort"</span>, <span class="string">"2181"</span>);</span><br><span class="line">        <span class="comment">// 如果是集群 则主机名用逗号分隔</span></span><br><span class="line">        configuration.set(<span class="string">"hbase.zookeeper.quorum"</span>, <span class="string">"hadoop001"</span>);</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            connection = ConnectionFactory.createConnection(configuration);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 创建 HBase 表</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> tableName      表名</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> columnFamilies 列族的数组</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">boolean</span> <span class="title">createTable</span><span class="params">(String tableName, List&lt;String&gt; columnFamilies)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            HBaseAdmin admin = (HBaseAdmin) connection.getAdmin();</span><br><span class="line">            <span class="keyword">if</span> (admin.tableExists(TableName.valueOf(tableName))) &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            TableDescriptorBuilder tableDescriptor = TableDescriptorBuilder.newBuilder(TableName.valueOf(tableName));</span><br><span class="line">            columnFamilies.forEach(columnFamily -&gt; &#123;</span><br><span class="line">                ColumnFamilyDescriptorBuilder cfDescriptorBuilder = ColumnFamilyDescriptorBuilder.newBuilder(Bytes.toBytes(columnFamily));</span><br><span class="line">                cfDescriptorBuilder.setMaxVersions(<span class="number">1</span>);</span><br><span class="line">                ColumnFamilyDescriptor familyDescriptor = cfDescriptorBuilder.build();</span><br><span class="line">                tableDescriptor.setColumnFamily(familyDescriptor);</span><br><span class="line">            &#125;);</span><br><span class="line">            admin.createTable(tableDescriptor.build());</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 删除 hBase 表</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> tableName 表名</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">boolean</span> <span class="title">deleteTable</span><span class="params">(String tableName)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            HBaseAdmin admin = (HBaseAdmin) connection.getAdmin();</span><br><span class="line">            <span class="comment">// 删除表前需要先禁用表</span></span><br><span class="line">            admin.disableTable(TableName.valueOf(tableName));</span><br><span class="line">            admin.deleteTable(TableName.valueOf(tableName));</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 插入数据</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> tableName        表名</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> rowKey           唯一标识</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> columnFamilyName 列族名</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> qualifier        列标识</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> value            数据</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">boolean</span> <span class="title">putRow</span><span class="params">(String tableName, String rowKey, String columnFamilyName, String qualifier,</span></span></span><br><span class="line"><span class="function"><span class="params">                                 String value)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            Table table = connection.getTable(TableName.valueOf(tableName));</span><br><span class="line">            Put put = <span class="keyword">new</span> Put(Bytes.toBytes(rowKey));</span><br><span class="line">            put.addColumn(Bytes.toBytes(columnFamilyName), Bytes.toBytes(qualifier), Bytes.toBytes(value));</span><br><span class="line">            table.put(put);</span><br><span class="line">            table.close();</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 插入数据</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> tableName        表名</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> rowKey           唯一标识</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> columnFamilyName 列族名</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> pairList         列标识和值的集合</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">boolean</span> <span class="title">putRow</span><span class="params">(String tableName, String rowKey, String columnFamilyName, List&lt;Pair&lt;String, String&gt;&gt; pairList)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            Table table = connection.getTable(TableName.valueOf(tableName));</span><br><span class="line">            Put put = <span class="keyword">new</span> Put(Bytes.toBytes(rowKey));</span><br><span class="line">            pairList.forEach(pair -&gt; put.addColumn(Bytes.toBytes(columnFamilyName), Bytes.toBytes(pair.getKey()), Bytes.toBytes(pair.getValue())));</span><br><span class="line">            table.put(put);</span><br><span class="line">            table.close();</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 根据 rowKey 获取指定行的数据</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> tableName 表名</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> rowKey    唯一标识</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> Result <span class="title">getRow</span><span class="params">(String tableName, String rowKey)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            Table table = connection.getTable(TableName.valueOf(tableName));</span><br><span class="line">            Get get = <span class="keyword">new</span> Get(Bytes.toBytes(rowKey));</span><br><span class="line">            <span class="keyword">return</span> table.get(get);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 获取指定行指定列 (cell) 的最新版本的数据</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> tableName    表名</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> rowKey       唯一标识</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> columnFamily 列族</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> qualifier    列标识</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> String <span class="title">getCell</span><span class="params">(String tableName, String rowKey, String columnFamily, String qualifier)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            Table table = connection.getTable(TableName.valueOf(tableName));</span><br><span class="line">            Get get = <span class="keyword">new</span> Get(Bytes.toBytes(rowKey));</span><br><span class="line">            <span class="keyword">if</span> (!get.isCheckExistenceOnly()) &#123;</span><br><span class="line">                get.addColumn(Bytes.toBytes(columnFamily), Bytes.toBytes(qualifier));</span><br><span class="line">                Result result = table.get(get);</span><br><span class="line">                <span class="keyword">byte</span>[] resultValue = result.getValue(Bytes.toBytes(columnFamily), Bytes.toBytes(qualifier));</span><br><span class="line">                <span class="keyword">return</span> Bytes.toString(resultValue);</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 检索全表</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> tableName 表名</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> ResultScanner <span class="title">getScanner</span><span class="params">(String tableName)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            Table table = connection.getTable(TableName.valueOf(tableName));</span><br><span class="line">            Scan scan = <span class="keyword">new</span> Scan();</span><br><span class="line">            <span class="keyword">return</span> table.getScanner(scan);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 检索表中指定数据</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> tableName  表名</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> filterList 过滤器</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> ResultScanner <span class="title">getScanner</span><span class="params">(String tableName, FilterList filterList)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            Table table = connection.getTable(TableName.valueOf(tableName));</span><br><span class="line">            Scan scan = <span class="keyword">new</span> Scan();</span><br><span class="line">            scan.setFilter(filterList);</span><br><span class="line">            <span class="keyword">return</span> table.getScanner(scan);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 检索表中指定数据</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> tableName   表名</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> startRowKey 起始 RowKey</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> endRowKey   终止 RowKey</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> filterList  过滤器</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> ResultScanner <span class="title">getScanner</span><span class="params">(String tableName, String startRowKey, String endRowKey,</span></span></span><br><span class="line"><span class="function"><span class="params">                                           FilterList filterList)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            Table table = connection.getTable(TableName.valueOf(tableName));</span><br><span class="line">            Scan scan = <span class="keyword">new</span> Scan();</span><br><span class="line">            scan.withStartRow(Bytes.toBytes(startRowKey));</span><br><span class="line">            scan.withStopRow(Bytes.toBytes(endRowKey));</span><br><span class="line">            scan.setFilter(filterList);</span><br><span class="line">            <span class="keyword">return</span> table.getScanner(scan);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 删除指定行记录</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> tableName 表名</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> rowKey    唯一标识</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">boolean</span> <span class="title">deleteRow</span><span class="params">(String tableName, String rowKey)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            Table table = connection.getTable(TableName.valueOf(tableName));</span><br><span class="line">            Delete delete = <span class="keyword">new</span> Delete(Bytes.toBytes(rowKey));</span><br><span class="line">            table.delete(delete);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 删除指定行指定列</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> tableName  表名</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> rowKey     唯一标识</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> familyName 列族</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> qualifier  列标识</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">boolean</span> <span class="title">deleteColumn</span><span class="params">(String tableName, String rowKey, String familyName,</span></span></span><br><span class="line"><span class="function"><span class="params">                                          String qualifier)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            Table table = connection.getTable(TableName.valueOf(tableName));</span><br><span class="line">            Delete delete = <span class="keyword">new</span> Delete(Bytes.toBytes(rowKey));</span><br><span class="line">            delete.addColumn(Bytes.toBytes(familyName), Bytes.toBytes(qualifier));</span><br><span class="line">            table.delete(delete);</span><br><span class="line">            table.close();</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="四、正确连接Hbase"><a href="#四、正确连接Hbase" class="headerlink" title="四、正确连接Hbase"></a>四、正确连接Hbase</h2><p>在上面的代码中，在类加载时就初始化了 Connection 连接，并且之后的方法都是复用这个 Connection，这时我们可能会考虑是否可以使用自定义连接池来获取更好的性能表现？实际上这是没有必要的。</p>
<p>首先官方对于 <code>Connection</code> 的使用说明如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Connection Pooling For applications which require high-end multithreaded   </span><br><span class="line">access (e.g., web-servers or  application servers  that may serve many   </span><br><span class="line">application threads in a single JVM), you can pre-create a Connection,   </span><br><span class="line">as shown in the following example:</span><br><span class="line"></span><br><span class="line">对于高并发多线程访问的应用程序（例如，在单个 JVM 中存在的为多个线程服务的 Web 服务器或应用程序服务器），  </span><br><span class="line">您只需要预先创建一个 Connection。例子如下：</span><br><span class="line"></span><br><span class="line">// Create a connection to the cluster.</span><br><span class="line">Configuration conf = HBaseConfiguration.create();</span><br><span class="line">try (Connection connection = ConnectionFactory.createConnection(conf);</span><br><span class="line">     Table table = connection.getTable(TableName.valueOf(tablename))) &#123;</span><br><span class="line">  // use table as needed, the table returned is lightweight</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>之所以能这样使用，这是因为 Connection 并不是一个简单的 socket 连接，<a href="https://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/Connection.html" target="_blank" rel="noopener">接口文档</a> 中对 Connection 的表述是：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">A cluster connection encapsulating lower level individual connections to actual servers and a  </span><br><span class="line">connection to zookeeper.  Connections are instantiated through the ConnectionFactory class.  </span><br><span class="line">The lifecycle of the connection is managed by the caller,  who has to close() the connection   </span><br><span class="line">to release the resources. </span><br><span class="line"></span><br><span class="line">Connection 是一个集群连接，封装了与多台服务器（Matser/Region Server）的底层连接以及与 zookeeper 的连接。  </span><br><span class="line">连接通过 ConnectionFactory  类实例化。连接的生命周期由调用者管理，调用者必须使用 close() 关闭连接以释放资源。</span><br></pre></td></tr></table></figure>
<p>之所以封装这些连接，是因为 HBase 客户端需要连接三个不同的服务角色：</p>
<ul>
<li><strong>Zookeeper</strong> ：主要用于获取 <code>meta</code> 表的位置信息，Master 的信息；</li>
<li><strong>HBase Master</strong> ：主要用于执行 HBaseAdmin 接口的一些操作，例如建表等；</li>
<li><strong>HBase RegionServer</strong> ：用于读、写数据。</li>
</ul>
<div align="center"> <img width="700px" src="../pictures/hbase-arc.png"> </div>

<p>Connection 对象和实际的 Socket 连接之间的对应关系如下图：</p>
<div align="center"> <img width="700px" src="../pictures/hbase-connection.png"> </div>

<blockquote>
<p>上面两张图片引用自博客：<a href="https://yq.aliyun.com/articles/581702?spm=a2c4e.11157919.spm-cont-list.1.146c27aeFxoMsN%20%E8%BF%9E%E6%8E%A5HBase%E7%9A%84%E6%AD%A3%E7%A1%AE%E5%A7%BF%E5%8A%BF" target="_blank" rel="noopener">连接 HBase 的正确姿势</a></p>
</blockquote>
<p>在 HBase 客户端代码中，真正对应 Socket 连接的是 <code>RpcConnection</code> 对象。HBase 使用 <code>PoolMap</code> 这种数据结构来存储客户端到 HBase 服务器之间的连接。<code>PoolMap</code> 的内部有一个 <code>ConcurrentHashMap</code> 实例，其 key 是 <code>ConnectionId</code>(封装了服务器地址和用户 ticket)，value 是一个 <code>RpcConnection</code> 对象的资源池。当 HBase 需要连接一个服务器时，首先会根据 <code>ConnectionId</code> 找到对应的连接池，然后从连接池中取出一个连接对象。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@InterfaceAudience</span>.Private</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">PoolMap</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt; <span class="keyword">implements</span> <span class="title">Map</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt; </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> PoolType poolType;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">int</span> poolMaxSize;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> Map&lt;K, Pool&lt;V&gt;&gt; pools = <span class="keyword">new</span> ConcurrentHashMap&lt;&gt;();</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="title">PoolMap</span><span class="params">(PoolType poolType)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>.poolType = poolType;</span><br><span class="line">  &#125;</span><br><span class="line">  .....</span><br></pre></td></tr></table></figure>
<p>HBase 中提供了三种资源池的实现，分别是 <code>Reusable</code>，<code>RoundRobin</code> 和 <code>ThreadLocal</code>。具体实现可以通 <code>hbase.client.ipc.pool.type</code> 配置项指定，默认为 <code>Reusable</code>。连接池的大小也可以通过 <code>hbase.client.ipc.pool.size</code> 配置项指定，默认为 1，即每个 Server 1 个连接。也可以通过修改配置实现：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">config.set(<span class="string">"hbase.client.ipc.pool.type"</span>,...);</span><br><span class="line">config.set(<span class="string">"hbase.client.ipc.pool.size"</span>,...);</span><br><span class="line">connection = ConnectionFactory.createConnection(config);</span><br></pre></td></tr></table></figure>
<p>由此可以看出 HBase 中 Connection 类已经实现了对连接的管理功能，所以我们不必在 Connection 上在做额外的管理。</p>
<p>另外，Connection 是线程安全的，但 Table 和 Admin 却不是线程安全的，因此正确的做法是一个进程共用一个 Connection 对象，而在不同的线程中使用单独的 Table 和 Admin 对象。Table 和 Admin 的获取操作 <code>getTable()</code> 和 <code>getAdmin()</code> 都是轻量级，所以不必担心性能的消耗，同时建议在使用完成后显示的调用 <code>close()</code> 方法来关闭它们。</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol>
<li><a href="https://yq.aliyun.com/articles/581702?spm=a2c4e.11157919.spm-cont-list.1.146c27aeFxoMsN%20%E8%BF%9E%E6%8E%A5HBase%E7%9A%84%E6%AD%A3%E7%A1%AE%E5%A7%BF%E5%8A%BF" target="_blank" rel="noopener">连接 HBase 的正确姿势</a></li>
<li><a href="http://hbase.apache.org/book.htm" target="_blank" rel="noopener">Apache HBase ™ Reference Guide</a></li>
</ol>
]]></content>
      <categories>
        <category>notes</category>
        <category>HBase</category>
      </categories>
      <tags>
        <tag>HBase</tag>
      </tags>
  </entry>
  <entry>
    <title>分布式计算框架——MapReduce</title>
    <url>/2021/03/17/Hadoop-MapReduce/</url>
    <content><![CDATA[<h2 id="一、MapReduce概述"><a href="#一、MapReduce概述" class="headerlink" title="一、MapReduce概述"></a>一、MapReduce概述</h2><p>Hadoop MapReduce 是一个分布式计算框架，用于编写批处理应用程序。编写好的程序可以提交到 Hadoop 集群上用于并行处理大规模的数据集。</p>
<p>MapReduce 作业通过将输入的数据集拆分为独立的块，这些块由 <code>map</code> 以并行的方式处理，框架对 <code>map</code> 的输出进行排序，然后输入到 <code>reduce</code> 中。MapReduce 框架专门用于 <code>&lt;key，value&gt;</code> 键值对处理，它将作业的输入视为一组 <code>&lt;key，value&gt;</code> 对，并生成一组 <code>&lt;key，value&gt;</code> 对作为输出。输出和输出的 <code>key</code> 和 <code>value</code> 都必须实现<a href="http://hadoop.apache.org/docs/stable/api/org/apache/hadoop/io/Writable.html" target="_blank" rel="noopener">Writable</a> 接口。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(input) &lt;k1, v1&gt; -&gt; map -&gt; &lt;k2, v2&gt; -&gt; combine -&gt; &lt;k2, v2&gt; -&gt; reduce -&gt; &lt;k3, v3&gt; (output)</span><br></pre></td></tr></table></figure>
<h2 id="二、MapReduce编程模型简述"><a href="#二、MapReduce编程模型简述" class="headerlink" title="二、MapReduce编程模型简述"></a>二、MapReduce编程模型简述</h2><p>这里以词频统计为例进行说明，MapReduce 处理的流程如下：</p>
<div align="center"> <img width="600px" src="../pictures/mapreduceProcess.png"> </div>

<ol>
<li><p><strong>input</strong> : 读取文本文件；</p>
</li>
<li><p><strong>splitting</strong> : 将文件按照行进行拆分，此时得到的 <code>K1</code> 行数，<code>V1</code> 表示对应行的文本内容；</p>
</li>
<li><p><strong>mapping</strong> : 并行将每一行按照空格进行拆分，拆分得到的 <code>List(K2,V2)</code>，其中 <code>K2</code> 代表每一个单词，由于是做词频统计，所以 <code>V2</code> 的值为 1，代表出现 1 次；</p>
</li>
<li><strong>shuffling</strong>：由于 <code>Mapping</code> 操作可能是在不同的机器上并行处理的，所以需要通过 <code>shuffling</code> 将相同 <code>key</code> 值的数据分发到同一个节点上去合并，这样才能统计出最终的结果，此时得到 <code>K2</code> 为每一个单词，<code>List(V2)</code> 为可迭代集合，<code>V2</code> 就是 Mapping 中的 V2；</li>
<li><strong>Reducing</strong> : 这里的案例是统计单词出现的总次数，所以 <code>Reducing</code> 对 <code>List(V2)</code> 进行归约求和操作，最终输出。</li>
</ol>
<p>MapReduce 编程模型中 <code>splitting</code> 和 <code>shuffing</code> 操作都是由框架实现的，需要我们自己编程实现的只有 <code>mapping</code> 和 <code>reducing</code>，这也就是 MapReduce 这个称呼的来源。</p>
<h2 id="三、combiner-amp-partitioner"><a href="#三、combiner-amp-partitioner" class="headerlink" title="三、combiner &amp; partitioner"></a>三、combiner &amp; partitioner</h2><div align="center"> <img width="600px" src="../pictures/Detailed-Hadoop-MapReduce-Data-Flow-14.png"> </div>

<h3 id="3-1-InputFormat-amp-RecordReaders"><a href="#3-1-InputFormat-amp-RecordReaders" class="headerlink" title="3.1 InputFormat &amp; RecordReaders"></a>3.1 InputFormat &amp; RecordReaders</h3><p><code>InputFormat</code> 将输出文件拆分为多个 <code>InputSplit</code>，并由 <code>RecordReaders</code> 将 <code>InputSplit</code> 转换为标准的&lt;key，value&gt;键值对，作为 map 的输出。这一步的意义在于只有先进行逻辑拆分并转为标准的键值对格式后，才能为多个 <code>map</code> 提供输入，以便进行并行处理。</p>
<h3 id="3-2-Combiner"><a href="#3-2-Combiner" class="headerlink" title="3.2 Combiner"></a>3.2 Combiner</h3><p><code>combiner</code> 是 <code>map</code> 运算后的可选操作，它实际上是一个本地化的 <code>reduce</code> 操作，它主要是在 <code>map</code> 计算出中间文件后做一个简单的合并重复 <code>key</code> 值的操作。这里以词频统计为例：</p>
<p><code>map</code> 在遇到一个 hadoop 的单词时就会记录为 1，但是这篇文章里 hadoop 可能会出现 n 多次，那么 <code>map</code> 输出文件冗余就会很多，因此在 <code>reduce</code> 计算前对相同的 key 做一个合并操作，那么需要传输的数据量就会减少，传输效率就可以得到提升。</p>
<p>但并非所有场景都适合使用 <code>combiner</code>，使用它的原则是 <code>combiner</code> 的输出不会影响到 <code>reduce</code> 计算的最终输入，例如：求总数，最大值，最小值时都可以使用 <code>combiner</code>，但是做平均值计算则不能使用 <code>combiner</code>。</p>
<p>不使用 combiner 的情况：</p>
<div align="center"> <img width="600px" src="../pictures/mapreduce-without-combiners.png"> </div>

<p>使用 combiner 的情况：</p>
<div align="center"> <img width="600px" src="../pictures/mapreduce-with-combiners.png"> </div>



<p>可以看到使用 combiner 的时候，需要传输到 reducer 中的数据由 12keys，降低到 10keys。降低的幅度取决于你 keys 的重复率，下文词频统计案例会演示用 combiner 降低数百倍的传输量。</p>
<h3 id="3-3-Partitioner"><a href="#3-3-Partitioner" class="headerlink" title="3.3 Partitioner"></a>3.3 Partitioner</h3><p><code>partitioner</code> 可以理解成分类器，将 <code>map</code> 的输出按照 key 值的不同分别分给对应的 <code>reducer</code>，支持自定义实现，下文案例会给出演示。</p>
<h2 id="四、MapReduce词频统计案例"><a href="#四、MapReduce词频统计案例" class="headerlink" title="四、MapReduce词频统计案例"></a>四、MapReduce词频统计案例</h2><h3 id="4-1-项目简介"><a href="#4-1-项目简介" class="headerlink" title="4.1 项目简介"></a>4.1 项目简介</h3><p>这里给出一个经典的词频统计的案例：统计如下样本数据中每个单词出现的次数。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Spark	HBase</span><br><span class="line">Hive	Flink	Storm	Hadoop	HBase	Spark</span><br><span class="line">Flink</span><br><span class="line">HBase	Storm</span><br><span class="line">HBase	Hadoop	Hive	Flink</span><br><span class="line">HBase	Flink	Hive	Storm</span><br><span class="line">Hive	Flink	Hadoop</span><br><span class="line">HBase	Hive</span><br><span class="line">Hadoop	Spark	HBase	Storm</span><br><span class="line">HBase	Hadoop	Hive	Flink</span><br><span class="line">HBase	Flink	Hive	Storm</span><br><span class="line">Hive	Flink	Hadoop</span><br><span class="line">HBase	Hive</span><br></pre></td></tr></table></figure>
<p>为方便大家开发，我在项目源码中放置了一个工具类 <code>WordCountDataUtils</code>，用于模拟产生词频统计的样本，生成的文件支持输出到本地或者直接写到 HDFS 上。</p>
<blockquote>
<p>项目完整源码下载地址：<a href="https://github.com/heibaiying/BigData-Notes/tree/master/code/Hadoop/hadoop-word-count" target="_blank" rel="noopener">hadoop-word-count</a></p>
</blockquote>
<h3 id="4-2-项目依赖"><a href="#4-2-项目依赖" class="headerlink" title="4.2 项目依赖"></a>4.2 项目依赖</h3><p>想要进行 MapReduce 编程，需要导入 <code>hadoop-client</code> 依赖：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;hadoop.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h3 id="4-3-WordCountMapper"><a href="#4-3-WordCountMapper" class="headerlink" title="4.3 WordCountMapper"></a>4.3 WordCountMapper</h3><p>将每行数据按照指定分隔符进行拆分。这里需要注意在 MapReduce 中必须使用 Hadoop 定义的类型，因为 Hadoop 预定义的类型都是可序列化，可比较的，所有类型均实现了 <code>WritableComparable</code> 接口。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, </span></span><br><span class="line"><span class="function">                                                                      InterruptedException </span>&#123;</span><br><span class="line">        String[] words = value.toString().split(<span class="string">"\t"</span>);</span><br><span class="line">        <span class="keyword">for</span> (String word : words) &#123;</span><br><span class="line">            context.write(<span class="keyword">new</span> Text(word), <span class="keyword">new</span> IntWritable(<span class="number">1</span>));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>WordCountMapper</code> 对应下图的 Mapping 操作：</p>
<div align="center"> <img src="../pictures/hadoop-code-mapping.png"> </div>



<p><code>WordCountMapper</code> 继承自 <code>Mappe</code> 类，这是一个泛型类，定义如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">WordCountMapper extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Mapper</span>&lt;<span class="title">KEYIN</span>, <span class="title">VALUEIN</span>, <span class="title">KEYOUT</span>, <span class="title">VALUEOUT</span>&gt; </span>&#123;</span><br><span class="line">   ......</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>KEYIN</strong> : <code>mapping</code> 输入 key 的类型，即每行的偏移量 (每行第一个字符在整个文本中的位置)，<code>Long</code> 类型，对应 Hadoop 中的 <code>LongWritable</code> 类型；</li>
<li><strong>VALUEIN</strong> : <code>mapping</code> 输入 value 的类型，即每行数据；<code>String</code> 类型，对应 Hadoop 中 <code>Text</code> 类型；</li>
<li><strong>KEYOUT</strong> ：<code>mapping</code> 输出的 key 的类型，即每个单词；<code>String</code> 类型，对应 Hadoop 中 <code>Text</code> 类型；</li>
<li><strong>VALUEOUT</strong>：<code>mapping</code> 输出 value 的类型，即每个单词出现的次数；这里用 <code>int</code> 类型，对应 <code>IntWritable</code> 类型。</li>
</ul>
<h3 id="4-4-WordCountReducer"><a href="#4-4-WordCountReducer" class="headerlink" title="4.4 WordCountReducer"></a>4.4 WordCountReducer</h3><p>在 Reduce 中进行单词出现次数的统计：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">IntWritable</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, </span></span><br><span class="line"><span class="function">                                                                                  InterruptedException </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> count = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (IntWritable value : values) &#123;</span><br><span class="line">            count += value.get();</span><br><span class="line">        &#125;</span><br><span class="line">        context.write(key, <span class="keyword">new</span> IntWritable(count));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>如下图，<code>shuffling</code> 的输出是 reduce 的输入。这里的 key 是每个单词，values 是一个可迭代的数据类型，类似 <code>(1,1,1,...)</code>。</p>
<div align="center"> <img src="../pictures/hadoop-code-reducer.png"> </div>

<h3 id="4-4-WordCountApp"><a href="#4-4-WordCountApp" class="headerlink" title="4.4 WordCountApp"></a>4.4 WordCountApp</h3><p>组装 MapReduce 作业，并提交到服务器运行，代码如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 组装作业 并提交到集群运行</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountApp</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 这里为了直观显示参数 使用了硬编码，实际开发中可以通过外部传参</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String HDFS_URL = <span class="string">"hdfs://192.168.0.107:8020"</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String HADOOP_USER_NAME = <span class="string">"root"</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//  文件输入路径和输出路径由外部传参指定</span></span><br><span class="line">        <span class="keyword">if</span> (args.length &lt; <span class="number">2</span>) &#123;</span><br><span class="line">            System.out.println(<span class="string">"Input and output paths are necessary!"</span>);</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 需要指明 hadoop 用户名，否则在 HDFS 上创建目录时可能会抛出权限不足的异常</span></span><br><span class="line">        System.setProperty(<span class="string">"HADOOP_USER_NAME"</span>, HADOOP_USER_NAME);</span><br><span class="line"></span><br><span class="line">        Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">        <span class="comment">// 指明 HDFS 的地址</span></span><br><span class="line">        configuration.set(<span class="string">"fs.defaultFS"</span>, HDFS_URL);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 创建一个 Job</span></span><br><span class="line">        Job job = Job.getInstance(configuration);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置运行的主类</span></span><br><span class="line">        job.setJarByClass(WordCountApp<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置 Mapper 和 Reducer</span></span><br><span class="line">        job.setMapperClass(WordCountMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setReducerClass(WordCountReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置 Mapper 输出 key 和 value 的类型</span></span><br><span class="line">        job.setMapOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setMapOutputValueClass(IntWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置 Reducer 输出 key 和 value 的类型</span></span><br><span class="line">        job.setOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setOutputValueClass(IntWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 如果输出目录已经存在，则必须先删除，否则重复运行程序时会抛出异常</span></span><br><span class="line">        FileSystem fileSystem = FileSystem.get(<span class="keyword">new</span> URI(HDFS_URL), configuration, HADOOP_USER_NAME);</span><br><span class="line">        Path outputPath = <span class="keyword">new</span> Path(args[<span class="number">1</span>]);</span><br><span class="line">        <span class="keyword">if</span> (fileSystem.exists(outputPath)) &#123;</span><br><span class="line">            fileSystem.delete(outputPath, <span class="keyword">true</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置作业输入文件和输出文件的路径</span></span><br><span class="line">        FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, outputPath);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 将作业提交到群集并等待它完成，参数设置为 true 代表打印显示对应的进度</span></span><br><span class="line">        <span class="keyword">boolean</span> result = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 关闭之前创建的 fileSystem</span></span><br><span class="line">        fileSystem.close();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 根据作业结果,终止当前运行的 Java 虚拟机,退出程序</span></span><br><span class="line">        System.exit(result ? <span class="number">0</span> : -<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>需要注意的是：如果不设置 <code>Mapper</code> 操作的输出类型，则程序默认它和 <code>Reducer</code> 操作输出的类型相同。</p>
<h3 id="4-5-提交到服务器运行"><a href="#4-5-提交到服务器运行" class="headerlink" title="4.5 提交到服务器运行"></a>4.5 提交到服务器运行</h3><p>在实际开发中，可以在本机配置 hadoop 开发环境，直接在 IDE 中启动进行测试。这里主要介绍一下打包提交到服务器运行。由于本项目没有使用除 Hadoop 外的第三方依赖，直接打包即可：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> mvn clean package</span></span><br></pre></td></tr></table></figure>
<p>使用以下命令提交作业：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop jar /usr/appjar/hadoop-word-count-1.0.jar \</span><br><span class="line">com.heibaiying.WordCountApp \</span><br><span class="line">/wordcount/input.txt /wordcount/output/WordCountApp</span><br></pre></td></tr></table></figure>
<p>作业完成后查看 HDFS 上生成目录：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 查看目录</span></span><br><span class="line">hadoop fs -ls /wordcount/output/WordCountApp</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看统计结果</span></span><br><span class="line">hadoop fs -cat /wordcount/output/WordCountApp/part-r-00000</span><br></pre></td></tr></table></figure>
<div align="center"> <img src="../pictures/hadoop-wordcountapp.png"> </div>



<h2 id="五、词频统计案例进阶之Combiner"><a href="#五、词频统计案例进阶之Combiner" class="headerlink" title="五、词频统计案例进阶之Combiner"></a>五、词频统计案例进阶之Combiner</h2><h3 id="5-1-代码实现"><a href="#5-1-代码实现" class="headerlink" title="5.1 代码实现"></a>5.1 代码实现</h3><p>想要使用 <code>combiner</code> 功能只要在组装作业时，添加下面一行代码即可：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 设置 Combiner</span></span><br><span class="line">job.setCombinerClass(WordCountReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br></pre></td></tr></table></figure>
<h3 id="5-2-执行结果"><a href="#5-2-执行结果" class="headerlink" title="5.2 执行结果"></a>5.2 执行结果</h3><p>加入 <code>combiner</code> 后统计结果是不会有变化的，但是可以从打印的日志看出 <code>combiner</code> 的效果：</p>
<p>没有加入 <code>combiner</code> 的打印日志：</p>
<div align="center"> <img src="../pictures/hadoop-no-combiner.png"> </div>

<p>加入 <code>combiner</code> 后的打印日志如下：</p>
<div align="center"> <img src="../pictures/hadoop-combiner.png"> </div>

<p>这里我们只有一个输入文件并且小于 128M，所以只有一个 Map 进行处理。可以看到经过 combiner 后，records 由 <code>3519</code> 降低为 <code>6</code>(样本中单词种类就只有 6 种)，在这个用例中 combiner 就能极大地降低需要传输的数据量。</p>
<h2 id="六、词频统计案例进阶之Partitioner"><a href="#六、词频统计案例进阶之Partitioner" class="headerlink" title="六、词频统计案例进阶之Partitioner"></a>六、词频统计案例进阶之Partitioner</h2><h3 id="6-1-默认的Partitioner"><a href="#6-1-默认的Partitioner" class="headerlink" title="6.1  默认的Partitioner"></a>6.1  默认的Partitioner</h3><p>这里假设有个需求：将不同单词的统计结果输出到不同文件。这种需求实际上比较常见，比如统计产品的销量时，需要将结果按照产品种类进行拆分。要实现这个功能，就需要用到自定义 <code>Partitioner</code>。</p>
<p>这里先介绍下 MapReduce 默认的分类规则：在构建 job 时候，如果不指定，默认的使用的是 <code>HashPartitioner</code>：对 key 值进行哈希散列并对 <code>numReduceTasks</code> 取余。其实现如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HashPartitioner</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt; <span class="keyword">extends</span> <span class="title">Partitioner</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getPartition</span><span class="params">(K key, V value,</span></span></span><br><span class="line"><span class="function"><span class="params">                          <span class="keyword">int</span> numReduceTasks)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> (key.hashCode() &amp; Integer.MAX_VALUE) % numReduceTasks;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="6-2-自定义Partitioner"><a href="#6-2-自定义Partitioner" class="headerlink" title="6.2 自定义Partitioner"></a>6.2 自定义Partitioner</h3><p>这里我们继承 <code>Partitioner</code> 自定义分类规则，这里按照单词进行分类：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomPartitioner</span> <span class="keyword">extends</span> <span class="title">Partitioner</span>&lt;<span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getPartition</span><span class="params">(Text text, IntWritable intWritable, <span class="keyword">int</span> numPartitions)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> WordCountDataUtils.WORD_LIST.indexOf(text.toString());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在构建 <code>job</code> 时候指定使用我们自己的分类规则，并设置 <code>reduce</code> 的个数：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 设置自定义分区规则</span></span><br><span class="line">job.setPartitionerClass(CustomPartitioner<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"><span class="comment">// 设置 reduce 个数</span></span><br><span class="line">job.setNumReduceTasks(WordCountDataUtils.WORD_LIST.size());</span><br></pre></td></tr></table></figure>
<h3 id="6-3-执行结果"><a href="#6-3-执行结果" class="headerlink" title="6.3  执行结果"></a>6.3  执行结果</h3><p>执行结果如下，分别生成 6 个文件，每个文件中为对应单词的统计结果：</p>
<div align="center"> <img src="../pictures/hadoop-wordcountcombinerpartition.png"> </div>





<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol>
<li><a href="https://zhuanlan.zhihu.com/p/28682581" target="_blank" rel="noopener">分布式计算框架 MapReduce</a></li>
<li><a href="http://hadoop.apache.org/docs/stable/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html" target="_blank" rel="noopener">Apache Hadoop 2.9.2 &gt; MapReduce Tutorial</a></li>
<li><a href="https://www.tutorialscampus.com/tutorials/map-reduce/combiners.htm" target="_blank" rel="noopener">MapReduce - Combiners</a></li>
</ol>
]]></content>
      <categories>
        <category>notes</category>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title>集群资源管理器——YARN</title>
    <url>/2021/03/17/Hadoop-YARN/</url>
    <content><![CDATA[<h2 id="一、hadoop-yarn-简介"><a href="#一、hadoop-yarn-简介" class="headerlink" title="一、hadoop yarn 简介"></a>一、hadoop yarn 简介</h2><p><strong>Apache YARN</strong> (Yet Another Resource Negotiator)  是 hadoop 2.0 引入的集群资源管理系统。用户可以将各种服务框架部署在 YARN 上，由 YARN 进行统一地管理和资源分配。</p>
<div align="center"> <img width="600px" src="../pictures/yarn-base.png"> </div>



<h2 id="二、YARN架构"><a href="#二、YARN架构" class="headerlink" title="二、YARN架构"></a>二、YARN架构</h2><div align="center"> <img width="600px" src="../pictures/Figure3Architecture-of-YARN.png"> </div>

<h3 id="1-ResourceManager"><a href="#1-ResourceManager" class="headerlink" title="1. ResourceManager"></a>1. ResourceManager</h3><p><code>ResourceManager</code> 通常在独立的机器上以后台进程的形式运行，它是整个集群资源的主要协调者和管理者。<code>ResourceManager</code> 负责给用户提交的所有应用程序分配资源，它根据应用程序优先级、队列容量、ACLs、数据位置等信息，做出决策，然后以共享的、安全的、多租户的方式制定分配策略，调度集群资源。</p>
<h3 id="2-NodeManager"><a href="#2-NodeManager" class="headerlink" title="2. NodeManager"></a>2. NodeManager</h3><p><code>NodeManager</code> 是 YARN 集群中的每个具体节点的管理者。主要负责该节点内所有容器的生命周期的管理，监视资源和跟踪节点健康。具体如下：</p>
<ul>
<li>启动时向 <code>ResourceManager</code> 注册并定时发送心跳消息，等待 <code>ResourceManager</code> 的指令；</li>
<li>维护 <code>Container</code> 的生命周期，监控 <code>Container</code> 的资源使用情况；</li>
<li>管理任务运行时的相关依赖，根据 <code>ApplicationMaster</code> 的需要，在启动 <code>Container</code> 之前将需要的程序及其依赖拷贝到本地。</li>
</ul>
<h3 id="3-ApplicationMaster"><a href="#3-ApplicationMaster" class="headerlink" title="3. ApplicationMaster"></a>3. ApplicationMaster</h3><p>在用户提交一个应用程序时，YARN 会启动一个轻量级的进程 <code>ApplicationMaster</code>。<code>ApplicationMaster</code> 负责协调来自 <code>ResourceManager</code> 的资源，并通过 <code>NodeManager</code> 监视容器内资源的使用情况，同时还负责任务的监控与容错。具体如下：</p>
<ul>
<li>根据应用的运行状态来决定动态计算资源需求；</li>
<li>向 <code>ResourceManager</code> 申请资源，监控申请的资源的使用情况；</li>
<li>跟踪任务状态和进度，报告资源的使用情况和应用的进度信息；</li>
<li>负责任务的容错。</li>
</ul>
<h3 id="4-Contain"><a href="#4-Contain" class="headerlink" title="4. Contain"></a>4. Contain</h3><p><code>Container</code> 是 YARN 中的资源抽象，它封装了某个节点上的多维度资源，如内存、CPU、磁盘、网络等。当 AM 向 RM 申请资源时，RM 为 AM 返回的资源是用 <code>Container</code> 表示的。YARN 会为每个任务分配一个 <code>Container</code>，该任务只能使用该 <code>Container</code> 中描述的资源。<code>ApplicationMaster</code> 可在 <code>Container</code> 内运行任何类型的任务。例如，<code>MapReduce ApplicationMaster</code> 请求一个容器来启动 map 或 reduce 任务，而 <code>Giraph ApplicationMaster</code> 请求一个容器来运行 Giraph 任务。</p>
<h2 id="三、YARN工作原理简述"><a href="#三、YARN工作原理简述" class="headerlink" title="三、YARN工作原理简述"></a>三、YARN工作原理简述</h2><div align="center"> <img src="../pictures/yarn工作原理简图.png"> </div>

<ol>
<li><p><code>Client</code> 提交作业到 YARN 上；</p>
</li>
<li><p><code>Resource Manager</code> 选择一个 <code>Node Manager</code>，启动一个 <code>Container</code> 并运行 <code>Application Master</code> 实例；</p>
</li>
<li><p><code>Application Master</code> 根据实际需要向 <code>Resource Manager</code> 请求更多的 <code>Container</code> 资源（如果作业很小, 应用管理器会选择在其自己的 JVM 中运行任务）；</p>
</li>
<li><p><code>Application Master</code> 通过获取到的 <code>Container</code> 资源执行分布式计算。</p>
</li>
</ol>
<h2 id="四、YARN工作原理详述"><a href="#四、YARN工作原理详述" class="headerlink" title="四、YARN工作原理详述"></a>四、YARN工作原理详述</h2><div align="center"> <img width="600px" src="../pictures/yarn工作原理.png"> </div>



<h4 id="1-作业提交"><a href="#1-作业提交" class="headerlink" title="1. 作业提交"></a>1. 作业提交</h4><p>client 调用 job.waitForCompletion 方法，向整个集群提交 MapReduce 作业 (第 1 步) 。新的作业 ID(应用 ID) 由资源管理器分配 (第 2 步)。作业的 client 核实作业的输出, 计算输入的 split, 将作业的资源 (包括 Jar 包，配置文件, split 信息) 拷贝给 HDFS(第 3 步)。 最后, 通过调用资源管理器的 submitApplication() 来提交作业 (第 4 步)。</p>
<h4 id="2-作业初始化"><a href="#2-作业初始化" class="headerlink" title="2. 作业初始化"></a>2. 作业初始化</h4><p>当资源管理器收到 submitApplciation() 的请求时, 就将该请求发给调度器 (scheduler), 调度器分配 container, 然后资源管理器在该 container 内启动应用管理器进程, 由节点管理器监控 (第 5 步)。</p>
<p>MapReduce 作业的应用管理器是一个主类为 MRAppMaster 的 Java 应用，其通过创造一些 bookkeeping 对象来监控作业的进度,  得到任务的进度和完成报告 (第 6 步)。然后其通过分布式文件系统得到由客户端计算好的输入 split(第 7 步)，然后为每个输入 split 创建一个 map 任务, 根据 mapreduce.job.reduces 创建 reduce 任务对象。</p>
<h4 id="3-任务分配"><a href="#3-任务分配" class="headerlink" title="3. 任务分配"></a>3. 任务分配</h4><p>如果作业很小, 应用管理器会选择在其自己的 JVM 中运行任务。</p>
<p>如果不是小作业,  那么应用管理器向资源管理器请求 container 来运行所有的 map 和 reduce 任务 (第 8 步)。这些请求是通过心跳来传输的,  包括每个 map 任务的数据位置，比如存放输入 split 的主机名和机架 (rack)，调度器利用这些信息来调度任务，尽量将任务分配给存储数据的节点, 或者分配给和存放输入 split 的节点相同机架的节点。</p>
<h4 id="4-任务运行"><a href="#4-任务运行" class="headerlink" title="4. 任务运行"></a>4. 任务运行</h4><p>当一个任务由资源管理器的调度器分配给一个 container 后，应用管理器通过联系节点管理器来启动 container(第 9 步)。任务由一个主类为 YarnChild 的 Java 应用执行， 在运行任务之前首先本地化任务需要的资源，比如作业配置，JAR 文件,  以及分布式缓存的所有文件 (第 10 步。 最后, 运行 map 或 reduce 任务 (第 11 步)。</p>
<p>YarnChild 运行在一个专用的 JVM 中, 但是 YARN 不支持 JVM 重用。</p>
<h4 id="5-进度和状态更新"><a href="#5-进度和状态更新" class="headerlink" title="5. 进度和状态更新"></a>5. 进度和状态更新</h4><p>YARN 中的任务将其进度和状态 (包括 counter) 返回给应用管理器, 客户端每秒 (通 mapreduce.client.progressmonitor.pollinterval 设置) 向应用管理器请求进度更新, 展示给用户。</p>
<h4 id="6-作业完成"><a href="#6-作业完成" class="headerlink" title="6. 作业完成"></a>6. 作业完成</h4><p>除了向应用管理器请求作业进度外,  客户端每 5 分钟都会通过调用 waitForCompletion() 来检查作业是否完成，时间间隔可以通过 mapreduce.client.completion.pollinterval 来设置。作业完成之后,  应用管理器和 container 会清理工作状态， OutputCommiter 的作业清理方法也会被调用。作业的信息会被作业历史服务器存储以备之后用户核查。</p>
<h2 id="五、提交作业到YARN上运行"><a href="#五、提交作业到YARN上运行" class="headerlink" title="五、提交作业到YARN上运行"></a>五、提交作业到YARN上运行</h2><p>这里以提交 Hadoop Examples 中计算 Pi 的 MApReduce 程序为例，相关 Jar 包在 Hadoop 安装目录的 <code>share/hadoop/mapreduce</code> 目录下：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 提交格式: hadoop jar jar包路径 主类名称 主类参数</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> hadoop jar hadoop-mapreduce-examples-2.6.0-cdh5.15.2.jar pi 3 3</span></span><br></pre></td></tr></table></figure>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol>
<li><p><a href="https://www.cnblogs.com/codeOfLife/p/5492740.html" target="_blank" rel="noopener">初步掌握 Yarn 的架构及原理</a></p>
</li>
<li><p><a href="http://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/YARN.html" target="_blank" rel="noopener">Apache Hadoop 2.9.2 &gt; Apache Hadoop YARN</a></p>
</li>
</ol>
]]></content>
      <categories>
        <category>notes</category>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title>Hbase 常用 Shell 命令</title>
    <url>/2021/03/17/Hbase_Shell/</url>
    <content><![CDATA[<h2 id="一、基本命令"><a href="#一、基本命令" class="headerlink" title="一、基本命令"></a>一、基本命令</h2><p>打开 Hbase Shell：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> hbase shell</span></span><br></pre></td></tr></table></figure>
<h4 id="1-1-获取帮助"><a href="#1-1-获取帮助" class="headerlink" title="1.1 获取帮助"></a>1.1 获取帮助</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 获取帮助</span></span><br><span class="line">help</span><br><span class="line"><span class="meta">#</span><span class="bash"> 获取命令的详细信息</span></span><br><span class="line">help 'status'</span><br></pre></td></tr></table></figure>
<h4 id="1-2-查看服务器状态"><a href="#1-2-查看服务器状态" class="headerlink" title="1.2 查看服务器状态"></a>1.2 查看服务器状态</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">status</span><br></pre></td></tr></table></figure>
<h4 id="1-3-查看版本信息"><a href="#1-3-查看版本信息" class="headerlink" title="1.3 查看版本信息"></a>1.3 查看版本信息</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">version</span><br></pre></td></tr></table></figure>
<h2 id="二、关于表的操作"><a href="#二、关于表的操作" class="headerlink" title="二、关于表的操作"></a>二、关于表的操作</h2><h4 id="2-1-查看所有表"><a href="#2-1-查看所有表" class="headerlink" title="2.1 查看所有表"></a>2.1 查看所有表</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">list</span><br></pre></td></tr></table></figure>
<h4 id="2-2-创建表"><a href="#2-2-创建表" class="headerlink" title="2.2 创建表"></a>2.2 创建表</h4><p> <strong>命令格式</strong>： create ‘表名称’, ‘列族名称 1’,’列族名称 2’,’列名称 N’</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 创建一张名为Student的表,包含基本信息（baseInfo）、学校信息（schoolInfo）两个列族</span></span><br><span class="line">create 'Student','baseInfo','schoolInfo'</span><br></pre></td></tr></table></figure>
<h4 id="2-3-查看表的基本信息"><a href="#2-3-查看表的基本信息" class="headerlink" title="2.3 查看表的基本信息"></a>2.3 查看表的基本信息</h4><p> <strong>命令格式</strong>：desc ‘表名’</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">describe 'Student'</span><br></pre></td></tr></table></figure>
<h4 id="2-4-表的启用-禁用"><a href="#2-4-表的启用-禁用" class="headerlink" title="2.4 表的启用/禁用"></a>2.4 表的启用/禁用</h4><p>enable 和 disable 可以启用/禁用这个表,is_enabled 和 is_disabled 来检查表是否被禁用</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 禁用表</span></span><br><span class="line">disable 'Student'</span><br><span class="line"><span class="meta">#</span><span class="bash"> 检查表是否被禁用</span></span><br><span class="line">is_disabled 'Student'</span><br><span class="line"><span class="meta">#</span><span class="bash"> 启用表</span></span><br><span class="line">enable 'Student'</span><br><span class="line"><span class="meta">#</span><span class="bash"> 检查表是否被启用</span></span><br><span class="line">is_enabled 'Student'</span><br></pre></td></tr></table></figure>
<h4 id="2-5-检查表是否存在"><a href="#2-5-检查表是否存在" class="headerlink" title="2.5 检查表是否存在"></a>2.5 检查表是否存在</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">exists 'Student'</span><br></pre></td></tr></table></figure>
<h4 id="2-6-删除表"><a href="#2-6-删除表" class="headerlink" title="2.6 删除表"></a>2.6 删除表</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 删除表前需要先禁用表</span></span><br><span class="line">disable 'Student'</span><br><span class="line"><span class="meta">#</span><span class="bash"> 删除表</span></span><br><span class="line">drop 'Student'</span><br></pre></td></tr></table></figure>
<h2 id="三、增删改"><a href="#三、增删改" class="headerlink" title="三、增删改"></a>三、增删改</h2><h4 id="3-1-添加列族"><a href="#3-1-添加列族" class="headerlink" title="3.1 添加列族"></a>3.1 添加列族</h4><p> <strong>命令格式</strong>： alter ‘表名’, ‘列族名’</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">alter 'Student', 'teacherInfo'</span><br></pre></td></tr></table></figure>
<h4 id="3-2-删除列族"><a href="#3-2-删除列族" class="headerlink" title="3.2 删除列族"></a>3.2 删除列族</h4><p> <strong>命令格式</strong>：alter ‘表名’, {NAME =&gt; ‘列族名’, METHOD =&gt; ‘delete’}</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">alter 'Student', &#123;NAME =&gt; 'teacherInfo', METHOD =&gt; 'delete'&#125;</span><br></pre></td></tr></table></figure>
<h4 id="3-3-更改列族存储版本的限制"><a href="#3-3-更改列族存储版本的限制" class="headerlink" title="3.3 更改列族存储版本的限制"></a>3.3 更改列族存储版本的限制</h4><p>默认情况下，列族只存储一个版本的数据，如果需要存储多个版本的数据，则需要修改列族的属性。修改后可通过 <code>desc</code> 命令查看。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">alter 'Student',&#123;NAME=&gt;'baseInfo',VERSIONS=&gt;3&#125;</span><br></pre></td></tr></table></figure>
<h4 id="3-4-插入数据"><a href="#3-4-插入数据" class="headerlink" title="3.4 插入数据"></a>3.4 插入数据</h4><p> <strong>命令格式</strong>：put ‘表名’, ‘行键’,’列族:列’,’值’</p>
<p><strong>注意：如果新增数据的行键值、列族名、列名与原有数据完全相同，则相当于更新操作</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">put 'Student', 'rowkey1','baseInfo:name','tom'</span><br><span class="line">put 'Student', 'rowkey1','baseInfo:birthday','1990-01-09'</span><br><span class="line">put 'Student', 'rowkey1','baseInfo:age','29'</span><br><span class="line">put 'Student', 'rowkey1','schoolInfo:name','Havard'</span><br><span class="line">put 'Student', 'rowkey1','schoolInfo:localtion','Boston'</span><br><span class="line"></span><br><span class="line">put 'Student', 'rowkey2','baseInfo:name','jack'</span><br><span class="line">put 'Student', 'rowkey2','baseInfo:birthday','1998-08-22'</span><br><span class="line">put 'Student', 'rowkey2','baseInfo:age','21'</span><br><span class="line">put 'Student', 'rowkey2','schoolInfo:name','yale'</span><br><span class="line">put 'Student', 'rowkey2','schoolInfo:localtion','New Haven'</span><br><span class="line"></span><br><span class="line">put 'Student', 'rowkey3','baseInfo:name','maike'</span><br><span class="line">put 'Student', 'rowkey3','baseInfo:birthday','1995-01-22'</span><br><span class="line">put 'Student', 'rowkey3','baseInfo:age','24'</span><br><span class="line">put 'Student', 'rowkey3','schoolInfo:name','yale'</span><br><span class="line">put 'Student', 'rowkey3','schoolInfo:localtion','New Haven'</span><br><span class="line"></span><br><span class="line">put 'Student', 'wrowkey4','baseInfo:name','maike-jack'</span><br></pre></td></tr></table></figure>
<h4 id="3-5-获取指定行、指定行中的列族、列的信息"><a href="#3-5-获取指定行、指定行中的列族、列的信息" class="headerlink" title="3.5 获取指定行、指定行中的列族、列的信息"></a>3.5 获取指定行、指定行中的列族、列的信息</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 获取指定行中所有列的数据信息</span></span><br><span class="line">get 'Student','rowkey3'</span><br><span class="line"><span class="meta">#</span><span class="bash"> 获取指定行中指定列族下所有列的数据信息</span></span><br><span class="line">get 'Student','rowkey3','baseInfo'</span><br><span class="line"><span class="meta">#</span><span class="bash"> 获取指定行中指定列的数据信息</span></span><br><span class="line">get 'Student','rowkey3','baseInfo:name'</span><br></pre></td></tr></table></figure>
<h4 id="3-6-删除指定行、指定行中的列"><a href="#3-6-删除指定行、指定行中的列" class="headerlink" title="3.6 删除指定行、指定行中的列"></a>3.6 删除指定行、指定行中的列</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 删除指定行</span></span><br><span class="line">delete 'Student','rowkey3'</span><br><span class="line"><span class="meta">#</span><span class="bash"> 删除指定行中指定列的数据</span></span><br><span class="line">delete 'Student','rowkey3','baseInfo:name'</span><br></pre></td></tr></table></figure>
<h2 id="四、查询"><a href="#四、查询" class="headerlink" title="四、查询"></a>四、查询</h2><p>hbase 中访问数据有两种基本的方式：</p>
<ul>
<li><p>按指定 rowkey 获取数据：get 方法；</p>
</li>
<li><p>按指定条件获取数据：scan 方法。</p>
</li>
</ul>
<p><code>scan</code> 可以设置 begin 和 end 参数来访问一个范围内所有的数据。get 本质上就是 begin 和 end 相等的一种特殊的 scan。</p>
<h4 id="4-1Get查询"><a href="#4-1Get查询" class="headerlink" title="4.1Get查询"></a>4.1Get查询</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 获取指定行中所有列的数据信息</span></span><br><span class="line">get 'Student','rowkey3'</span><br><span class="line"><span class="meta">#</span><span class="bash"> 获取指定行中指定列族下所有列的数据信息</span></span><br><span class="line">get 'Student','rowkey3','baseInfo'</span><br><span class="line"><span class="meta">#</span><span class="bash"> 获取指定行中指定列的数据信息</span></span><br><span class="line">get 'Student','rowkey3','baseInfo:name'</span><br></pre></td></tr></table></figure>
<h4 id="4-2-查询整表数据"><a href="#4-2-查询整表数据" class="headerlink" title="4.2 查询整表数据"></a>4.2 查询整表数据</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">scan 'Student'</span><br></pre></td></tr></table></figure>
<h4 id="4-3-查询指定列簇的数据"><a href="#4-3-查询指定列簇的数据" class="headerlink" title="4.3 查询指定列簇的数据"></a>4.3 查询指定列簇的数据</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">scan 'Student', &#123;COLUMN=&gt;'baseInfo'&#125;</span><br></pre></td></tr></table></figure>
<h4 id="4-4-条件查询"><a href="#4-4-条件查询" class="headerlink" title="4.4  条件查询"></a>4.4  条件查询</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 查询指定列的数据</span></span><br><span class="line">scan 'Student', &#123;COLUMNS=&gt; 'baseInfo:birthday'&#125;</span><br></pre></td></tr></table></figure>
<p>除了列 <code>（COLUMNS）</code> 修饰词外，HBase 还支持 <code>Limit</code>（限制查询结果行数），<code>STARTROW</code>（<code>ROWKEY</code> 起始行，会先根据这个 <code>key</code> 定位到 <code>region</code>，再向后扫描）、<code>STOPROW</code>(结束行)、<code>TIMERANGE</code>（限定时间戳范围）、<code>VERSIONS</code>（版本数）、和 <code>FILTER</code>（按条件过滤行）等。</p>
<p>如下代表从 <code>rowkey2</code> 这个 <code>rowkey</code> 开始，查找下两个行的最新 3 个版本的 name 列的数据：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">scan 'Student', &#123;COLUMNS=&gt; 'baseInfo:name',STARTROW =&gt; 'rowkey2',STOPROW =&gt; 'wrowkey4',LIMIT=&gt;2, VERSIONS=&gt;3&#125;</span><br></pre></td></tr></table></figure>
<h4 id="4-5-条件过滤"><a href="#4-5-条件过滤" class="headerlink" title="4.5  条件过滤"></a>4.5  条件过滤</h4><p>Filter 可以设定一系列条件来进行过滤。如我们要查询值等于 24 的所有数据：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">scan 'Student', FILTER=&gt;"ValueFilter(=,'binary:24')"</span><br></pre></td></tr></table></figure>
<p>值包含 yale 的所有数据：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">scan 'Student', FILTER=&gt;"ValueFilter(=,'substring:yale')"</span><br></pre></td></tr></table></figure>
<p>列名中的前缀为 birth 的：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">scan 'Student', FILTER=&gt;"ColumnPrefixFilter('birth')"</span><br></pre></td></tr></table></figure>
<p>FILTER 中支持多个过滤条件通过括号、AND 和 OR 进行组合：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 列名中的前缀为birth且列值中包含1998的数据</span></span><br><span class="line">scan 'Student', FILTER=&gt;"ColumnPrefixFilter('birth') AND ValueFilter ValueFilter(=,'substring:1998')"</span><br></pre></td></tr></table></figure>
<p><code>PrefixFilter</code> 用于对 Rowkey 的前缀进行判断：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">scan 'Student', FILTER=&gt;"PrefixFilter('wr')"</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>notes</category>
        <category>HBase</category>
      </categories>
      <tags>
        <tag>HBase</tag>
      </tags>
  </entry>
  <entry>
    <title>Hbase 协处理器</title>
    <url>/2021/03/17/Hbase%E5%8D%8F%E5%A4%84%E7%90%86%E5%99%A8%E8%AF%A6%E8%A7%A3/</url>
    <content><![CDATA[<h2 id="一、简述"><a href="#一、简述" class="headerlink" title="一、简述"></a>一、简述</h2><p>在使用 HBase 时，如果你的数据量达到了数十亿行或数百万列，此时能否在查询中返回大量数据将受制于网络的带宽，即便网络状况允许，但是客户端的计算处理也未必能够满足要求。在这种情况下，协处理器（Coprocessors）应运而生。它允许你将业务计算代码放入在 RegionServer 的协处理器中，将处理好的数据再返回给客户端，这可以极大地降低需要传输的数据量，从而获得性能上的提升。同时协处理器也允许用户扩展实现 HBase 目前所不具备的功能，如权限校验、二级索引、完整性约束等。</p>
<h2 id="二、协处理器类型"><a href="#二、协处理器类型" class="headerlink" title="二、协处理器类型"></a>二、协处理器类型</h2><h3 id="2-1-Observer协处理器"><a href="#2-1-Observer协处理器" class="headerlink" title="2.1 Observer协处理器"></a>2.1 Observer协处理器</h3><h4 id="1-功能"><a href="#1-功能" class="headerlink" title="1. 功能"></a>1. 功能</h4><p>Observer 协处理器类似于关系型数据库中的触发器，当发生某些事件的时候这类协处理器会被 Server 端调用。通常可以用来实现下面功能：</p>
<ul>
<li><strong>权限校验</strong>：在执行 <code>Get</code> 或 <code>Put</code> 操作之前，您可以使用 <code>preGet</code> 或 <code>prePut</code> 方法检查权限；</li>
<li><strong>完整性约束</strong>： HBase 不支持关系型数据库中的外键功能，可以通过触发器在插入或者删除数据的时候，对关联的数据进行检查；</li>
<li><strong>二级索引</strong>： 可以使用协处理器来维护二级索引。</li>
</ul>
<p><br></p>
<h4 id="2-类型"><a href="#2-类型" class="headerlink" title="2. 类型"></a>2. 类型</h4><p>当前 Observer 协处理器有以下四种类型：</p>
<ul>
<li><strong>RegionObserver</strong> :<br>允许您观察 Region 上的事件，例如 Get 和 Put 操作。</li>
<li><strong>RegionServerObserver</strong> :<br>允许您观察与 RegionServer 操作相关的事件，例如启动，停止或执行合并，提交或回滚。</li>
<li><strong>MasterObserver</strong> :<br>允许您观察与 HBase Master 相关的事件，例如表创建，删除或 schema 修改。</li>
<li><strong>WalObserver</strong> :<br>允许您观察与预写日志（WAL）相关的事件。</li>
</ul>
<p><br></p>
<h4 id="3-接口"><a href="#3-接口" class="headerlink" title="3. 接口"></a>3. 接口</h4><p>以上四种类型的 Observer 协处理器均继承自 <code>Coprocessor</code> 接口，这四个接口中分别定义了所有可用的钩子方法，以便在对应方法前后执行特定的操作。通常情况下，我们并不会直接实现上面接口，而是继承其 Base 实现类，Base 实现类只是简单空实现了接口中的方法，这样我们在实现自定义的协处理器时，就不必实现所有方法，只需要重写必要方法即可。</p>
<div align="center"> <img src="../pictures/hbase-coprocessor.png"> </div>

<p>这里以 <code>RegionObservers</code> 为例，其接口类中定义了所有可用的钩子方法，下面截取了部分方法的定义，多数方法都是成对出现的，有 <code>pre</code> 就有 <code>post</code>：</p>
<div align="center"> <img src="../pictures/RegionObserver.png"> </div>

<p><br></p>
<h4 id="4-执行流程"><a href="#4-执行流程" class="headerlink" title="4. 执行流程"></a>4. 执行流程</h4><div align="center"> <img src="../pictures/RegionObservers-works.png"> </div>

<ul>
<li>客户端发出 put 请求</li>
<li>该请求被分派给合适的 RegionServer 和 region</li>
<li>coprocessorHost 拦截该请求，然后在该表的每个 RegionObserver 上调用 prePut()</li>
<li>如果没有被 <code>prePut()</code> 拦截，该请求继续送到 region，然后进行处理</li>
<li>region 产生的结果再次被 CoprocessorHost 拦截，调用 <code>postPut()</code></li>
<li>假如没有 <code>postPut()</code> 拦截该响应，最终结果被返回给客户端</li>
</ul>
<p>如果大家了解 Spring，可以将这种执行方式类比于其 AOP 的执行原理即可，官方文档当中也是这样类比的：</p>
<blockquote>
<p>If you are familiar with Aspect Oriented Programming (AOP), you can think of a coprocessor as applying advice by intercepting a request and then running some custom code,before passing the request on to its final destination (or even changing the destination).</p>
<p>如果您熟悉面向切面编程（AOP），您可以将协处理器视为通过拦截请求然后运行一些自定义代码来使用 Advice，然后将请求传递到其最终目标（或者更改目标）。</p>
</blockquote>
<h3 id="2-2-Endpoint协处理器"><a href="#2-2-Endpoint协处理器" class="headerlink" title="2.2  Endpoint协处理器"></a>2.2  Endpoint协处理器</h3><p>Endpoint 协处理器类似于关系型数据库中的存储过程。客户端可以调用 Endpoint 协处理器在服务端对数据进行处理，然后再返回。</p>
<p>以聚集操作为例，如果没有协处理器，当用户需要找出一张表中的最大数据，即 max 聚合操作，就必须进行全表扫描，然后在客户端上遍历扫描结果，这必然会加重了客户端处理数据的压力。利用 Coprocessor，用户可以将求最大值的代码部署到 HBase Server 端，HBase 将利用底层 cluster 的多个节点并发执行求最大值的操作。即在每个 Region 范围内执行求最大值的代码，将每个 Region 的最大值在 Region Server 端计算出来，仅仅将该 max 值返回给客户端。之后客户端只需要将每个 Region 的最大值进行比较而找到其中最大的值即可。</p>
<h2 id="三、协处理的加载方式"><a href="#三、协处理的加载方式" class="headerlink" title="三、协处理的加载方式"></a>三、协处理的加载方式</h2><p>要使用我们自己开发的协处理器，必须通过静态（使用 HBase 配置）或动态（使用 HBase Shell 或 Java API）加载它。</p>
<ul>
<li>静态加载的协处理器称之为 <strong>System Coprocessor</strong>（系统级协处理器）,作用范围是整个 HBase 上的所有表，需要重启 HBase 服务；</li>
<li>动态加载的协处理器称之为 <strong>Table Coprocessor</strong>（表处理器），作用于指定的表，不需要重启 HBase 服务。</li>
</ul>
<p>其加载和卸载方式分别介绍如下。</p>
<h2 id="四、静态加载与卸载"><a href="#四、静态加载与卸载" class="headerlink" title="四、静态加载与卸载"></a>四、静态加载与卸载</h2><h3 id="4-1-静态加载"><a href="#4-1-静态加载" class="headerlink" title="4.1 静态加载"></a>4.1 静态加载</h3><p>静态加载分以下三步：</p>
<ol>
<li>在 <code>hbase-site.xml</code> 定义需要加载的协处理器。</li>
</ol>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.coprocessor.region.classes<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.myname.hbase.coprocessor.endpoint.SumEndPoint<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p> <code>&lt;name&gt;</code> 标签的值必须是下面其中之一：</p>
<ul>
<li>RegionObservers 和 Endpoints 协处理器：<code>hbase.coprocessor.region.classes</code> </li>
<li>WALObservers 协处理器： <code>hbase.coprocessor.wal.classes</code> </li>
<li><p>MasterObservers 协处理器：<code>hbase.coprocessor.master.classes</code></p>
<p><code>&lt;value&gt;</code> 必须是协处理器实现类的全限定类名。如果为加载指定了多个类，则类名必须以逗号分隔。</p>
</li>
</ul>
<ol start="2">
<li><p>将 jar(包含代码和所有依赖项) 放入 HBase 安装目录中的 <code>lib</code> 目录下；</p>
</li>
<li><p>重启 HBase。</p>
</li>
</ol>
<p><br></p>
<h3 id="4-2-静态卸载"><a href="#4-2-静态卸载" class="headerlink" title="4.2 静态卸载"></a>4.2 静态卸载</h3><ol>
<li><p>从 hbase-site.xml 中删除配置的协处理器的\<property>元素及其子元素；</property></p>
</li>
<li><p>从类路径或 HBase 的 lib 目录中删除协处理器的 JAR 文件（可选）；</p>
</li>
<li><p>重启 HBase。</p>
</li>
</ol>
<h2 id="五、动态加载与卸载"><a href="#五、动态加载与卸载" class="headerlink" title="五、动态加载与卸载"></a>五、动态加载与卸载</h2><p>使用动态加载协处理器，不需要重新启动 HBase。但动态加载的协处理器是基于每个表加载的，只能用于所指定的表。<br>此外，在使用动态加载必须使表脱机（disable）以加载协处理器。动态加载通常有两种方式：Shell 和 Java API 。 </p>
<blockquote>
<p>以下示例基于两个前提：</p>
<ol>
<li>coprocessor.jar 包含协处理器实现及其所有依赖项。</li>
<li>JAR 包存放在 HDFS 上的路径为：hdfs：// \<namenode>：\<port> / user / \<hadoop-user> /coprocessor.jar</hadoop-user></port></namenode></li>
</ol>
</blockquote>
<h3 id="5-1-HBase-Shell动态加载"><a href="#5-1-HBase-Shell动态加载" class="headerlink" title="5.1 HBase Shell动态加载"></a>5.1 HBase Shell动态加载</h3><ol>
<li>使用 HBase Shell 禁用表</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hbase &gt; disable 'tableName'</span><br></pre></td></tr></table></figure>
<ol start="2">
<li>使用如下命令加载协处理器</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hbase &gt; alter 'tableName', METHOD =&gt; 'table_att', 'Coprocessor'=&gt;'hdfs://&lt;namenode&gt;:&lt;port&gt;/</span><br><span class="line">user/&lt;hadoop-user&gt;/coprocessor.jar| org.myname.hbase.Coprocessor.RegionObserverExample|1073741823|</span><br><span class="line">arg1=1,arg2=2'</span><br></pre></td></tr></table></figure>
<p><code>Coprocessor</code> 包含由管道（|）字符分隔的四个参数，按顺序解释如下：</p>
<ul>
<li><strong>JAR 包路径</strong>：通常为 JAR 包在 HDFS 上的路径。关于路径以下两点需要注意：</li>
<li><p>允许使用通配符，例如：<code>hdfs://&lt;namenode&gt;:&lt;port&gt;/user/&lt;hadoop-user&gt;/*.jar</code> 来添加指定的 JAR 包；</p>
</li>
<li><p>可以使指定目录，例如：<code>hdfs://&lt;namenode&gt;:&lt;port&gt;/user/&lt;hadoop-user&gt;/</code> ，这会添加目录中的所有 JAR 包，但不会搜索子目录中的 JAR 包。</p>
</li>
<li><p><strong>类名</strong>：协处理器的完整类名。</p>
</li>
<li><strong>优先级</strong>：协处理器的优先级，遵循数字的自然序，即值越小优先级越高。可以为空，在这种情况下，将分配默认优先级值。</li>
<li><strong>可选参数</strong> ：传递的协处理器的可选参数。</li>
</ul>
<ol start="3">
<li>启用表</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hbase &gt; enable 'tableName'</span><br></pre></td></tr></table></figure>
<ol start="4">
<li>验证协处理器是否已加载</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hbase &gt; describe 'tableName'</span><br></pre></td></tr></table></figure>
<p>协处理器出现在 <code>TABLE_ATTRIBUTES</code> 属性中则代表加载成功。</p>
<p><br></p>
<h3 id="5-2-HBase-Shell动态卸载"><a href="#5-2-HBase-Shell动态卸载" class="headerlink" title="5.2 HBase Shell动态卸载"></a>5.2 HBase Shell动态卸载</h3><ol>
<li><p>禁用表</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">hbase&gt;</span><span class="bash"> <span class="built_in">disable</span> <span class="string">'tableName'</span></span></span><br></pre></td></tr></table></figure>
</li>
<li><p>移除表协处理器</p>
</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">hbase&gt;</span><span class="bash"> alter <span class="string">'tableName'</span>, METHOD =&gt; <span class="string">'table_att_unset'</span>, NAME =&gt; <span class="string">'coprocessor$1'</span></span></span><br></pre></td></tr></table></figure>
<ol start="3">
<li>启用表</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">hbase&gt;</span><span class="bash"> <span class="built_in">enable</span> <span class="string">'tableName'</span></span></span><br></pre></td></tr></table></figure>
<p><br></p>
<h3 id="5-3-Java-API-动态加载"><a href="#5-3-Java-API-动态加载" class="headerlink" title="5.3 Java API 动态加载"></a>5.3 Java API 动态加载</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">TableName tableName = TableName.valueOf(<span class="string">"users"</span>);</span><br><span class="line">String path = <span class="string">"hdfs://&lt;namenode&gt;:&lt;port&gt;/user/&lt;hadoop-user&gt;/coprocessor.jar"</span>;</span><br><span class="line">Configuration conf = HBaseConfiguration.create();</span><br><span class="line">Connection connection = ConnectionFactory.createConnection(conf);</span><br><span class="line">Admin admin = connection.getAdmin();</span><br><span class="line">admin.disableTable(tableName);</span><br><span class="line">HTableDescriptor hTableDescriptor = <span class="keyword">new</span> HTableDescriptor(tableName);</span><br><span class="line">HColumnDescriptor columnFamily1 = <span class="keyword">new</span> HColumnDescriptor(<span class="string">"personalDet"</span>);</span><br><span class="line">columnFamily1.setMaxVersions(<span class="number">3</span>);</span><br><span class="line">hTableDescriptor.addFamily(columnFamily1);</span><br><span class="line">HColumnDescriptor columnFamily2 = <span class="keyword">new</span> HColumnDescriptor(<span class="string">"salaryDet"</span>);</span><br><span class="line">columnFamily2.setMaxVersions(<span class="number">3</span>);</span><br><span class="line">hTableDescriptor.addFamily(columnFamily2);</span><br><span class="line">hTableDescriptor.setValue(<span class="string">"COPROCESSOR$1"</span>, path + <span class="string">"|"</span></span><br><span class="line">+ RegionObserverExample.class.getCanonicalName() + "|"</span><br><span class="line">+ Coprocessor.PRIORITY_USER);</span><br><span class="line">admin.modifyTable(tableName, hTableDescriptor);</span><br><span class="line">admin.enableTable(tableName);</span><br></pre></td></tr></table></figure>
<p>在 HBase 0.96 及其以后版本中，HTableDescriptor 的 addCoprocessor() 方法提供了一种更为简便的加载方法。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">TableName tableName = TableName.valueOf(<span class="string">"users"</span>);</span><br><span class="line">Path path = <span class="keyword">new</span> Path(<span class="string">"hdfs://&lt;namenode&gt;:&lt;port&gt;/user/&lt;hadoop-user&gt;/coprocessor.jar"</span>);</span><br><span class="line">Configuration conf = HBaseConfiguration.create();</span><br><span class="line">Connection connection = ConnectionFactory.createConnection(conf);</span><br><span class="line">Admin admin = connection.getAdmin();</span><br><span class="line">admin.disableTable(tableName);</span><br><span class="line">HTableDescriptor hTableDescriptor = <span class="keyword">new</span> HTableDescriptor(tableName);</span><br><span class="line">HColumnDescriptor columnFamily1 = <span class="keyword">new</span> HColumnDescriptor(<span class="string">"personalDet"</span>);</span><br><span class="line">columnFamily1.setMaxVersions(<span class="number">3</span>);</span><br><span class="line">hTableDescriptor.addFamily(columnFamily1);</span><br><span class="line">HColumnDescriptor columnFamily2 = <span class="keyword">new</span> HColumnDescriptor(<span class="string">"salaryDet"</span>);</span><br><span class="line">columnFamily2.setMaxVersions(<span class="number">3</span>);</span><br><span class="line">hTableDescriptor.addFamily(columnFamily2);</span><br><span class="line">hTableDescriptor.addCoprocessor(RegionObserverExample<span class="class">.<span class="keyword">class</span>.<span class="title">getCanonicalName</span>(), <span class="title">path</span>,</span></span><br><span class="line"><span class="class"><span class="title">Coprocessor</span>.<span class="title">PRIORITY_USER</span>, <span class="title">null</span>)</span>;</span><br><span class="line">admin.modifyTable(tableName, hTableDescriptor);</span><br><span class="line">admin.enableTable(tableName);</span><br></pre></td></tr></table></figure>
<h3 id="5-4-Java-API-动态卸载"><a href="#5-4-Java-API-动态卸载" class="headerlink" title="5.4 Java API 动态卸载"></a>5.4 Java API 动态卸载</h3><p>卸载其实就是重新定义表但不设置协处理器。这会删除所有表上的协处理器。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">TableName tableName = TableName.valueOf(<span class="string">"users"</span>);</span><br><span class="line">String path = <span class="string">"hdfs://&lt;namenode&gt;:&lt;port&gt;/user/&lt;hadoop-user&gt;/coprocessor.jar"</span>;</span><br><span class="line">Configuration conf = HBaseConfiguration.create();</span><br><span class="line">Connection connection = ConnectionFactory.createConnection(conf);</span><br><span class="line">Admin admin = connection.getAdmin();</span><br><span class="line">admin.disableTable(tableName);</span><br><span class="line">HTableDescriptor hTableDescriptor = <span class="keyword">new</span> HTableDescriptor(tableName);</span><br><span class="line">HColumnDescriptor columnFamily1 = <span class="keyword">new</span> HColumnDescriptor(<span class="string">"personalDet"</span>);</span><br><span class="line">columnFamily1.setMaxVersions(<span class="number">3</span>);</span><br><span class="line">hTableDescriptor.addFamily(columnFamily1);</span><br><span class="line">HColumnDescriptor columnFamily2 = <span class="keyword">new</span> HColumnDescriptor(<span class="string">"salaryDet"</span>);</span><br><span class="line">columnFamily2.setMaxVersions(<span class="number">3</span>);</span><br><span class="line">hTableDescriptor.addFamily(columnFamily2);</span><br><span class="line">admin.modifyTable(tableName, hTableDescriptor);</span><br><span class="line">admin.enableTable(tableName);</span><br></pre></td></tr></table></figure>
<h2 id="六、协处理器案例"><a href="#六、协处理器案例" class="headerlink" title="六、协处理器案例"></a>六、协处理器案例</h2><p>这里给出一个简单的案例，实现一个类似于 Redis 中 <code>append</code> 命令的协处理器，当我们对已有列执行 put 操作时候，HBase 默认执行的是 update 操作，这里我们修改为执行 append 操作。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> redis append 命令示例</span></span><br><span class="line"><span class="meta">redis&gt;</span><span class="bash">  EXISTS mykey</span></span><br><span class="line">(integer) 0</span><br><span class="line"><span class="meta">redis&gt;</span><span class="bash">  APPEND mykey <span class="string">"Hello"</span></span></span><br><span class="line">(integer) 5</span><br><span class="line"><span class="meta">redis&gt;</span><span class="bash">  APPEND mykey <span class="string">" World"</span></span></span><br><span class="line">(integer) 11</span><br><span class="line"><span class="meta">redis&gt;</span><span class="bash">  GET mykey </span></span><br><span class="line">"Hello World"</span><br></pre></td></tr></table></figure>
<h3 id="6-1-创建测试表"><a href="#6-1-创建测试表" class="headerlink" title="6.1 创建测试表"></a>6.1 创建测试表</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 创建一张杂志表 有文章和图片两个列族</span></span><br><span class="line">hbase &gt;  create 'magazine','article','picture'</span><br></pre></td></tr></table></figure>
<h3 id="6-2-协处理器编程"><a href="#6-2-协处理器编程" class="headerlink" title="6.2 协处理器编程"></a>6.2 协处理器编程</h3><blockquote>
<p>完整代码可见本仓库：<a href="https://github.com/heibaiying/BigData-Notes/tree/master/code/Hbase\hbase-observer-coprocessor" target="_blank" rel="noopener">hbase-observer-coprocessor</a></p>
</blockquote>
<p>新建 Maven 工程，导入下面依赖：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hbase<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hbase-common<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hbase<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hbase-server<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>继承 <code>BaseRegionObserver</code> 实现我们自定义的 <code>RegionObserver</code>,对相同的 <code>article:content</code> 执行 put 命令时，将新插入的内容添加到原有内容的末尾，代码如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">AppendRegionObserver</span> <span class="keyword">extends</span> <span class="title">BaseRegionObserver</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">byte</span>[] columnFamily = Bytes.toBytes(<span class="string">"article"</span>);</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">byte</span>[] qualifier = Bytes.toBytes(<span class="string">"content"</span>);</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">prePut</span><span class="params">(ObserverContext&lt;RegionCoprocessorEnvironment&gt; e, Put put, WALEdit edit,</span></span></span><br><span class="line"><span class="function"><span class="params">                       Durability durability)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (put.has(columnFamily, qualifier)) &#123;</span><br><span class="line">            <span class="comment">// 遍历查询结果，获取指定列的原值</span></span><br><span class="line">            Result rs = e.getEnvironment().getRegion().get(<span class="keyword">new</span> Get(put.getRow()));</span><br><span class="line">            String oldValue = <span class="string">""</span>;</span><br><span class="line">            <span class="keyword">for</span> (Cell cell : rs.rawCells())</span><br><span class="line">                <span class="keyword">if</span> (CellUtil.matchingColumn(cell, columnFamily, qualifier)) &#123;</span><br><span class="line">                    oldValue = Bytes.toString(CellUtil.cloneValue(cell));</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 获取指定列新插入的值</span></span><br><span class="line">            List&lt;Cell&gt; cells = put.get(columnFamily, qualifier);</span><br><span class="line">            String newValue = <span class="string">""</span>;</span><br><span class="line">            <span class="keyword">for</span> (Cell cell : cells) &#123;</span><br><span class="line">                <span class="keyword">if</span> (CellUtil.matchingColumn(cell, columnFamily, qualifier)) &#123;</span><br><span class="line">                    newValue = Bytes.toString(CellUtil.cloneValue(cell));</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// Append 操作</span></span><br><span class="line">            put.addColumn(columnFamily, qualifier, Bytes.toBytes(oldValue + newValue));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="6-3-打包项目"><a href="#6-3-打包项目" class="headerlink" title="6.3 打包项目"></a>6.3 打包项目</h3><p>使用 maven 命令进行打包，打包后的文件名为 <code>hbase-observer-coprocessor-1.0-SNAPSHOT.jar</code></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> mvn clean package</span></span><br></pre></td></tr></table></figure>
<h3 id="6-4-上传JAR包到HDFS"><a href="#6-4-上传JAR包到HDFS" class="headerlink" title="6.4 上传JAR包到HDFS"></a>6.4 上传JAR包到HDFS</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 上传项目到HDFS上的hbase目录</span></span><br><span class="line">hadoop fs -put /usr/app/hbase-observer-coprocessor-1.0-SNAPSHOT.jar /hbase</span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看上传是否成功</span></span><br><span class="line">hadoop fs -ls /hbase</span><br></pre></td></tr></table></figure>
<div align="center"> <img src="../pictures/hbase-cp-hdfs.png"> </div>

<h3 id="6-5-加载协处理器"><a href="#6-5-加载协处理器" class="headerlink" title="6.5 加载协处理器"></a>6.5 加载协处理器</h3><ol>
<li>加载协处理器前需要先禁用表</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hbase &gt;  disable 'magazine'</span><br></pre></td></tr></table></figure>
<ol start="2">
<li>加载协处理器</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hbase &gt;   alter 'magazine', METHOD =&gt; 'table_att', 'Coprocessor'=&gt;'hdfs://hadoop001:8020/hbase/hbase-observer-coprocessor-1.0-SNAPSHOT.jar|com.heibaiying.AppendRegionObserver|1001|'</span><br></pre></td></tr></table></figure>
<ol start="3">
<li>启用表</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hbase &gt;  enable 'magazine'</span><br></pre></td></tr></table></figure>
<ol start="4">
<li>查看协处理器是否加载成功</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hbase &gt;  desc 'magazine'</span><br></pre></td></tr></table></figure>
<p>协处理器出现在 <code>TABLE_ATTRIBUTES</code> 属性中则代表加载成功，如下图：</p>
<div align="center"> <img src="../pictures/hbase-cp-load.png"> </div>

<h3 id="6-6-测试加载结果"><a href="#6-6-测试加载结果" class="headerlink" title="6.6 测试加载结果"></a>6.6 测试加载结果</h3><p>插入一组测试数据：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hbase &gt; put 'magazine', 'rowkey1','article:content','Hello'</span><br><span class="line">hbase &gt; get 'magazine','rowkey1','article:content'</span><br><span class="line">hbase &gt; put 'magazine', 'rowkey1','article:content','World'</span><br><span class="line">hbase &gt; get 'magazine','rowkey1','article:content'</span><br></pre></td></tr></table></figure>
<p>可以看到对于指定列的值已经执行了 append 操作：</p>
<div align="center"> <img src="../pictures/hbase-cp-helloworld.png"> </div>

<p>插入一组对照数据：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hbase &gt; put 'magazine', 'rowkey1','article:author','zhangsan'</span><br><span class="line">hbase &gt; get 'magazine','rowkey1','article:author'</span><br><span class="line">hbase &gt; put 'magazine', 'rowkey1','article:author','lisi'</span><br><span class="line">hbase &gt; get 'magazine','rowkey1','article:author'</span><br></pre></td></tr></table></figure>
<p>可以看到对于正常的列还是执行 update 操作:</p>
<div align="center"> <img src="../pictures/hbase-cp-lisi.png"> </div>

<h3 id="6-7-卸载协处理器"><a href="#6-7-卸载协处理器" class="headerlink" title="6.7 卸载协处理器"></a>6.7 卸载协处理器</h3><ol>
<li>卸载协处理器前需要先禁用表</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hbase &gt;  disable 'magazine'</span><br></pre></td></tr></table></figure>
<ol start="2">
<li>卸载协处理器</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hbase &gt; alter 'magazine', METHOD =&gt; 'table_att_unset', NAME =&gt; 'coprocessor$1'</span><br></pre></td></tr></table></figure>
<ol start="3">
<li>启用表</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hbase &gt;  enable 'magazine'</span><br></pre></td></tr></table></figure>
<ol start="4">
<li>查看协处理器是否卸载成功</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hbase &gt;  desc 'magazine'</span><br></pre></td></tr></table></figure>
<div align="center"> <img src="../pictures/hbase-co-unload.png"> </div>

<h3 id="6-8-测试卸载结果"><a href="#6-8-测试卸载结果" class="headerlink" title="6.8 测试卸载结果"></a>6.8 测试卸载结果</h3><p>依次执行下面命令可以测试卸载是否成功</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hbase &gt; get 'magazine','rowkey1','article:content'</span><br><span class="line">hbase &gt; put 'magazine', 'rowkey1','article:content','Hello'</span><br><span class="line">hbase &gt; get 'magazine','rowkey1','article:content'</span><br></pre></td></tr></table></figure>
<div align="center"> <img src="../pictures/hbase-unload-test.png"> </div>



<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol>
<li><a href="http://hbase.apache.org/book.html#cp" target="_blank" rel="noopener">Apache HBase Coprocessors</a></li>
<li><a href="https://blogs.apache.org/hbase/entry/coprocessor_introduction" target="_blank" rel="noopener">Apache HBase Coprocessor Introduction</a></li>
<li><a href="https://www.itread01.com/content/1546245908.html" target="_blank" rel="noopener">HBase 高階知識</a></li>
</ol>
]]></content>
      <categories>
        <category>notes</category>
        <category>HBase</category>
      </categories>
      <tags>
        <tag>HBase</tag>
      </tags>
  </entry>
  <entry>
    <title>Hbase容灾与备份</title>
    <url>/2021/03/17/Hbase%E5%AE%B9%E7%81%BE%E4%B8%8E%E5%A4%87%E4%BB%BD/</url>
    <content><![CDATA[<h2 id="一、前言"><a href="#一、前言" class="headerlink" title="一、前言"></a>一、前言</h2><p>本文主要介绍 Hbase 常用的三种简单的容灾备份方案，即<strong>CopyTable</strong>、<strong>Export</strong>/<strong>Import</strong>、<strong>Snapshot</strong>。分别介绍如下：</p>
<h2 id="二、CopyTable"><a href="#二、CopyTable" class="headerlink" title="二、CopyTable"></a>二、CopyTable</h2><h3 id="2-1-简介"><a href="#2-1-简介" class="headerlink" title="2.1 简介"></a>2.1 简介</h3><p><strong>CopyTable</strong>可以将现有表的数据复制到新表中，具有以下特点：</p>
<ul>
<li>支持时间区间 、row 区间 、改变表名称 、改变列族名称 、以及是否 Copy 已被删除的数据等功能；</li>
<li>执行命令前，需先创建与原表结构相同的新表；</li>
<li><code>CopyTable</code> 的操作是基于 HBase Client API 进行的，即采用 <code>scan</code> 进行查询, 采用 <code>put</code> 进行写入。</li>
</ul>
<h3 id="2-2-命令格式"><a href="#2-2-命令格式" class="headerlink" title="2.2 命令格式"></a>2.2 命令格式</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">Usage: CopyTable [general options] [--starttime=X] [--endtime=Y] [--new.name=NEW] [--peer.adr=ADR] &lt;tablename&gt;</span><br></pre></td></tr></table></figure>
<h3 id="2-3-常用命令"><a href="#2-3-常用命令" class="headerlink" title="2.3 常用命令"></a>2.3 常用命令</h3><ol>
<li>同集群下 CopyTable</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hbase org.apache.hadoop.hbase.mapreduce.CopyTable --new.name=tableCopy  tableOrig</span><br></pre></td></tr></table></figure>
<ol start="2">
<li>不同集群下 CopyTable</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 两表名称相同的情况</span></span><br><span class="line">hbase org.apache.hadoop.hbase.mapreduce.CopyTable \</span><br><span class="line">--peer.adr=dstClusterZK:2181:/hbase tableOrig</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 也可以指新的表名</span></span><br><span class="line">hbase org.apache.hadoop.hbase.mapreduce.CopyTable \</span><br><span class="line">--peer.adr=dstClusterZK:2181:/hbase \</span><br><span class="line">--new.name=tableCopy tableOrig</span><br></pre></td></tr></table></figure>
<ol start="3">
<li>下面是一个官方给的比较完整的例子，指定开始和结束时间，集群地址，以及只复制指定的列族：</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hbase org.apache.hadoop.hbase.mapreduce.CopyTable \</span><br><span class="line">--starttime=1265875194289 \</span><br><span class="line">--endtime=1265878794289 \</span><br><span class="line">--peer.adr=server1,server2,server3:2181:/hbase \</span><br><span class="line">--families=myOldCf:myNewCf,cf2,cf3 TestTable</span><br></pre></td></tr></table></figure>
<h3 id="2-4-更多参数"><a href="#2-4-更多参数" class="headerlink" title="2.4 更多参数"></a>2.4 更多参数</h3><p>可以通过 <code>--help</code> 查看更多支持的参数</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> hbase org.apache.hadoop.hbase.mapreduce.CopyTable --<span class="built_in">help</span></span></span><br></pre></td></tr></table></figure>
<div align="center"> <img src="../pictures/hbase-copy-table.png"> </div>



<h2 id="三、Export-Import"><a href="#三、Export-Import" class="headerlink" title="三、Export/Import"></a>三、Export/Import</h2><h3 id="3-1-简介"><a href="#3-1-简介" class="headerlink" title="3.1 简介"></a>3.1 简介</h3><ul>
<li><code>Export</code> 支持导出数据到 HDFS, <code>Import</code> 支持从 HDFS 导入数据。<code>Export</code> 还支持指定导出数据的开始时间和结束时间，因此可以用于增量备份。</li>
<li><code>Export</code> 导出与 <code>CopyTable</code> 一样，依赖 HBase 的 <code>scan</code> 操作</li>
</ul>
<h3 id="3-2-命令格式"><a href="#3-2-命令格式" class="headerlink" title="3.2 命令格式"></a>3.2 命令格式</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Export</span></span><br><span class="line">hbase org.apache.hadoop.hbase.mapreduce.Export &lt;tablename&gt; &lt;outputdir&gt; [&lt;versions&gt; [&lt;starttime&gt; [&lt;endtime&gt;]]]</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Inport</span></span><br><span class="line">hbase org.apache.hadoop.hbase.mapreduce.Import &lt;tablename&gt; &lt;inputdir&gt;</span><br></pre></td></tr></table></figure>
<ul>
<li>导出的 <code>outputdir</code> 目录可以不用预先创建，程序会自动创建。导出完成后，导出文件的所有权将由执行导出命令的用户所拥有。</li>
<li>默认情况下，仅导出给定 <code>Cell</code> 的最新版本，而不管历史版本。要导出多个版本，需要将 <code>&lt;versions&gt;</code> 参数替换为所需的版本数。</li>
</ul>
<h3 id="3-3-常用命令"><a href="#3-3-常用命令" class="headerlink" title="3.3 常用命令"></a>3.3 常用命令</h3><ol>
<li>导出命令</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hbase org.apache.hadoop.hbase.mapreduce.Export tableName  hdfs 路径/tableName.db</span><br></pre></td></tr></table></figure>
<ol start="2">
<li>导入命令</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hbase org.apache.hadoop.hbase.mapreduce.Import tableName  hdfs 路径/tableName.db</span><br></pre></td></tr></table></figure>
<h2 id="四、Snapshot"><a href="#四、Snapshot" class="headerlink" title="四、Snapshot"></a>四、Snapshot</h2><h3 id="4-1-简介"><a href="#4-1-简介" class="headerlink" title="4.1 简介"></a>4.1 简介</h3><p>HBase 的快照 (Snapshot) 功能允许您获取表的副本 (包括内容和元数据)，并且性能开销很小。因为快照存储的仅仅是表的元数据和 HFiles 的信息。快照的 <code>clone</code> 操作会从该快照创建新表，快照的 <code>restore</code> 操作会将表的内容还原到快照节点。<code>clone</code> 和 <code>restore</code> 操作不需要复制任何数据，因为底层 HFiles(包含 HBase 表数据的文件) 不会被修改，修改的只是表的元数据信息。</p>
<h3 id="4-2-配置"><a href="#4-2-配置" class="headerlink" title="4.2 配置"></a>4.2 配置</h3><p>HBase 快照功能默认没有开启，如果要开启快照，需要在 <code>hbase-site.xml</code> 文件中添加如下配置项：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.snapshot.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h3 id="4-3-常用命令"><a href="#4-3-常用命令" class="headerlink" title="4.3 常用命令"></a>4.3 常用命令</h3><p>快照的所有命令都需要在 Hbase Shell 交互式命令行中执行。</p>
<h4 id="1-Take-a-Snapshot"><a href="#1-Take-a-Snapshot" class="headerlink" title="1. Take a Snapshot"></a>1. Take a Snapshot</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 拍摄快照</span></span><br><span class="line"><span class="meta">hbase&gt;</span><span class="bash"> snapshot <span class="string">'表名'</span>, <span class="string">'快照名'</span></span></span><br></pre></td></tr></table></figure>
<p>默认情况下拍摄快照之前会在内存中执行数据刷新。以保证内存中的数据包含在快照中。但是如果你不希望包含内存中的数据，则可以使用 <code>SKIP_FLUSH</code> 选项禁止刷新。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 禁止内存刷新</span></span><br><span class="line"><span class="meta">hbase&gt;</span><span class="bash"> snapshot  <span class="string">'表名'</span>, <span class="string">'快照名'</span>, &#123;SKIP_FLUSH =&gt; <span class="literal">true</span>&#125;</span></span><br></pre></td></tr></table></figure>
<h4 id="2-Listing-Snapshots"><a href="#2-Listing-Snapshots" class="headerlink" title="2. Listing Snapshots"></a>2. Listing Snapshots</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 获取快照列表</span></span><br><span class="line"><span class="meta">hbase&gt;</span><span class="bash"> list_snapshots</span></span><br></pre></td></tr></table></figure>
<h4 id="3-Deleting-Snapshots"><a href="#3-Deleting-Snapshots" class="headerlink" title="3. Deleting Snapshots"></a>3. Deleting Snapshots</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 删除快照</span></span><br><span class="line"><span class="meta">hbase&gt;</span><span class="bash"> delete_snapshot <span class="string">'快照名'</span></span></span><br></pre></td></tr></table></figure>
<h4 id="4-Clone-a-table-from-snapshot"><a href="#4-Clone-a-table-from-snapshot" class="headerlink" title="4. Clone a table from snapshot"></a>4. Clone a table from snapshot</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 从现有的快照创建一张新表</span></span><br><span class="line"><span class="meta">hbase&gt;</span><span class="bash">  clone_snapshot <span class="string">'快照名'</span>, <span class="string">'新表名'</span></span></span><br></pre></td></tr></table></figure>
<h4 id="5-Restore-a-snapshot"><a href="#5-Restore-a-snapshot" class="headerlink" title="5. Restore a snapshot"></a>5. Restore a snapshot</h4><p>将表恢复到快照节点，恢复操作需要先禁用表</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">hbase&gt;</span><span class="bash"> <span class="built_in">disable</span> <span class="string">'表名'</span></span></span><br><span class="line"><span class="meta">hbase&gt;</span><span class="bash"> restore_snapshot <span class="string">'快照名'</span></span></span><br></pre></td></tr></table></figure>
<p>这里需要注意的是：是如果 HBase 配置了基于 Replication 的主从复制，由于 Replication 在日志级别工作，而快照在文件系统级别工作，因此在还原之后，会出现副本与主服务器处于不同的状态的情况。这时候可以先停止同步，所有服务器还原到一致的数据点后再重新建立同步。</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol>
<li><a href="https://blog.cloudera.com/blog/2012/06/online-hbase-backups-with-copytable-2/" target="_blank" rel="noopener">Online Apache HBase Backups with CopyTable</a></li>
<li><a href="http://hbase.apache.org/book.htm" target="_blank" rel="noopener">Apache HBase ™ Reference Guide</a></li>
</ol>
]]></content>
      <categories>
        <category>notes</category>
        <category>HBase</category>
      </categories>
      <tags>
        <tag>HBase</tag>
      </tags>
  </entry>
  <entry>
    <title>Hbase的SQL中间层——Phoenix</title>
    <url>/2021/03/17/Hbase%E7%9A%84SQL%E4%B8%AD%E9%97%B4%E5%B1%82_Phoenix/</url>
    <content><![CDATA[<h2 id="一、Phoenix简介"><a href="#一、Phoenix简介" class="headerlink" title="一、Phoenix简介"></a>一、Phoenix简介</h2><p><code>Phoenix</code> 是 HBase 的开源 SQL 中间层，它允许你使用标准 JDBC 的方式来操作 HBase 上的数据。在 <code>Phoenix</code> 之前，如果你要访问 HBase，只能调用它的 Java API，但相比于使用一行 SQL 就能实现数据查询，HBase 的 API 还是过于复杂。<code>Phoenix</code> 的理念是 <code>we put sql SQL back in NOSQL</code>，即你可以使用标准的 SQL 就能完成对 HBase 上数据的操作。同时这也意味着你可以通过集成 <code>Spring Data  JPA</code> 或 <code>Mybatis</code> 等常用的持久层框架来操作 HBase。</p>
<p>其次 <code>Phoenix</code> 的性能表现也非常优异，<code>Phoenix</code> 查询引擎会将 SQL 查询转换为一个或多个 HBase Scan，通过并行执行来生成标准的 JDBC 结果集。它通过直接使用 HBase API 以及协处理器和自定义过滤器，可以为小型数据查询提供毫秒级的性能，为千万行数据的查询提供秒级的性能。同时 Phoenix 还拥有二级索引等 HBase 不具备的特性，因为以上的优点，所以 <code>Phoenix</code> 成为了 HBase 最优秀的 SQL 中间层。</p>
<div align="center"> <img width="600px" src="../pictures/Phoenix-hadoop.png"> </div>


<h2 id="二、Phoenix安装"><a href="#二、Phoenix安装" class="headerlink" title="二、Phoenix安装"></a>二、Phoenix安装</h2><blockquote>
<p>我们可以按照官方安装说明进行安装，官方说明如下：</p>
<ul>
<li>download and expand our installation tar</li>
<li>copy the phoenix server jar that is compatible with your HBase installation into the lib directory of every region server</li>
<li>restart the region servers</li>
<li>add the phoenix client jar to the classpath of your HBase client</li>
<li>download and setup SQuirrel as your SQL client so you can issue adhoc SQL against your HBase cluster</li>
</ul>
</blockquote>
<h3 id="2-1-下载并解压"><a href="#2-1-下载并解压" class="headerlink" title="2.1 下载并解压"></a>2.1 下载并解压</h3><p>官方针对 Apache 版本和 CDH 版本的 HBase 均提供了安装包，按需下载即可。官方下载地址: <a href="http://phoenix.apache.org/download.html" target="_blank" rel="noopener">http://phoenix.apache.org/download.html</a></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 下载</span></span><br><span class="line">wget http://mirror.bit.edu.cn/apache/phoenix/apache-phoenix-4.14.0-cdh5.14.2/bin/apache-phoenix-4.14.0-cdh5.14.2-bin.tar.gz</span><br><span class="line"><span class="meta">#</span><span class="bash"> 解压</span></span><br><span class="line">tar tar apache-phoenix-4.14.0-cdh5.14.2-bin.tar.gz</span><br></pre></td></tr></table></figure>
<h3 id="2-2-拷贝Jar包"><a href="#2-2-拷贝Jar包" class="headerlink" title="2.2 拷贝Jar包"></a>2.2 拷贝Jar包</h3><p>按照官方文档的说明，需要将 <code>phoenix server jar</code> 添加到所有 <code>Region Servers</code> 的安装目录的 <code>lib</code> 目录下。</p>
<p>这里由于我搭建的是 HBase 伪集群，所以只需要拷贝到当前机器的 HBase 的 lib 目录下。如果是真实集群，则使用 scp 命令分发到所有 <code>Region Servers</code> 机器上。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cp /usr/app/apache-phoenix-4.14.0-cdh5.14.2-bin/phoenix-4.14.0-cdh5.14.2-server.jar /usr/app/hbase-1.2.0-cdh5.15.2/lib</span><br></pre></td></tr></table></figure>
<h3 id="2-3-重启-Region-Servers"><a href="#2-3-重启-Region-Servers" class="headerlink" title="2.3 重启 Region Servers"></a>2.3 重启 Region Servers</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 停止Hbase</span></span><br><span class="line">stop-hbase.sh</span><br><span class="line"><span class="meta">#</span><span class="bash"> 启动Hbase</span></span><br><span class="line">start-hbase.sh</span><br></pre></td></tr></table></figure>
<h3 id="2-4-启动Phoenix"><a href="#2-4-启动Phoenix" class="headerlink" title="2.4 启动Phoenix"></a>2.4 启动Phoenix</h3><p>在 Phoenix 解压目录下的 <code>bin</code> 目录下执行如下命令，需要指定 Zookeeper 的地址：</p>
<ul>
<li>如果 HBase 采用 Standalone 模式或者伪集群模式搭建，则默认采用内置的 Zookeeper 服务，端口为 2181；</li>
<li>如果是 HBase 是集群模式并采用外置的 Zookeeper 集群，则按照自己的实际情况进行指定。</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> ./sqlline.py hadoop001:2181</span></span><br></pre></td></tr></table></figure>
<h3 id="2-5-启动结果"><a href="#2-5-启动结果" class="headerlink" title="2.5 启动结果"></a>2.5 启动结果</h3><p>启动后则进入了 Phoenix 交互式 SQL 命令行，可以使用 <code>!table</code> 或 <code>!tables</code> 查看当前所有表的信息</p>
<div align="center"> <img src="../pictures/phoenix-shell.png"> </div>


<h2 id="三、Phoenix-简单使用"><a href="#三、Phoenix-简单使用" class="headerlink" title="三、Phoenix 简单使用"></a>三、Phoenix 简单使用</h2><h3 id="3-1-创建表"><a href="#3-1-创建表" class="headerlink" title="3.1 创建表"></a>3.1 创建表</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="keyword">IF</span> <span class="keyword">NOT</span> <span class="keyword">EXISTS</span> us_population (</span><br><span class="line">      state <span class="built_in">CHAR</span>(<span class="number">2</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">      city <span class="built_in">VARCHAR</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">      population <span class="built_in">BIGINT</span></span><br><span class="line">      <span class="keyword">CONSTRAINT</span> my_pk PRIMARY <span class="keyword">KEY</span> (state, city));</span><br></pre></td></tr></table></figure>
<div align="center"> <img src="../pictures/Phoenix-create-table.png"> </div><br>新建的表会按照特定的规则转换为 HBase 上的表，关于表的信息，可以通过 Hbase Web UI 进行查看：<br><br><div align="center"> <img src="../pictures/hbase-web-ui-phoenix.png"> </div><br>### 3.2 插入数据<br><br>Phoenix 中插入数据采用的是 <code>UPSERT</code> 而不是 <code>INSERT</code>,因为 Phoenix 并没有更新操作，插入相同主键的数据就视为更新，所以 <code>UPSERT</code> 就相当于 <code>UPDATE</code>+<code>INSERT</code><br><br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">UPSERT INTO us_population VALUES('NY','New York',8143197);</span><br><span class="line">UPSERT INTO us_population VALUES('CA','Los Angeles',3844829);</span><br><span class="line">UPSERT INTO us_population VALUES('IL','Chicago',2842518);</span><br><span class="line">UPSERT INTO us_population VALUES('TX','Houston',2016582);</span><br><span class="line">UPSERT INTO us_population VALUES('PA','Philadelphia',1463281);</span><br><span class="line">UPSERT INTO us_population VALUES('AZ','Phoenix',1461575);</span><br><span class="line">UPSERT INTO us_population VALUES('TX','San Antonio',1256509);</span><br><span class="line">UPSERT INTO us_population VALUES('CA','San Diego',1255540);</span><br><span class="line">UPSERT INTO us_population VALUES('TX','Dallas',1213825);</span><br><span class="line">UPSERT INTO us_population VALUES('CA','San Jose',912332);</span><br></pre></td></tr></table></figure><br><br>### 3.3 修改数据<br><br><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 插入主键相同的数据就视为更新</span></span><br><span class="line">UPSERT INTO us_population <span class="keyword">VALUES</span>(<span class="string">'NY'</span>,<span class="string">'New York'</span>,<span class="number">999999</span>);</span><br></pre></td></tr></table></figure><br><br><div align="center"> <img src="../pictures/Phoenix-update.png"> </div><br>### 3.4 删除数据<br><br><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">DELETE</span> <span class="keyword">FROM</span> us_population <span class="keyword">WHERE</span> city=<span class="string">'Dallas'</span>;</span><br></pre></td></tr></table></figure><br><br><div align="center"> <img src="../pictures/Phoenix-delete.png"> </div><br>### 3.5 查询数据<br><br><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> state <span class="keyword">as</span> <span class="string">"州"</span>,<span class="keyword">count</span>(city) <span class="keyword">as</span> <span class="string">"市"</span>,<span class="keyword">sum</span>(population) <span class="keyword">as</span> <span class="string">"热度"</span></span><br><span class="line"><span class="keyword">FROM</span> us_population</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> state</span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> <span class="keyword">sum</span>(population) <span class="keyword">DESC</span>;</span><br></pre></td></tr></table></figure><br><br><div align="center"> <img src="../pictures/Phoenix-select.png"> </div>


<h3 id="3-6-退出命令"><a href="#3-6-退出命令" class="headerlink" title="3.6 退出命令"></a>3.6 退出命令</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">!quit</span><br></pre></td></tr></table></figure>
<h3 id="3-7-扩展"><a href="#3-7-扩展" class="headerlink" title="3.7 扩展"></a>3.7 扩展</h3><p>从上面的操作中可以看出，Phoenix 支持大多数标准的 SQL 语法。关于 Phoenix 支持的语法、数据类型、函数、序列等详细信息，因为涉及内容很多，可以参考其官方文档，官方文档上有详细的说明：</p>
<ul>
<li><p><strong>语法 (Grammar)</strong> ：<a href="https://phoenix.apache.org/language/index.html" target="_blank" rel="noopener">https://phoenix.apache.org/language/index.html</a></p>
</li>
<li><p><strong>函数 (Functions)</strong> ：<a href="http://phoenix.apache.org/language/functions.html" target="_blank" rel="noopener">http://phoenix.apache.org/language/functions.html</a></p>
</li>
<li><p><strong>数据类型 (Datatypes)</strong> ：<a href="http://phoenix.apache.org/language/datatypes.html" target="_blank" rel="noopener">http://phoenix.apache.org/language/datatypes.html</a></p>
</li>
<li><p><strong>序列 (Sequences)</strong> :<a href="http://phoenix.apache.org/sequences.html" target="_blank" rel="noopener">http://phoenix.apache.org/sequences.html</a></p>
</li>
<li><p><strong>联结查询 (Joins)</strong> ：<a href="http://phoenix.apache.org/joins.html" target="_blank" rel="noopener">http://phoenix.apache.org/joins.html</a></p>
</li>
</ul>
<h2 id="四、Phoenix-Java-API"><a href="#四、Phoenix-Java-API" class="headerlink" title="四、Phoenix Java API"></a>四、Phoenix Java API</h2><p>因为 Phoenix 遵循 JDBC 规范，并提供了对应的数据库驱动 <code>PhoenixDriver</code>，这使得采用 Java 语言对其进行操作的时候，就如同对其他关系型数据库一样，下面给出基本的使用示例。</p>
<h3 id="4-1-引入Phoenix-core-JAR包"><a href="#4-1-引入Phoenix-core-JAR包" class="headerlink" title="4.1 引入Phoenix core JAR包"></a>4.1 引入Phoenix core JAR包</h3><p>如果是 maven 项目，直接在 maven 中央仓库找到对应的版本，导入依赖即可：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- https://mvnrepository.com/artifact/org.apache.phoenix/phoenix-core --&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.phoenix<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>phoenix-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">version</span>&gt;</span>4.14.0-cdh5.14.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>如果是普通项目，则可以从 Phoenix 解压目录下找到对应的 JAR 包，然后手动引入：</p>
<div align="center"> <img src="../pictures/phoenix-core-jar.png"> </div><br>### 4.2 简单的Java API实例<br><br><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.sql.Connection;</span><br><span class="line"><span class="keyword">import</span> java.sql.DriverManager;</span><br><span class="line"><span class="keyword">import</span> java.sql.PreparedStatement;</span><br><span class="line"><span class="keyword">import</span> java.sql.ResultSet;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">PhoenixJavaApi</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 加载数据库驱动</span></span><br><span class="line">        Class.forName(<span class="string">"org.apache.phoenix.jdbc.PhoenixDriver"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * 指定数据库地址,格式为 jdbc:phoenix:Zookeeper 地址</span></span><br><span class="line"><span class="comment">         * 如果 HBase 采用 Standalone 模式或者伪集群模式搭建，则 HBase 默认使用内置的 Zookeeper，默认端口为 2181</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        Connection connection = DriverManager.getConnection(<span class="string">"jdbc:phoenix:192.168.200.226:2181"</span>);</span><br><span class="line"></span><br><span class="line">        PreparedStatement statement = connection.prepareStatement(<span class="string">"SELECT * FROM us_population"</span>);</span><br><span class="line"></span><br><span class="line">        ResultSet resultSet = statement.executeQuery();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> (resultSet.next()) &#123;</span><br><span class="line">            System.out.println(resultSet.getString(<span class="string">"city"</span>) + <span class="string">" "</span></span><br><span class="line">                    + resultSet.getInt(<span class="string">"population"</span>));</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        statement.close();</span><br><span class="line">        connection.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br><br>结果如下：<br><br><div align="center"> <img src="../pictures/Phoenix-java-api-result.png"> </div>


<p>实际的开发中我们通常都是采用第三方框架来操作数据库，如 <code>mybatis</code>，<code>Hibernate</code>，<code>Spring Data</code> 等。关于 Phoenix 与这些框架的整合步骤参见下一篇文章：<a href="https://github.com/heibaiying/BigData-Notes/blob/master/notes/Spring+Mybtais+Phoenix整合.md" target="_blank" rel="noopener">Spring/Spring Boot + Mybatis + Phoenix</a></p>
<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ol>
<li><a href="http://phoenix.apache.org/" target="_blank" rel="noopener">http://phoenix.apache.org/</a></li>
</ol>
]]></content>
      <categories>
        <category>notes</category>
        <category>HBase</category>
      </categories>
      <tags>
        <tag>HBase</tag>
      </tags>
  </entry>
  <entry>
    <title>HBase简介</title>
    <url>/2021/03/17/Hbase%E7%AE%80%E4%BB%8B/</url>
    <content><![CDATA[<h2 id="一、Hadoop的局限"><a href="#一、Hadoop的局限" class="headerlink" title="一、Hadoop的局限"></a>一、Hadoop的局限</h2><p>HBase 是一个构建在 Hadoop 文件系统之上的面向列的数据库管理系统。</p>
<div align="center"> <img src="../pictures/hbase.jpg"> </div>

<p>要想明白为什么产生 HBase，就需要先了解一下 Hadoop 存在的限制？Hadoop 可以通过 HDFS 来存储结构化、半结构甚至非结构化的数据，它是传统数据库的补充，是海量数据存储的最佳方法，它针对大文件的存储，批量访问和流式访问都做了优化，同时也通过多副本解决了容灾问题。</p>
<p>但是 Hadoop 的缺陷在于它只能执行批处理，并且只能以顺序方式访问数据，这意味着即使是最简单的工作，也必须搜索整个数据集，无法实现对数据的随机访问。实现数据的随机访问是传统的关系型数据库所擅长的，但它们却不能用于海量数据的存储。在这种情况下，必须有一种新的方案来解决海量数据存储和随机访问的问题，HBase 就是其中之一 (HBase，Cassandra，couchDB，Dynamo 和 MongoDB 都能存储海量数据并支持随机访问)。</p>
<blockquote>
<p>注：数据结构分类：</p>
<ul>
<li>结构化数据：即以关系型数据库表形式管理的数据；</li>
<li>半结构化数据：非关系模型的，有基本固定结构模式的数据，例如日志文件、XML 文档、JSON 文档、Email 等；</li>
<li>非结构化数据：没有固定模式的数据，如 WORD、PDF、PPT、EXL，各种格式的图片、视频等。</li>
</ul>
</blockquote>
<h2 id="二、HBase简介"><a href="#二、HBase简介" class="headerlink" title="二、HBase简介"></a>二、HBase简介</h2><p>HBase 是一个构建在 Hadoop 文件系统之上的面向列的数据库管理系统。</p>
<p>HBase 是一种类似于 <code>Google’s Big Table</code> 的数据模型，它是 Hadoop 生态系统的一部分，它将数据存储在 HDFS 上，客户端可以通过 HBase 实现对 HDFS 上数据的随机访问。它具有以下特性：</p>
<ul>
<li>不支持复杂的事务，只支持行级事务，即单行数据的读写都是原子性的；</li>
<li>由于是采用 HDFS 作为底层存储，所以和 HDFS 一样，支持结构化、半结构化和非结构化的存储；</li>
<li>支持通过增加机器进行横向扩展；</li>
<li>支持数据分片；</li>
<li>支持 RegionServers 之间的自动故障转移；</li>
<li>易于使用的 Java 客户端 API；</li>
<li>支持 BlockCache 和布隆过滤器；</li>
<li>过滤器支持谓词下推。</li>
</ul>
<h2 id="三、HBase-Table"><a href="#三、HBase-Table" class="headerlink" title="三、HBase Table"></a>三、HBase Table</h2><p>HBase 是一个面向 <code>列</code> 的数据库管理系统，这里更为确切的而说，HBase 是一个面向 <code>列族</code> 的数据库管理系统。表 schema 仅定义列族，表具有多个列族，每个列族可以包含任意数量的列，列由多个单元格（cell ）组成，单元格可以存储多个版本的数据，多个版本数据以时间戳进行区分。</p>
<p>下图为 HBase 中一张表的：</p>
<ul>
<li>RowKey 为行的唯一标识，所有行按照 RowKey 的字典序进行排序；</li>
<li>该表具有两个列族，分别是 personal 和 office;</li>
<li>其中列族 personal 拥有 name、city、phone 三个列，列族 office 拥有 tel、addres 两个列。</li>
</ul>
<div align="center"> <img src="../pictures/HBase_table-iteblog.png"> </div>

<blockquote>
<p><em>图片引用自 : HBase 是列式存储数据库吗</em> <em><a href="https://www.iteblog.com/archives/2498.html" target="_blank" rel="noopener">https://www.iteblog.com/archives/2498.html</a></em></p>
</blockquote>
<p>Hbase 的表具有以下特点：</p>
<ul>
<li><p>容量大：一个表可以有数十亿行，上百万列；</p>
</li>
<li><p>面向列：数据是按照列存储，每一列都单独存放，数据即索引，在查询时可以只访问指定列的数据，有效地降低了系统的 I/O 负担；</p>
</li>
<li><p>稀疏性：空 (null) 列并不占用存储空间，表可以设计的非常稀疏  ；    </p>
</li>
<li><p>数据多版本：每个单元中的数据可以有多个版本，按照时间戳排序，新的数据在最上面；     </p>
</li>
<li><p>存储类型：所有数据的底层存储格式都是字节数组 (byte[])。</p>
</li>
</ul>
<h2 id="四、Phoenix"><a href="#四、Phoenix" class="headerlink" title="四、Phoenix"></a>四、Phoenix</h2><p><code>Phoenix</code> 是 HBase 的开源 SQL 中间层，它允许你使用标准 JDBC 的方式来操作 HBase 上的数据。在 <code>Phoenix</code> 之前，如果你要访问 HBase，只能调用它的 Java API，但相比于使用一行 SQL 就能实现数据查询，HBase 的 API 还是过于复杂。<code>Phoenix</code> 的理念是 <code>we put sql SQL back in NOSQL</code>，即你可以使用标准的 SQL 就能完成对 HBase 上数据的操作。同时这也意味着你可以通过集成 <code>Spring Data  JPA</code> 或 <code>Mybatis</code> 等常用的持久层框架来操作 HBase。</p>
<p>其次 <code>Phoenix</code> 的性能表现也非常优异，<code>Phoenix</code> 查询引擎会将 SQL 查询转换为一个或多个 HBase Scan，通过并行执行来生成标准的 JDBC 结果集。它通过直接使用 HBase API 以及协处理器和自定义过滤器，可以为小型数据查询提供毫秒级的性能，为千万行数据的查询提供秒级的性能。同时 Phoenix 还拥有二级索引等 HBase 不具备的特性，因为以上的优点，所以 <code>Phoenix</code> 成为了 HBase 最优秀的 SQL 中间层。</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol>
<li><a href="https://www.tutorialspoint.com/hbase/hbase_overview.htm" target="_blank" rel="noopener">HBase - Overview</a></li>
</ol>
]]></content>
      <categories>
        <category>notes</category>
        <category>HBase</category>
      </categories>
      <tags>
        <tag>HBase</tag>
      </tags>
  </entry>
  <entry>
    <title>Hbase系统架构及数据结构</title>
    <url>/2021/03/17/Hbase%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E5%8F%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/</url>
    <content><![CDATA[<h2 id="一、基本概念"><a href="#一、基本概念" class="headerlink" title="一、基本概念"></a>一、基本概念</h2><p>一个典型的 Hbase Table 表如下：</p>
<div align="center"> <img src="../pictures/hbase-webtable.png"> </div>

<h3 id="1-1-Row-Key-行键"><a href="#1-1-Row-Key-行键" class="headerlink" title="1.1 Row Key (行键)"></a>1.1 Row Key (行键)</h3><p><code>Row Key</code> 是用来检索记录的主键。想要访问 HBase Table 中的数据，只有以下三种方式：</p>
<ul>
<li><p>通过指定的 <code>Row Key</code> 进行访问；</p>
</li>
<li><p>通过 Row Key 的 range 进行访问，即访问指定范围内的行；</p>
</li>
<li><p>进行全表扫描。</p>
</li>
</ul>
<p><code>Row Key</code> 可以是任意字符串，存储时数据按照 <code>Row Key</code> 的字典序进行排序。这里需要注意以下两点：</p>
<ul>
<li><p>因为字典序对 Int 排序的结果是 1,10,100,11,12,13,14,15,16,17,18,19,2,20,21,…,9,91,92,93,94,95,96,97,98,99。如果你使用整型的字符串作为行键，那么为了保持整型的自然序，行键必须用 0 作左填充。</p>
</li>
<li><p>行的一次读写操作时原子性的 (不论一次读写多少列)。</p>
</li>
</ul>
<h3 id="1-2-Column-Family（列族）"><a href="#1-2-Column-Family（列族）" class="headerlink" title="1.2 Column Family（列族）"></a>1.2 Column Family（列族）</h3><p>HBase 表中的每个列，都归属于某个列族。列族是表的 Schema 的一部分，所以列族需要在创建表时进行定义。列族的所有列都以列族名作为前缀，例如 <code>courses:history</code>，<code>courses:math</code> 都属于 <code>courses</code> 这个列族。</p>
<h3 id="1-3-Column-Qualifier-列限定符"><a href="#1-3-Column-Qualifier-列限定符" class="headerlink" title="1.3 Column Qualifier (列限定符)"></a>1.3 Column Qualifier (列限定符)</h3><p>列限定符，你可以理解为是具体的列名，例如 <code>courses:history</code>，<code>courses:math</code> 都属于 <code>courses</code> 这个列族，它们的列限定符分别是 <code>history</code> 和 <code>math</code>。需要注意的是列限定符不是表 Schema 的一部分，你可以在插入数据的过程中动态创建列。</p>
<h3 id="1-4-Column-列"><a href="#1-4-Column-列" class="headerlink" title="1.4 Column(列)"></a>1.4 Column(列)</h3><p>HBase 中的列由列族和列限定符组成，它们由 <code>:</code>(冒号) 进行分隔，即一个完整的列名应该表述为 <code>列族名 ：列限定符</code>。</p>
<h3 id="1-5-Cell"><a href="#1-5-Cell" class="headerlink" title="1.5 Cell"></a>1.5 Cell</h3><p><code>Cell</code> 是行，列族和列限定符的组合，并包含值和时间戳。你可以等价理解为关系型数据库中由指定行和指定列确定的一个单元格，但不同的是 HBase 中的一个单元格是由多个版本的数据组成的，每个版本的数据用时间戳进行区分。</p>
<h3 id="1-6-Timestamp-时间戳"><a href="#1-6-Timestamp-时间戳" class="headerlink" title="1.6 Timestamp(时间戳)"></a>1.6 Timestamp(时间戳)</h3><p>HBase 中通过 <code>row key</code> 和 <code>column</code> 确定的为一个存储单元称为 <code>Cell</code>。每个 <code>Cell</code> 都保存着同一份数据的多个版本。版本通过时间戳来索引，时间戳的类型是 64 位整型，时间戳可以由 HBase 在数据写入时自动赋值，也可以由客户显式指定。每个 <code>Cell</code> 中，不同版本的数据按照时间戳倒序排列，即最新的数据排在最前面。</p>
<h2 id="二、存储结构"><a href="#二、存储结构" class="headerlink" title="二、存储结构"></a>二、存储结构</h2><h3 id="2-1-Regions"><a href="#2-1-Regions" class="headerlink" title="2.1 Regions"></a>2.1 Regions</h3><p>HBase Table 中的所有行按照 <code>Row Key</code> 的字典序排列。HBase Tables 通过行键的范围 (row key range) 被水平切分成多个 <code>Region</code>, 一个 <code>Region</code> 包含了在 start key 和 end key 之间的所有行。</p>
<div align="center"> <img src="../pictures/HBaseArchitecture-Blog-Fig2.png"> </div>

<p>每个表一开始只有一个 <code>Region</code>，随着数据不断增加，<code>Region</code> 会不断增大，当增大到一个阀值的时候，<code>Region</code> 就会等分为两个新的 <code>Region</code>。当 Table 中的行不断增多，就会有越来越多的 <code>Region</code>。</p>
<div align="center"> <img width="600px" src="../pictures/hbase-region-splite.png"> </div>

<p><code>Region</code> 是 HBase 中<strong>分布式存储和负载均衡的最小单元</strong>。这意味着不同的 <code>Region</code> 可以分布在不同的 <code>Region Server</code> 上。但一个 <code>Region</code> 是不会拆分到多个 Server 上的。</p>
<div align="center"> <img width="600px" src="../pictures/hbase-region-dis.png"> </div>

<h3 id="2-2-Region-Server"><a href="#2-2-Region-Server" class="headerlink" title="2.2 Region Server"></a>2.2 Region Server</h3><p><code>Region Server</code> 运行在 HDFS 的 DataNode 上。它具有以下组件：</p>
<ul>
<li><strong>WAL(Write Ahead Log，预写日志)</strong>：用于存储尚未进持久化存储的数据记录，以便在发生故障时进行恢复。</li>
<li><strong>BlockCache</strong>：读缓存。它将频繁读取的数据存储在内存中，如果存储不足，它将按照 <code>最近最少使用原则</code> 清除多余的数据。</li>
<li><strong>MemStore</strong>：写缓存。它存储尚未写入磁盘的新数据，并会在数据写入磁盘之前对其进行排序。每个 Region 上的每个列族都有一个 MemStore。</li>
<li><strong>HFile</strong> ：将行数据按照 Key\Values 的形式存储在文件系统上。</li>
</ul>
<div align="center"> <img src="../pictures/hbase-Region-Server.png"> </div>



<p>Region Server 存取一个子表时，会创建一个 Region 对象，然后对表的每个列族创建一个 <code>Store</code> 实例，每个 <code>Store</code> 会有 0 个或多个 <code>StoreFile</code> 与之对应，每个 <code>StoreFile</code> 则对应一个 <code>HFile</code>，HFile 就是实际存储在 HDFS 上的文件。</p>
<div align="center"> <img src="../pictures/hbase-hadoop.png"> </div>



<h2 id="三、Hbase系统架构"><a href="#三、Hbase系统架构" class="headerlink" title="三、Hbase系统架构"></a>三、Hbase系统架构</h2><h3 id="3-1-系统架构"><a href="#3-1-系统架构" class="headerlink" title="3.1 系统架构"></a>3.1 系统架构</h3><p>HBase 系统遵循 Master/Salve 架构，由三种不同类型的组件组成：</p>
<p><strong>Zookeeper</strong></p>
<ol>
<li><p>保证任何时候，集群中只有一个 Master；</p>
</li>
<li><p>存贮所有 Region 的寻址入口；</p>
</li>
<li><p>实时监控 Region Server 的状态，将 Region Server 的上线和下线信息实时通知给 Master；</p>
</li>
<li><p>存储 HBase 的 Schema，包括有哪些 Table，每个 Table 有哪些 Column Family 等信息。</p>
</li>
</ol>
<p><strong>Master</strong></p>
<ol>
<li><p>为 Region Server 分配 Region ；</p>
</li>
<li><p>负责 Region Server 的负载均衡 ；</p>
</li>
<li><p>发现失效的 Region Server 并重新分配其上的 Region； </p>
</li>
<li><p>GFS 上的垃圾文件回收；</p>
</li>
<li><p>处理 Schema 的更新请求。</p>
</li>
</ol>
<p><strong>Region Server</strong></p>
<ol>
<li><p>Region Server 负责维护 Master 分配给它的 Region ，并处理发送到 Region 上的 IO 请求；</p>
</li>
<li><p>Region Server 负责切分在运行过程中变得过大的 Region。</p>
</li>
</ol>
<div align="center"> <img width="600px" src="../pictures/HBaseArchitecture-Blog-Fig1.png"> </div>

<h3 id="3-2-组件间的协作"><a href="#3-2-组件间的协作" class="headerlink" title="3.2 组件间的协作"></a>3.2 组件间的协作</h3><p> HBase 使用 ZooKeeper 作为分布式协调服务来维护集群中的服务器状态。 Zookeeper 负责维护可用服务列表，并提供服务故障通知等服务：</p>
<ul>
<li><p>每个 Region Server 都会在 ZooKeeper 上创建一个临时节点，Master 通过 Zookeeper 的 Watcher 机制对节点进行监控，从而可以发现新加入的 Region Server 或故障退出的 Region Server；</p>
</li>
<li><p>所有 Masters 会竞争性地在 Zookeeper 上创建同一个临时节点，由于 Zookeeper 只能有一个同名节点，所以必然只有一个 Master 能够创建成功，此时该 Master 就是主 Master，主 Master 会定期向 Zookeeper 发送心跳。备用 Masters 则通过 Watcher 机制对主 HMaster 所在节点进行监听；</p>
</li>
<li><p>如果主 Master 未能定时发送心跳，则其持有的 Zookeeper 会话会过期，相应的临时节点也会被删除，这会触发定义在该节点上的 Watcher 事件，使得备用的 Master Servers 得到通知。所有备用的 Master Servers 在接到通知后，会再次去竞争性地创建临时节点，完成主 Master 的选举。</p>
</li>
</ul>
<div align="center"> <img src="../pictures/HBaseArchitecture-Blog-Fig5.png"> </div>



<h2 id="四、数据的读写流程简述"><a href="#四、数据的读写流程简述" class="headerlink" title="四、数据的读写流程简述"></a>四、数据的读写流程简述</h2><h3 id="4-1-写入数据的流程"><a href="#4-1-写入数据的流程" class="headerlink" title="4.1 写入数据的流程"></a>4.1 写入数据的流程</h3><ol>
<li><p>Client 向 Region Server 提交写请求；</p>
</li>
<li><p>Region Server 找到目标 Region；</p>
</li>
<li><p>Region 检查数据是否与 Schema 一致；</p>
</li>
<li><p>如果客户端没有指定版本，则获取当前系统时间作为数据版本；</p>
</li>
<li><p>将更新写入 WAL Log；</p>
</li>
<li><p>将更新写入 Memstore；</p>
</li>
<li><p>判断 Memstore 存储是否已满，如果存储已满则需要 flush 为 Store Hfile 文件。</p>
</li>
</ol>
<blockquote>
<p>更为详细写入流程可以参考：<a href="http://hbasefly.com/2016/03/23/hbase_writer/" target="_blank" rel="noopener">HBase － 数据写入流程解析</a></p>
</blockquote>
<h3 id="4-2-读取数据的流程"><a href="#4-2-读取数据的流程" class="headerlink" title="4.2 读取数据的流程"></a>4.2 读取数据的流程</h3><p>以下是客户端首次读写 HBase 上数据的流程：</p>
<ol>
<li><p>客户端从 Zookeeper 获取 <code>META</code> 表所在的 Region Server；</p>
</li>
<li><p>客户端访问 <code>META</code> 表所在的 Region Server，从 <code>META</code> 表中查询到访问行键所在的 Region Server，之后客户端将缓存这些信息以及 <code>META</code> 表的位置；</p>
</li>
<li><p>客户端从行键所在的 Region Server 上获取数据。</p>
</li>
</ol>
<p>如果再次读取，客户端将从缓存中获取行键所在的 Region Server。这样客户端就不需要再次查询 <code>META</code> 表，除非 Region 移动导致缓存失效，这样的话，则将会重新查询并更新缓存。</p>
<p>注：<code>META</code> 表是 HBase 中一张特殊的表，它保存了所有 Region 的位置信息，META 表自己的位置信息则存储在 ZooKeeper 上。</p>
<div align="center"> <img src="../pictures/HBaseArchitecture-Blog-Fig7.png"> </div>

<blockquote>
<p>更为详细读取数据流程参考：</p>
<p><a href="http://hbasefly.com/2016/12/21/hbase-getorscan/" target="_blank" rel="noopener">HBase 原理－数据读取流程解析</a></p>
<p><a href="http://hbasefly.com/2017/06/11/hbase-scan-2/" target="_blank" rel="noopener">HBase 原理－迟到的‘数据读取流程部分细节</a></p>
</blockquote>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>本篇文章内容主要参考自官方文档和以下两篇博客，图片也主要引用自以下两篇博客：</p>
<ul>
<li><p><a href="https://mapr.com/blog/in-depth-look-hbase-architecture/#.VdMxvWSqqko" target="_blank" rel="noopener">HBase Architectural Components</a></p>
</li>
<li><p><a href="https://www.open-open.com/lib/view/open1346821084631.html" target="_blank" rel="noopener">Hbase 系统架构及数据结构</a></p>
</li>
</ul>
<p>官方文档：</p>
<ul>
<li><a href="https://hbase.apache.org/2.1/book.html" target="_blank" rel="noopener">Apache HBase ™ Reference Guide</a></li>
</ul>
]]></content>
      <categories>
        <category>notes</category>
        <category>HBase</category>
      </categories>
      <tags>
        <tag>HBase</tag>
      </tags>
  </entry>
  <entry>
    <title>Hbase 过滤器详解</title>
    <url>/2021/03/17/Hbase%E8%BF%87%E6%BB%A4%E5%99%A8%E8%AF%A6%E8%A7%A3/</url>
    <content><![CDATA[<h2 id="一、HBase过滤器简介"><a href="#一、HBase过滤器简介" class="headerlink" title="一、HBase过滤器简介"></a>一、HBase过滤器简介</h2><p>Hbase 提供了种类丰富的过滤器（filter）来提高数据处理的效率，用户可以通过内置或自定义的过滤器来对数据进行过滤，所有的过滤器都在服务端生效，即谓词下推（predicate push down）。这样可以保证过滤掉的数据不会被传送到客户端，从而减轻网络传输和客户端处理的压力。</p>
<div align="center"> <img src="../pictures/hbase-fliter.png"> </div>



<h2 id="二、过滤器基础"><a href="#二、过滤器基础" class="headerlink" title="二、过滤器基础"></a>二、过滤器基础</h2><h3 id="2-1-Filter接口和FilterBase抽象类"><a href="#2-1-Filter接口和FilterBase抽象类" class="headerlink" title="2.1  Filter接口和FilterBase抽象类"></a>2.1  Filter接口和FilterBase抽象类</h3><p>Filter 接口中定义了过滤器的基本方法，FilterBase 抽象类实现了 Filter 接口。所有内置的过滤器则直接或者间接继承自 FilterBase 抽象类。用户只需要将定义好的过滤器通过 <code>setFilter</code> 方法传递给 <code>Scan</code> 或 <code>put</code> 的实例即可。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">setFilter(Filter filter)</span><br></pre></td></tr></table></figure>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Scan 中定义的 setFilter</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"> <span class="function"><span class="keyword">public</span> Scan <span class="title">setFilter</span><span class="params">(Filter filter)</span> </span>&#123;</span><br><span class="line">   <span class="keyword">super</span>.setFilter(filter);</span><br><span class="line">   <span class="keyword">return</span> <span class="keyword">this</span>;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"> <span class="comment">// Get 中定义的 setFilter</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"> <span class="function"><span class="keyword">public</span> Get <span class="title">setFilter</span><span class="params">(Filter filter)</span> </span>&#123;</span><br><span class="line">   <span class="keyword">super</span>.setFilter(filter);</span><br><span class="line">   <span class="keyword">return</span> <span class="keyword">this</span>;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>
<p>FilterBase 的所有子类过滤器如下：<div align="center"> <img src="../pictures/hbase-filterbase-subclass.png"> </div></p>
<blockquote>
<p>说明：上图基于当前时间点（2019.4）最新的 Hbase-2.1.4 ，下文所有说明均基于此版本。</p>
</blockquote>
<h3 id="2-2-过滤器分类"><a href="#2-2-过滤器分类" class="headerlink" title="2.2 过滤器分类"></a>2.2 过滤器分类</h3><p>HBase 内置过滤器可以分为三类：分别是比较过滤器，专用过滤器和包装过滤器。分别在下面的三个小节中做详细的介绍。</p>
<h2 id="三、比较过滤器"><a href="#三、比较过滤器" class="headerlink" title="三、比较过滤器"></a>三、比较过滤器</h2><p>所有比较过滤器均继承自 <code>CompareFilter</code>。创建一个比较过滤器需要两个参数，分别是<strong>比较运算符</strong>和<strong>比较器实例</strong>。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">CompareFilter</span><span class="params">(<span class="keyword">final</span> CompareOp compareOp,<span class="keyword">final</span> ByteArrayComparable comparator)</span> </span>&#123;</span><br><span class="line">   <span class="keyword">this</span>.compareOp = compareOp;</span><br><span class="line">   <span class="keyword">this</span>.comparator = comparator;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>
<h3 id="3-1-比较运算符"><a href="#3-1-比较运算符" class="headerlink" title="3.1 比较运算符"></a>3.1 比较运算符</h3><ul>
<li>LESS (&lt;)</li>
<li>LESS_OR_EQUAL (&lt;=)</li>
<li>EQUAL (=)</li>
<li>NOT_EQUAL (!=)</li>
<li>GREATER_OR_EQUAL (&gt;=)</li>
<li>GREATER (&gt;)</li>
<li>NO_OP (排除所有符合条件的值)</li>
</ul>
<p>比较运算符均定义在枚举类 <code>CompareOperator</code> 中</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@InterfaceAudience</span>.Public</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">enum</span> CompareOperator &#123;</span><br><span class="line">  LESS,</span><br><span class="line">  LESS_OR_EQUAL,</span><br><span class="line">  EQUAL,</span><br><span class="line">  NOT_EQUAL,</span><br><span class="line">  GREATER_OR_EQUAL,</span><br><span class="line">  GREATER,</span><br><span class="line">  NO_OP,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意：在 1.x 版本的 HBase 中，比较运算符定义在 <code>CompareFilter.CompareOp</code> 枚举类中，但在 2.0 之后这个类就被标识为 @deprecated ，并会在 3.0 移除。所以 2.0 之后版本的 HBase 需要使用 <code>CompareOperator</code> 这个枚举类。</p>
</blockquote>
<h3 id="3-2-比较器"><a href="#3-2-比较器" class="headerlink" title="3.2 比较器"></a>3.2 比较器</h3><p>所有比较器均继承自 <code>ByteArrayComparable</code> 抽象类，常用的有以下几种：</p>
<div align="center"> <img src="../pictures/hbase-bytearraycomparable.png"> </div>

<ul>
<li><strong>BinaryComparator</strong>  : 使用 <code>Bytes.compareTo(byte []，byte [])</code> 按字典序比较指定的字节数组。</li>
<li><strong>BinaryPrefixComparator</strong> : 按字典序与指定的字节数组进行比较，但只比较到这个字节数组的长度。</li>
<li><strong>RegexStringComparator</strong> :  使用给定的正则表达式与指定的字节数组进行比较。仅支持 <code>EQUAL</code> 和 <code>NOT_EQUAL</code> 操作。</li>
<li><strong>SubStringComparator</strong> : 测试给定的子字符串是否出现在指定的字节数组中，比较不区分大小写。仅支持 <code>EQUAL</code> 和 <code>NOT_EQUAL</code> 操作。</li>
<li><strong>NullComparator</strong> ：判断给定的值是否为空。</li>
<li><strong>BitComparator</strong> ：按位进行比较。</li>
</ul>
<p><code>BinaryPrefixComparator</code> 和 <code>BinaryComparator</code> 的区别不是很好理解，这里举例说明一下：</p>
<p>在进行 <code>EQUAL</code> 的比较时，如果比较器传入的是 <code>abcd</code> 的字节数组，但是待比较数据是 <code>abcdefgh</code>：</p>
<ul>
<li>如果使用的是 <code>BinaryPrefixComparator</code> 比较器，则比较以 <code>abcd</code> 字节数组的长度为准，即 <code>efgh</code> 不会参与比较，这时候认为 <code>abcd</code> 与 <code>abcdefgh</code> 是满足 <code>EQUAL</code> 条件的；</li>
<li>如果使用的是 <code>BinaryComparator</code> 比较器，则认为其是不相等的。</li>
</ul>
<h3 id="3-3-比较过滤器种类"><a href="#3-3-比较过滤器种类" class="headerlink" title="3.3 比较过滤器种类"></a>3.3 比较过滤器种类</h3><p>比较过滤器共有五个（Hbase 1.x 版本和 2.x 版本相同），见下图：</p>
<div align="center"> <img src="../pictures/hbase-compareFilter.png"> </div>

<ul>
<li><strong>RowFilter</strong> ：基于行键来过滤数据；</li>
<li><strong>FamilyFilterr</strong> ：基于列族来过滤数据；</li>
<li><strong>QualifierFilterr</strong> ：基于列限定符（列名）来过滤数据；</li>
<li><strong>ValueFilterr</strong> ：基于单元格 (cell) 的值来过滤数据；</li>
<li><strong>DependentColumnFilter</strong> ：指定一个参考列来过滤其他列的过滤器，过滤的原则是基于参考列的时间戳来进行筛选 。</li>
</ul>
<p>前四种过滤器的使用方法相同，均只要传递比较运算符和运算器实例即可构建，然后通过 <code>setFilter</code> 方法传递给 <code>scan</code>：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Filter filter  = <span class="keyword">new</span> RowFilter(CompareOperator.LESS_OR_EQUAL,</span><br><span class="line">                               <span class="keyword">new</span> BinaryComparator(Bytes.toBytes(<span class="string">"xxx"</span>)));</span><br><span class="line"> scan.setFilter(filter);</span><br></pre></td></tr></table></figure>
<p><code>DependentColumnFilter</code> 的使用稍微复杂一点，这里单独做下说明。</p>
<h3 id="3-4-DependentColumnFilter"><a href="#3-4-DependentColumnFilter" class="headerlink" title="3.4 DependentColumnFilter"></a>3.4 DependentColumnFilter</h3><p>可以把 <code>DependentColumnFilter</code> 理解为<strong>一个 valueFilter 和一个时间戳过滤器的组合</strong>。<code>DependentColumnFilter</code> 有三个带参构造器，这里选择一个参数最全的进行说明：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DependentColumnFilter(<span class="keyword">final</span> <span class="keyword">byte</span> [] family, <span class="keyword">final</span> <span class="keyword">byte</span>[] qualifier,</span><br><span class="line">                               <span class="keyword">final</span> <span class="keyword">boolean</span> dropDependentColumn, <span class="keyword">final</span> CompareOperator op,</span><br><span class="line">                               <span class="keyword">final</span> ByteArrayComparable valueComparator)</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>family</strong>  ：列族</li>
<li><strong>qualifier</strong> ：列限定符（列名）</li>
<li><strong>dropDependentColumn</strong> ：决定参考列是否被包含在返回结果内，为 true 时表示参考列被返回，为 false 时表示被丢弃</li>
<li><strong>op</strong> ：比较运算符</li>
<li><strong>valueComparator</strong> ：比较器</li>
</ul>
<p>这里举例进行说明：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DependentColumnFilter dependentColumnFilter = <span class="keyword">new</span> DependentColumnFilter( </span><br><span class="line">    Bytes.toBytes(<span class="string">"student"</span>),</span><br><span class="line">    Bytes.toBytes(<span class="string">"name"</span>),</span><br><span class="line">    <span class="keyword">false</span>,</span><br><span class="line">    CompareOperator.EQUAL, </span><br><span class="line">    <span class="keyword">new</span> BinaryPrefixComparator(Bytes.toBytes(<span class="string">"xiaolan"</span>)));</span><br></pre></td></tr></table></figure>
<ul>
<li><p>首先会去查找 <code>student:name</code> 中值以 <code>xiaolan</code> 开头的所有数据获得 <code>参考数据集</code>，这一步等同于 valueFilter 过滤器；</p>
</li>
<li><p>其次再用参考数据集中所有数据的时间戳去检索其他列，获得时间戳相同的其他列的数据作为 <code>结果数据集</code>，这一步等同于时间戳过滤器；</p>
</li>
<li><p>最后如果 <code>dropDependentColumn</code> 为 true，则返回 <code>参考数据集</code>+<code>结果数据集</code>，若为 false，则抛弃参考数据集，只返回 <code>结果数据集</code>。</p>
</li>
</ul>
<h2 id="四、专用过滤器"><a href="#四、专用过滤器" class="headerlink" title="四、专用过滤器"></a>四、专用过滤器</h2><p>专用过滤器通常直接继承自 <code>FilterBase</code>，适用于范围更小的筛选规则。</p>
<h3 id="4-1-单列列值过滤器-SingleColumnValueFilter"><a href="#4-1-单列列值过滤器-SingleColumnValueFilter" class="headerlink" title="4.1 单列列值过滤器 (SingleColumnValueFilter)"></a>4.1 单列列值过滤器 (SingleColumnValueFilter)</h3><p>基于某列（参考列）的值决定某行数据是否被过滤。其实例有以下方法：</p>
<ul>
<li><strong>setFilterIfMissing(boolean filterIfMissing)</strong> ：默认值为 false，即如果该行数据不包含参考列，其依然被包含在最后的结果中；设置为 true 时，则不包含；</li>
<li><strong>setLatestVersionOnly(boolean latestVersionOnly)</strong> ：默认为 true，即只检索参考列的最新版本数据；设置为 false，则检索所有版本数据。</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">SingleColumnValueFilter singleColumnValueFilter = new SingleColumnValueFilter(</span><br><span class="line">                "student".getBytes(), </span><br><span class="line">                "name".getBytes(), </span><br><span class="line">                CompareOperator.EQUAL, </span><br><span class="line">                new SubstringComparator("xiaolan"));</span><br><span class="line">singleColumnValueFilter.setFilterIfMissing(true);</span><br><span class="line">scan.setFilter(singleColumnValueFilter);</span><br></pre></td></tr></table></figure>
<h3 id="4-2-单列列值排除器-SingleColumnValueExcludeFilter"><a href="#4-2-单列列值排除器-SingleColumnValueExcludeFilter" class="headerlink" title="4.2 单列列值排除器 (SingleColumnValueExcludeFilter)"></a>4.2 单列列值排除器 (SingleColumnValueExcludeFilter)</h3><p><code>SingleColumnValueExcludeFilter</code> 继承自上面的 <code>SingleColumnValueFilter</code>，过滤行为与其相反。</p>
<h3 id="4-3-行键前缀过滤器-PrefixFilter"><a href="#4-3-行键前缀过滤器-PrefixFilter" class="headerlink" title="4.3 行键前缀过滤器 (PrefixFilter)"></a>4.3 行键前缀过滤器 (PrefixFilter)</h3><p>基于 RowKey 值决定某行数据是否被过滤。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">PrefixFilter prefixFilter = <span class="keyword">new</span> PrefixFilter(Bytes.toBytes(<span class="string">"xxx"</span>));</span><br><span class="line">scan.setFilter(prefixFilter);</span><br></pre></td></tr></table></figure>
<h3 id="4-4-列名前缀过滤器-ColumnPrefixFilter"><a href="#4-4-列名前缀过滤器-ColumnPrefixFilter" class="headerlink" title="4.4 列名前缀过滤器 (ColumnPrefixFilter)"></a>4.4 列名前缀过滤器 (ColumnPrefixFilter)</h3><p>基于列限定符（列名）决定某行数据是否被过滤。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">ColumnPrefixFilter columnPrefixFilter = <span class="keyword">new</span> ColumnPrefixFilter(Bytes.toBytes(<span class="string">"xxx"</span>));</span><br><span class="line"> scan.setFilter(columnPrefixFilter);</span><br></pre></td></tr></table></figure>
<h3 id="4-5-分页过滤器-PageFilter"><a href="#4-5-分页过滤器-PageFilter" class="headerlink" title="4.5 分页过滤器 (PageFilter)"></a>4.5 分页过滤器 (PageFilter)</h3><p>可以使用这个过滤器实现对结果按行进行分页，创建 PageFilter 实例的时候需要传入每页的行数。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">PageFilter</span><span class="params">(<span class="keyword">final</span> <span class="keyword">long</span> pageSize)</span> </span>&#123;</span><br><span class="line">    Preconditions.checkArgument(pageSize &gt;= <span class="number">0</span>, <span class="string">"must be positive %s"</span>, pageSize);</span><br><span class="line">    <span class="keyword">this</span>.pageSize = pageSize;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>下面的代码体现了客户端实现分页查询的主要逻辑，这里对其进行一下解释说明：</p>
<p>客户端进行分页查询，需要传递 <code>startRow</code>(起始 RowKey)，知道起始 <code>startRow</code> 后，就可以返回对应的 pageSize 行数据。这里唯一的问题就是，对于第一次查询，显然 <code>startRow</code> 就是表格的第一行数据，但是之后第二次、第三次查询我们并不知道 <code>startRow</code>，只能知道上一次查询的最后一条数据的 RowKey（简单称之为 <code>lastRow</code>）。</p>
<p>我们不能将 <code>lastRow</code> 作为新一次查询的 <code>startRow</code> 传入，因为 scan 的查询区间是[startRow，endRow) ，即前开后闭区间，这样 <code>startRow</code> 在新的查询也会被返回，这条数据就重复了。</p>
<p>同时在不使用第三方数据库存储 RowKey 的情况下，我们是无法通过知道 <code>lastRow</code> 的下一个 RowKey 的，因为 RowKey 的设计可能是连续的也有可能是不连续的。</p>
<p>由于 Hbase 的 RowKey 是按照字典序进行排序的。这种情况下，就可以在 <code>lastRow</code> 后面加上 <code>0</code> ，作为 <code>startRow</code> 传入，因为按照字典序的规则，某个值加上 <code>0</code> 后的新值，在字典序上一定是这个值的下一个值，对于 HBase 来说下一个 RowKey 在字典序上一定也是等于或者大于这个新值的。</p>
<p>所以最后传入 <code>lastRow</code>+<code>0</code>，如果等于这个值的 RowKey 存在就从这个值开始 scan,否则从字典序的下一个 RowKey 开始 scan。</p>
<blockquote>
<p>25 个字母以及数字字符，字典排序如下:</p>
<p><code>&#39;0&#39; &lt; &#39;1&#39; &lt; &#39;2&#39; &lt; ... &lt; &#39;9&#39; &lt; &#39;a&#39; &lt; &#39;b&#39; &lt; ... &lt; &#39;z&#39;</code></p>
</blockquote>
<p>分页查询主要实现逻辑：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">byte</span>[] POSTFIX = <span class="keyword">new</span> <span class="keyword">byte</span>[] &#123; <span class="number">0x00</span> &#125;;</span><br><span class="line">Filter filter = <span class="keyword">new</span> PageFilter(<span class="number">15</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> totalRows = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">byte</span>[] lastRow = <span class="keyword">null</span>;</span><br><span class="line"><span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">    Scan scan = <span class="keyword">new</span> Scan();</span><br><span class="line">    scan.setFilter(filter);</span><br><span class="line">    <span class="keyword">if</span> (lastRow != <span class="keyword">null</span>) &#123;</span><br><span class="line">        <span class="comment">// 如果不是首行 则 lastRow + 0</span></span><br><span class="line">        <span class="keyword">byte</span>[] startRow = Bytes.add(lastRow, POSTFIX);</span><br><span class="line">        System.out.println(<span class="string">"start row: "</span> +</span><br><span class="line">                           Bytes.toStringBinary(startRow));</span><br><span class="line">        scan.withStartRow(startRow);</span><br><span class="line">    &#125;</span><br><span class="line">    ResultScanner scanner = table.getScanner(scan);</span><br><span class="line">    <span class="keyword">int</span> localRows = <span class="number">0</span>;</span><br><span class="line">    Result result;</span><br><span class="line">    <span class="keyword">while</span> ((result = scanner.next()) != <span class="keyword">null</span>) &#123;</span><br><span class="line">        System.out.println(localRows++ + <span class="string">": "</span> + result);</span><br><span class="line">        totalRows++;</span><br><span class="line">        lastRow = result.getRow();</span><br><span class="line">    &#125;</span><br><span class="line">    scanner.close();</span><br><span class="line">    <span class="comment">//最后一页，查询结束  </span></span><br><span class="line">    <span class="keyword">if</span> (localRows == <span class="number">0</span>) <span class="keyword">break</span>;</span><br><span class="line">&#125;</span><br><span class="line">System.out.println(<span class="string">"total rows: "</span> + totalRows);</span><br></pre></td></tr></table></figure>
<blockquote>
<p>需要注意的是在多台 Regin Services 上执行分页过滤的时候，由于并行执行的过滤器不能共享它们的状态和边界，所以有可能每个过滤器都会在完成扫描前获取了 PageCount 行的结果，这种情况下会返回比分页条数更多的数据，分页过滤器就有失效的可能。</p>
</blockquote>
<h3 id="4-6-时间戳过滤器-TimestampsFilter"><a href="#4-6-时间戳过滤器-TimestampsFilter" class="headerlink" title="4.6 时间戳过滤器 (TimestampsFilter)"></a>4.6 时间戳过滤器 (TimestampsFilter)</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">List&lt;Long&gt; list = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">list.add(<span class="number">1554975573000L</span>);</span><br><span class="line">TimestampsFilter timestampsFilter = <span class="keyword">new</span> TimestampsFilter(list);</span><br><span class="line">scan.setFilter(timestampsFilter);</span><br></pre></td></tr></table></figure>
<h3 id="4-7-首次行键过滤器-FirstKeyOnlyFilter"><a href="#4-7-首次行键过滤器-FirstKeyOnlyFilter" class="headerlink" title="4.7 首次行键过滤器 (FirstKeyOnlyFilter)"></a>4.7 首次行键过滤器 (FirstKeyOnlyFilter)</h3><p><code>FirstKeyOnlyFilter</code> 只扫描每行的第一列，扫描完第一列后就结束对当前行的扫描，并跳转到下一行。相比于全表扫描，其性能更好，通常用于行数统计的场景，因为如果某一行存在，则行中必然至少有一列。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">FirstKeyOnlyFilter firstKeyOnlyFilter = <span class="keyword">new</span> FirstKeyOnlyFilter();</span><br><span class="line">scan.set(firstKeyOnlyFilter);</span><br></pre></td></tr></table></figure>
<h2 id="五、包装过滤器"><a href="#五、包装过滤器" class="headerlink" title="五、包装过滤器"></a>五、包装过滤器</h2><p>包装过滤器就是通过包装其他过滤器以实现某些拓展的功能。</p>
<h3 id="5-1-SkipFilter过滤器"><a href="#5-1-SkipFilter过滤器" class="headerlink" title="5.1 SkipFilter过滤器"></a>5.1 SkipFilter过滤器</h3><p><code>SkipFilter</code> 包装一个过滤器，当被包装的过滤器遇到一个需要过滤的 KeyValue 实例时，则拓展过滤整行数据。下面是一个使用示例：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 定义 ValueFilter 过滤器</span></span><br><span class="line">Filter filter1 = <span class="keyword">new</span> ValueFilter(CompareOperator.NOT_EQUAL,</span><br><span class="line">      <span class="keyword">new</span> BinaryComparator(Bytes.toBytes(<span class="string">"xxx"</span>)));</span><br><span class="line"><span class="comment">// 使用 SkipFilter 进行包装</span></span><br><span class="line">Filter filter2 = <span class="keyword">new</span> SkipFilter(filter1);</span><br></pre></td></tr></table></figure>
<h3 id="5-2-WhileMatchFilter过滤器"><a href="#5-2-WhileMatchFilter过滤器" class="headerlink" title="5.2 WhileMatchFilter过滤器"></a>5.2 WhileMatchFilter过滤器</h3><p><code>WhileMatchFilter</code> 包装一个过滤器，当被包装的过滤器遇到一个需要过滤的 KeyValue 实例时，<code>WhileMatchFilter</code> 则结束本次扫描，返回已经扫描到的结果。下面是其使用示例：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Filter filter1 = <span class="keyword">new</span> RowFilter(CompareOperator.NOT_EQUAL,</span><br><span class="line">                               <span class="keyword">new</span> BinaryComparator(Bytes.toBytes(<span class="string">"rowKey4"</span>)));</span><br><span class="line"></span><br><span class="line">Scan scan = <span class="keyword">new</span> Scan();</span><br><span class="line">scan.setFilter(filter1);</span><br><span class="line">ResultScanner scanner1 = table.getScanner(scan);</span><br><span class="line"><span class="keyword">for</span> (Result result : scanner1) &#123;</span><br><span class="line">    <span class="keyword">for</span> (Cell cell : result.listCells()) &#123;</span><br><span class="line">        System.out.println(cell);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">scanner1.close();</span><br><span class="line"></span><br><span class="line">System.out.println(<span class="string">"--------------------"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 使用 WhileMatchFilter 进行包装</span></span><br><span class="line">Filter filter2 = <span class="keyword">new</span> WhileMatchFilter(filter1);</span><br><span class="line"></span><br><span class="line">scan.setFilter(filter2);</span><br><span class="line">ResultScanner scanner2 = table.getScanner(scan);</span><br><span class="line"><span class="keyword">for</span> (Result result : scanner1) &#123;</span><br><span class="line">    <span class="keyword">for</span> (Cell cell : result.listCells()) &#123;</span><br><span class="line">        System.out.println(cell);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">scanner2.close();</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">rowKey0/student:name/1555035006994/Put/vlen=8/seqid=0</span><br><span class="line">rowKey1/student:name/1555035007019/Put/vlen=8/seqid=0</span><br><span class="line">rowKey2/student:name/1555035007025/Put/vlen=8/seqid=0</span><br><span class="line">rowKey3/student:name/1555035007037/Put/vlen=8/seqid=0</span><br><span class="line">rowKey5/student:name/1555035007051/Put/vlen=8/seqid=0</span><br><span class="line">rowKey6/student:name/1555035007057/Put/vlen=8/seqid=0</span><br><span class="line">rowKey7/student:name/1555035007062/Put/vlen=8/seqid=0</span><br><span class="line">rowKey8/student:name/1555035007068/Put/vlen=8/seqid=0</span><br><span class="line">rowKey9/student:name/1555035007073/Put/vlen=8/seqid=0</span><br><span class="line">--------------------</span><br><span class="line">rowKey0/student:name/1555035006994/Put/vlen=8/seqid=0</span><br><span class="line">rowKey1/student:name/1555035007019/Put/vlen=8/seqid=0</span><br><span class="line">rowKey2/student:name/1555035007025/Put/vlen=8/seqid=0</span><br><span class="line">rowKey3/student:name/1555035007037/Put/vlen=8/seqid=0</span><br></pre></td></tr></table></figure>
<p>可以看到被包装后，只返回了 <code>rowKey4</code> 之前的数据。</p>
<h2 id="六、FilterList"><a href="#六、FilterList" class="headerlink" title="六、FilterList"></a>六、FilterList</h2><p>以上都是讲解单个过滤器的作用，当需要多个过滤器共同作用于一次查询的时候，就需要使用 <code>FilterList</code>。<code>FilterList</code> 支持通过构造器或者 <code>addFilter</code> 方法传入多个过滤器。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 构造器传入</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">FilterList</span><span class="params">(<span class="keyword">final</span> Operator operator, <span class="keyword">final</span> List&lt;Filter&gt; filters)</span></span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">FilterList</span><span class="params">(<span class="keyword">final</span> List&lt;Filter&gt; filters)</span></span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">FilterList</span><span class="params">(<span class="keyword">final</span> Filter... filters)</span></span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function"><span class="comment">// 方法传入</span></span></span><br><span class="line"><span class="function"> <span class="keyword">public</span> <span class="keyword">void</span> <span class="title">addFilter</span><span class="params">(List&lt;Filter&gt; filters)</span></span></span><br><span class="line"><span class="function"> <span class="keyword">public</span> <span class="keyword">void</span> <span class="title">addFilter</span><span class="params">(Filter filter)</span></span></span><br></pre></td></tr></table></figure>
<p>多个过滤器组合的结果由 <code>operator</code> 参数定义 ，其可选参数定义在 <code>Operator</code> 枚举类中。只有 <code>MUST_PASS_ALL</code> 和 <code>MUST_PASS_ONE</code> 两个可选的值：</p>
<ul>
<li><strong>MUST_PASS_ALL</strong> ：相当于 AND，必须所有的过滤器都通过才认为通过；</li>
<li><strong>MUST_PASS_ONE</strong> ：相当于 OR，只有要一个过滤器通过则认为通过。</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@InterfaceAudience</span>.Public</span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">enum</span> Operator &#123;</span><br><span class="line">    <span class="comment">/** !AND */</span></span><br><span class="line">    MUST_PASS_ALL,</span><br><span class="line">    <span class="comment">/** !OR */</span></span><br><span class="line">    MUST_PASS_ONE</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>使用示例如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">List&lt;Filter&gt; filters = <span class="keyword">new</span> ArrayList&lt;Filter&gt;();</span><br><span class="line"></span><br><span class="line">Filter filter1 = <span class="keyword">new</span> RowFilter(CompareOperator.GREATER_OR_EQUAL,</span><br><span class="line">                               <span class="keyword">new</span> BinaryComparator(Bytes.toBytes(<span class="string">"XXX"</span>)));</span><br><span class="line">filters.add(filter1);</span><br><span class="line"></span><br><span class="line">Filter filter2 = <span class="keyword">new</span> RowFilter(CompareOperator.LESS_OR_EQUAL,</span><br><span class="line">                               <span class="keyword">new</span> BinaryComparator(Bytes.toBytes(<span class="string">"YYY"</span>)));</span><br><span class="line">filters.add(filter2);</span><br><span class="line"></span><br><span class="line">Filter filter3 = <span class="keyword">new</span> QualifierFilter(CompareOperator.EQUAL,</span><br><span class="line">                                     <span class="keyword">new</span> RegexStringComparator(<span class="string">"ZZZ"</span>));</span><br><span class="line">filters.add(filter3);</span><br><span class="line"></span><br><span class="line">FilterList filterList = <span class="keyword">new</span> FilterList(filters);</span><br><span class="line"></span><br><span class="line">Scan scan = <span class="keyword">new</span> Scan();</span><br><span class="line">scan.setFilter(filterList);</span><br></pre></td></tr></table></figure>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://www.oreilly.com/library/view/hbase-the-definitive/9781449314682/ch04.html" target="_blank" rel="noopener">HBase: The Definitive Guide _&gt;  Chapter 4. Client API: Advanced Features</a></p>
]]></content>
      <categories>
        <category>notes</category>
        <category>HBase</category>
      </categories>
      <tags>
        <tag>HBase</tag>
      </tags>
  </entry>
  <entry>
    <title>Hive CLI和Beeline命令行的基本使用</title>
    <url>/2021/03/18/HiveCLI%E5%92%8CBeeline%E5%91%BD%E4%BB%A4%E8%A1%8C%E7%9A%84%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<h2 id="一、Hive-CLI"><a href="#一、Hive-CLI" class="headerlink" title="一、Hive CLI"></a>一、Hive CLI</h2><h3 id="1-1-Help"><a href="#1-1-Help" class="headerlink" title="1.1 Help"></a>1.1 Help</h3><p>使用 <code>hive -H</code> 或者 <code>hive --help</code> 命令可以查看所有命令的帮助，显示如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">usage: hive</span><br><span class="line"> -d,--define &lt;key=value&gt;          Variable subsitution to apply to hive </span><br><span class="line">                                  commands. e.g. -d A=B or --define A=B  --定义用户自定义变量</span><br><span class="line">    --database &lt;databasename&gt;     Specify the database to use  -- 指定使用的数据库</span><br><span class="line"> -e &lt;quoted-query-string&gt;         SQL from command line   -- 执行指定的 SQL</span><br><span class="line"> -f &lt;filename&gt;                    SQL from files   --执行 SQL 脚本</span><br><span class="line"> -H,--help                        Print help information  -- 打印帮助信息</span><br><span class="line">    --hiveconf &lt;property=value&gt;   Use value for given property    --自定义配置</span><br><span class="line">    --hivevar &lt;key=value&gt;         Variable subsitution to apply to hive  --自定义变量</span><br><span class="line">                                  commands. e.g. --hivevar A=B</span><br><span class="line"> -i &lt;filename&gt;                    Initialization SQL file  --在进入交互模式之前运行初始化脚本</span><br><span class="line"> -S,--silent                      Silent mode in interactive shell    --静默模式</span><br><span class="line"> -v,--verbose                     Verbose mode (echo executed SQL to the  console)  --详细模式</span><br></pre></td></tr></table></figure>
<h3 id="1-2-交互式命令行"><a href="#1-2-交互式命令行" class="headerlink" title="1.2 交互式命令行"></a>1.2 交互式命令行</h3><p>直接使用 <code>Hive</code> 命令，不加任何参数，即可进入交互式命令行。</p>
<h3 id="1-3-执行SQL命令"><a href="#1-3-执行SQL命令" class="headerlink" title="1.3 执行SQL命令"></a>1.3 执行SQL命令</h3><p>在不进入交互式命令行的情况下，可以使用 <code>hive -e</code> 执行 SQL 命令。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive -e '<span class="keyword">select</span> * <span class="keyword">from</span> emp<span class="string">';</span></span><br></pre></td></tr></table></figure>
<div align="center"> <img width="700px" src="../pictures/hive-e.png"> </div>



<h3 id="1-4-执行SQL脚本"><a href="#1-4-执行SQL脚本" class="headerlink" title="1.4 执行SQL脚本"></a>1.4 执行SQL脚本</h3><p>用于执行的 sql 脚本可以在本地文件系统，也可以在 HDFS 上。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 本地文件系统</span></span><br><span class="line">hive -f /usr/file/simple.sql;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> HDFS文件系统</span></span><br><span class="line">hive -f hdfs://hadoop001:8020/tmp/simple.sql;</span><br></pre></td></tr></table></figure>
<p>其中 <code>simple.sql</code> 内容如下：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp;</span><br></pre></td></tr></table></figure>
<h3 id="1-5-配置Hive变量"><a href="#1-5-配置Hive变量" class="headerlink" title="1.5 配置Hive变量"></a>1.5 配置Hive变量</h3><p>可以使用 <code>--hiveconf</code> 设置 Hive 运行时的变量。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive -e '<span class="keyword">select</span> * <span class="keyword">from</span> emp<span class="string">' \</span></span><br><span class="line"><span class="string">--hiveconf hive.exec.scratchdir=/tmp/hive_scratch  \</span></span><br><span class="line"><span class="string">--hiveconf mapred.reduce.tasks=4;</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>hive.exec.scratchdir：指定 HDFS 上目录位置，用于存储不同 map/reduce 阶段的执行计划和这些阶段的中间输出结果。</p>
</blockquote>
<h3 id="1-6-配置文件启动"><a href="#1-6-配置文件启动" class="headerlink" title="1.6 配置文件启动"></a>1.6 配置文件启动</h3><p>使用 <code>-i</code> 可以在进入交互模式之前运行初始化脚本，相当于指定配置文件启动。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hive -i /usr/file/hive-init.conf;</span><br></pre></td></tr></table></figure>
<p>其中 <code>hive-init.conf</code> 的内容如下：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.exec.mode.local.auto = <span class="literal">true</span>;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>hive.exec.mode.local.auto 默认值为 false，这里设置为 true ，代表开启本地模式。</p>
</blockquote>
<h3 id="1-7-用户自定义变量"><a href="#1-7-用户自定义变量" class="headerlink" title="1.7 用户自定义变量"></a>1.7 用户自定义变量</h3><p><code>--define &lt;key=value&gt;</code> 和 <code>--hivevar &lt;key=value&gt;</code> 在功能上是等价的，都是用来实现自定义变量，这里给出一个示例:</p>
<p>定义变量：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive  <span class="comment">--define  n=ename --hiveconf  --hivevar j=job;</span></span><br></pre></td></tr></table></figure>
<p>在查询中引用自定义变量：</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 以下两条语句等价</span></span><br><span class="line">hive &gt; select $&#123;n&#125; from emp;</span><br><span class="line">hive &gt;  select $&#123;hivevar:n&#125; from emp;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 以下两条语句等价</span></span><br><span class="line">hive &gt; select $&#123;j&#125; from emp;</span><br><span class="line">hive &gt;  select $&#123;hivevar:j&#125; from emp;</span><br></pre></td></tr></table></figure>
<p>结果如下：</p>
<div align="center"> <img width="700px" src="../pictures/hive-n-j.png"> </div>

<h2 id="二、Beeline"><a href="#二、Beeline" class="headerlink" title="二、Beeline"></a>二、Beeline</h2><h3 id="2-1-HiveServer2"><a href="#2-1-HiveServer2" class="headerlink" title="2.1 HiveServer2"></a>2.1 HiveServer2</h3><p>Hive 内置了 HiveServer 和 HiveServer2 服务，两者都允许客户端使用多种编程语言进行连接，但是 HiveServer 不能处理多个客户端的并发请求，所以产生了 HiveServer2。</p>
<p>HiveServer2（HS2）允许远程客户端可以使用各种编程语言向 Hive 提交请求并检索结果，支持多客户端并发访问和身份验证。HS2 是由多个服务组成的单个进程，其包括基于 Thrift 的 Hive 服务（TCP 或 HTTP）和用于 Web UI 的 Jetty Web 服务器。</p>
<p> HiveServer2 拥有自己的 CLI(Beeline)，Beeline 是一个基于 SQLLine 的 JDBC 客户端。由于 HiveServer2 是 Hive 开发维护的重点 (Hive0.15 后就不再支持 hiveserver)，所以 Hive CLI 已经不推荐使用了，官方更加推荐使用 Beeline。</p>
<h3 id="2-1-Beeline"><a href="#2-1-Beeline" class="headerlink" title="2.1 Beeline"></a>2.1 Beeline</h3><p>Beeline 拥有更多可使用参数，可以使用 <code>beeline --help</code> 查看，完整参数如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Usage: java org.apache.hive.cli.beeline.BeeLine</span><br><span class="line">   -u &lt;database url&gt;               the JDBC URL to connect to</span><br><span class="line">   -r                              reconnect to last saved connect url (in conjunction with !save)</span><br><span class="line">   -n &lt;username&gt;                   the username to connect as</span><br><span class="line">   -p &lt;password&gt;                   the password to connect as</span><br><span class="line">   -d &lt;driver class&gt;               the driver class to use</span><br><span class="line">   -i &lt;init file&gt;                  script file for initialization</span><br><span class="line">   -e &lt;query&gt;                      query that should be executed</span><br><span class="line">   -f &lt;exec file&gt;                  script file that should be executed</span><br><span class="line">   -w (or) --password-file &lt;password file&gt;  the password file to read password from</span><br><span class="line">   --hiveconf property=value       Use value for given property</span><br><span class="line">   --hivevar name=value            hive variable name and value</span><br><span class="line">                                   This is Hive specific settings in which variables</span><br><span class="line">                                   can be set at session level and referenced in Hive</span><br><span class="line">                                   commands or queries.</span><br><span class="line">   --property-file=&lt;property-file&gt; the file to read connection properties (url, driver, user, password) from</span><br><span class="line">   --color=[true/false]            control whether color is used for display</span><br><span class="line">   --showHeader=[true/false]       show column names in query results</span><br><span class="line">   --headerInterval=ROWS;          the interval between which heades are displayed</span><br><span class="line">   --fastConnect=[true/false]      skip building table/column list for tab-completion</span><br><span class="line">   --autoCommit=[true/false]       enable/disable automatic transaction commit</span><br><span class="line">   --verbose=[true/false]          show verbose error messages and debug info</span><br><span class="line">   --showWarnings=[true/false]     display connection warnings</span><br><span class="line">   --showNestedErrs=[true/false]   display nested errors</span><br><span class="line">   --numberFormat=[pattern]        format numbers using DecimalFormat pattern</span><br><span class="line">   --force=[true/false]            continue running script even after errors</span><br><span class="line">   --maxWidth=MAXWIDTH             the maximum width of the terminal</span><br><span class="line">   --maxColumnWidth=MAXCOLWIDTH    the maximum width to use when displaying columns</span><br><span class="line">   --silent=[true/false]           be more silent</span><br><span class="line">   --autosave=[true/false]         automatically save preferences</span><br><span class="line">   --outputformat=[table/vertical/csv2/tsv2/dsv/csv/tsv]  format mode for result display</span><br><span class="line">   --incrementalBufferRows=NUMROWS the number of rows to buffer when printing rows on stdout,</span><br><span class="line">                                   defaults to 1000; only applicable if --incremental=true</span><br><span class="line">                                   and --outputformat=table</span><br><span class="line">   --truncateTable=[true/false]    truncate table column when it exceeds length</span><br><span class="line">   --delimiterForDSV=DELIMITER     specify the delimiter for delimiter-separated values output format (default: |)</span><br><span class="line">   --isolation=LEVEL               set the transaction isolation level</span><br><span class="line">   --nullemptystring=[true/false]  set to true to get historic behavior of printing null as empty string</span><br><span class="line">   --maxHistoryRows=MAXHISTORYROWS The maximum number of rows to store beeline history.</span><br><span class="line">   --convertBinaryArrayToString=[true/false]    display binary column data as string or as byte array</span><br><span class="line">   --help                          display this message</span><br></pre></td></tr></table></figure>
<h3 id="2-3-常用参数"><a href="#2-3-常用参数" class="headerlink" title="2.3 常用参数"></a>2.3 常用参数</h3><p>在 Hive CLI 中支持的参数，Beeline 都支持，常用的参数如下。更多参数说明可以参见官方文档 <a href="https://cwiki.apache.org/confluence/display/Hive/HiveServer2+Clients#HiveServer2Clients-Beeline%E2%80%93NewCommandLineShell" target="_blank" rel="noopener">Beeline Command Options</a></p>
<table>
<thead>
<tr>
<th>参数</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>-u \<database url></database></strong></td>
<td>数据库地址</td>
</tr>
<tr>
<td><strong>-n \<username></username></strong></td>
<td>用户名</td>
</tr>
<tr>
<td><strong>-p \<password></password></strong></td>
<td>密码</td>
</tr>
<tr>
<td><strong>-d \<driver class></driver></strong></td>
<td>驱动 (可选)</td>
</tr>
<tr>
<td><strong>-e \<query></query></strong></td>
<td>执行 SQL 命令</td>
</tr>
<tr>
<td><strong>-f \<file></file></strong></td>
<td>执行 SQL 脚本</td>
</tr>
<tr>
<td><strong>-i  (or)–init  \<file or files></file></strong></td>
<td>在进入交互模式之前运行初始化脚本</td>
</tr>
<tr>
<td><strong>–property-file \<file></file></strong></td>
<td>指定配置文件</td>
</tr>
<tr>
<td><strong>–hiveconf</strong> <em>property<strong>=</strong>value</em></td>
<td>指定配置属性</td>
</tr>
<tr>
<td><strong>–hivevar</strong> <em>name<strong>=</strong>value</em></td>
<td>用户自定义属性，在会话级别有效</td>
</tr>
</tbody>
</table>
<p>示例： 使用用户名和密码连接 Hive</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> beeline -u jdbc:hive2://localhost:10000  -n username -p password</span></span><br></pre></td></tr></table></figure>
<p>​         </p>
<h2 id="三、Hive配置"><a href="#三、Hive配置" class="headerlink" title="三、Hive配置"></a>三、Hive配置</h2><p>可以通过三种方式对 Hive 的相关属性进行配置，分别介绍如下：</p>
<h3 id="3-1-配置文件"><a href="#3-1-配置文件" class="headerlink" title="3.1 配置文件"></a>3.1 配置文件</h3><p>方式一为使用配置文件，使用配置文件指定的配置是永久有效的。Hive 有以下三个可选的配置文件：</p>
<ul>
<li><p>hive-site.xml ：Hive 的主要配置文件；</p>
</li>
<li><p>hivemetastore-site.xml： 关于元数据的配置；</p>
</li>
<li>hiveserver2-site.xml：关于 HiveServer2 的配置。</li>
</ul>
<p>示例如下,在 hive-site.xml 配置 <code>hive.exec.scratchdir</code>：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.exec.scratchdir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>/tmp/mydir<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">description</span>&gt;</span>Scratch space for Hive jobs<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h3 id="3-2-hiveconf"><a href="#3-2-hiveconf" class="headerlink" title="3.2 hiveconf"></a>3.2 hiveconf</h3><p>方式二为在启动命令行 (Hive CLI / Beeline) 的时候使用 <code>--hiveconf</code> 指定配置，这种方式指定的配置作用于整个 Session。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hive --hiveconf hive.exec.scratchdir=/tmp/mydir</span><br></pre></td></tr></table></figure>
<h3 id="3-3-set"><a href="#3-3-set" class="headerlink" title="3.3 set"></a>3.3 set</h3><p>方式三为在交互式环境下 (Hive CLI / Beeline)，使用 set 命令指定。这种设置的作用范围也是 Session 级别的，配置对于执行该命令后的所有命令生效。set 兼具设置参数和查看参数的功能。如下：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">0: jdbc:hive2://hadoop001:10000&gt; set hive.exec.scratchdir=/tmp/mydir;</span><br><span class="line">No rows affected (0.025 seconds)</span><br><span class="line">0: jdbc:hive2://hadoop001:10000&gt; set hive.exec.scratchdir;</span><br><span class="line">+----------------------------------+--+</span><br><span class="line">|               set                |</span><br><span class="line">+----------------------------------+--+</span><br><span class="line">| hive.exec.scratchdir=/tmp/mydir  |</span><br><span class="line">+----------------------------------+--+</span><br></pre></td></tr></table></figure>
<h3 id="3-4-配置优先级"><a href="#3-4-配置优先级" class="headerlink" title="3.4 配置优先级"></a>3.4 配置优先级</h3><p>配置的优先顺序如下 (由低到高)：<br><code>hive-site.xml</code> - &gt;<code>hivemetastore-site.xml</code>- &gt; <code>hiveserver2-site.xml</code> - &gt;<code>-- hiveconf</code>- &gt; <code>set</code></p>
<h3 id="3-5-配置参数"><a href="#3-5-配置参数" class="headerlink" title="3.5 配置参数"></a>3.5 配置参数</h3><p>Hive 可选的配置参数非常多，在用到时查阅官方文档即可<a href="https://cwiki.apache.org/confluence/display/Hive/AdminManual+Configuration" target="_blank" rel="noopener">AdminManual Configuration</a></p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol>
<li><a href="https://cwiki.apache.org/confluence/display/Hive/HiveServer2+Clients" target="_blank" rel="noopener">HiveServer2 Clients</a></li>
<li><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Cli" target="_blank" rel="noopener">LanguageManual Cli</a></li>
<li><a href="https://cwiki.apache.org/confluence/display/Hive/AdminManual+Configuration" target="_blank" rel="noopener">AdminManual Configuration</a></li>
</ol>
]]></content>
      <categories>
        <category>notes</category>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title>Hive常用DDL操作</title>
    <url>/2021/03/18/Hive%E5%B8%B8%E7%94%A8DDL%E6%93%8D%E4%BD%9C/</url>
    <content><![CDATA[<h2 id="一、Database"><a href="#一、Database" class="headerlink" title="一、Database"></a>一、Database</h2><h3 id="1-1-查看数据列表"><a href="#1-1-查看数据列表" class="headerlink" title="1.1 查看数据列表"></a>1.1 查看数据列表</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">show</span> <span class="keyword">databases</span>;</span><br></pre></td></tr></table></figure>
<div align="center"> <img width="700px" src="../pictures/hive-show-database.png"> </div>

<h3 id="1-2-使用数据库"><a href="#1-2-使用数据库" class="headerlink" title="1.2 使用数据库"></a>1.2 使用数据库</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">USE</span> database_name;</span><br></pre></td></tr></table></figure>
<h3 id="1-3-新建数据库"><a href="#1-3-新建数据库" class="headerlink" title="1.3 新建数据库"></a>1.3 新建数据库</h3><p>语法：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> (<span class="keyword">DATABASE</span>|<span class="keyword">SCHEMA</span>) [<span class="keyword">IF</span> <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>] database_name   <span class="comment">--DATABASE|SCHEMA 是等价的</span></span><br><span class="line">  [<span class="keyword">COMMENT</span> database_comment] <span class="comment">--数据库注释</span></span><br><span class="line">  [LOCATION hdfs_path] <span class="comment">--存储在 HDFS 上的位置</span></span><br><span class="line">  [<span class="keyword">WITH</span> DBPROPERTIES (property_name=property_value, ...)]; <span class="comment">--指定额外属性</span></span><br></pre></td></tr></table></figure>
<p>示例：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">DATABASE</span> <span class="keyword">IF</span> <span class="keyword">NOT</span> <span class="keyword">EXISTS</span> hive_test</span><br><span class="line">  <span class="keyword">COMMENT</span> <span class="string">'hive database for test'</span></span><br><span class="line">  <span class="keyword">WITH</span> DBPROPERTIES (<span class="string">'create'</span>=<span class="string">'heibaiying'</span>);</span><br></pre></td></tr></table></figure>
<h3 id="1-4-查看数据库信息"><a href="#1-4-查看数据库信息" class="headerlink" title="1.4 查看数据库信息"></a>1.4 查看数据库信息</h3><p>语法：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">DESC DATABASE [EXTENDED] db_name; <span class="comment">--EXTENDED 表示是否显示额外属性</span></span><br></pre></td></tr></table></figure>
<p>示例：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">DESC DATABASE  EXTENDED hive_test;</span><br></pre></td></tr></table></figure>
<h3 id="1-5-删除数据库"><a href="#1-5-删除数据库" class="headerlink" title="1.5 删除数据库"></a>1.5 删除数据库</h3><p>语法：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">DROP</span> (<span class="keyword">DATABASE</span>|<span class="keyword">SCHEMA</span>) [<span class="keyword">IF</span> <span class="keyword">EXISTS</span>] database_name [RESTRICT|<span class="keyword">CASCADE</span>];</span><br></pre></td></tr></table></figure>
<ul>
<li>默认行为是 RESTRICT，如果数据库中存在表则删除失败。要想删除库及其中的表，可以使用 CASCADE 级联删除。</li>
</ul>
<p>示例：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">DROP</span> <span class="keyword">DATABASE</span> <span class="keyword">IF</span> <span class="keyword">EXISTS</span> hive_test <span class="keyword">CASCADE</span>;</span><br></pre></td></tr></table></figure>
<h2 id="二、创建表"><a href="#二、创建表" class="headerlink" title="二、创建表"></a>二、创建表</h2><h3 id="2-1-建表语法"><a href="#2-1-建表语法" class="headerlink" title="2.1 建表语法"></a>2.1 建表语法</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> [<span class="keyword">TEMPORARY</span>] [<span class="keyword">EXTERNAL</span>] <span class="keyword">TABLE</span> [<span class="keyword">IF</span> <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>] [db_name.]table_name     <span class="comment">--表名</span></span><br><span class="line">  [(col_name data_type [<span class="keyword">COMMENT</span> col_comment],</span><br><span class="line">    ... [constraint_specification])]  <span class="comment">--列名 列数据类型</span></span><br><span class="line">  [<span class="keyword">COMMENT</span> table_comment]   <span class="comment">--表描述</span></span><br><span class="line">  [PARTITIONED <span class="keyword">BY</span> (col_name data_type [<span class="keyword">COMMENT</span> col_comment], ...)]  <span class="comment">--分区表分区规则</span></span><br><span class="line">  [</span><br><span class="line">    CLUSTERED <span class="keyword">BY</span> (col_name, col_name, ...) </span><br><span class="line">   [SORTED <span class="keyword">BY</span> (col_name [<span class="keyword">ASC</span>|<span class="keyword">DESC</span>], ...)] <span class="keyword">INTO</span> num_buckets BUCKETS</span><br><span class="line">  ]  <span class="comment">--分桶表分桶规则</span></span><br><span class="line">  [SKEWED <span class="keyword">BY</span> (col_name, col_name, ...) <span class="keyword">ON</span> ((col_value, col_value, ...), (col_value, col_value, ...), ...)  </span><br><span class="line">   [<span class="keyword">STORED</span> <span class="keyword">AS</span> DIRECTORIES] </span><br><span class="line">  ]  <span class="comment">--指定倾斜列和值</span></span><br><span class="line">  [</span><br><span class="line">   [<span class="keyword">ROW</span> <span class="keyword">FORMAT</span> row_format]    </span><br><span class="line">   [<span class="keyword">STORED</span> <span class="keyword">AS</span> file_format]</span><br><span class="line">     | <span class="keyword">STORED</span> <span class="keyword">BY</span> <span class="string">'storage.handler.class.name'</span> [<span class="keyword">WITH</span> SERDEPROPERTIES (...)]  </span><br><span class="line">  ]  <span class="comment">-- 指定行分隔符、存储文件格式或采用自定义存储格式</span></span><br><span class="line">  [LOCATION hdfs_path]  <span class="comment">-- 指定表的存储位置</span></span><br><span class="line">  [TBLPROPERTIES (property_name=property_value, ...)]  <span class="comment">--指定表的属性</span></span><br><span class="line">  [<span class="keyword">AS</span> select_statement];   <span class="comment">--从查询结果创建表</span></span><br></pre></td></tr></table></figure>
<h3 id="2-2-内部表"><a href="#2-2-内部表" class="headerlink" title="2.2 内部表"></a>2.2 内部表</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> emp(</span><br><span class="line">  empno <span class="built_in">INT</span>,</span><br><span class="line">  ename <span class="keyword">STRING</span>,</span><br><span class="line">  job <span class="keyword">STRING</span>,</span><br><span class="line">  mgr <span class="built_in">INT</span>,</span><br><span class="line">  hiredate <span class="built_in">TIMESTAMP</span>,</span><br><span class="line">  sal <span class="built_in">DECIMAL</span>(<span class="number">7</span>,<span class="number">2</span>),</span><br><span class="line">  comm <span class="built_in">DECIMAL</span>(<span class="number">7</span>,<span class="number">2</span>),</span><br><span class="line">  deptno <span class="built_in">INT</span>)</span><br><span class="line">  <span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">"\t"</span>;</span><br></pre></td></tr></table></figure>
<h3 id="2-3-外部表"><a href="#2-3-外部表" class="headerlink" title="2.3 外部表"></a>2.3 外部表</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">EXTERNAL</span> <span class="keyword">TABLE</span> emp_external(</span><br><span class="line">  empno <span class="built_in">INT</span>,</span><br><span class="line">  ename <span class="keyword">STRING</span>,</span><br><span class="line">  job <span class="keyword">STRING</span>,</span><br><span class="line">  mgr <span class="built_in">INT</span>,</span><br><span class="line">  hiredate <span class="built_in">TIMESTAMP</span>,</span><br><span class="line">  sal <span class="built_in">DECIMAL</span>(<span class="number">7</span>,<span class="number">2</span>),</span><br><span class="line">  comm <span class="built_in">DECIMAL</span>(<span class="number">7</span>,<span class="number">2</span>),</span><br><span class="line">  deptno <span class="built_in">INT</span>)</span><br><span class="line">  <span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">"\t"</span></span><br><span class="line">  LOCATION <span class="string">'/hive/emp_external'</span>;</span><br></pre></td></tr></table></figure>
<p>使用 <code>desc format  emp_external</code> 命令可以查看表的详细信息如下：</p>
<div align="center"> <img width="700px" src="../pictures/hive-external-table.png"> </div>

<h3 id="2-4-分区表"><a href="#2-4-分区表" class="headerlink" title="2.4 分区表"></a>2.4 分区表</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">EXTERNAL</span> <span class="keyword">TABLE</span> emp_partition(</span><br><span class="line">  empno <span class="built_in">INT</span>,</span><br><span class="line">  ename <span class="keyword">STRING</span>,</span><br><span class="line">  job <span class="keyword">STRING</span>,</span><br><span class="line">  mgr <span class="built_in">INT</span>,</span><br><span class="line">  hiredate <span class="built_in">TIMESTAMP</span>,</span><br><span class="line">  sal <span class="built_in">DECIMAL</span>(<span class="number">7</span>,<span class="number">2</span>),</span><br><span class="line">  comm <span class="built_in">DECIMAL</span>(<span class="number">7</span>,<span class="number">2</span>)</span><br><span class="line">  )</span><br><span class="line">  PARTITIONED <span class="keyword">BY</span> (deptno <span class="built_in">INT</span>)   <span class="comment">-- 按照部门编号进行分区</span></span><br><span class="line">  <span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">"\t"</span></span><br><span class="line">  LOCATION <span class="string">'/hive/emp_partition'</span>;</span><br></pre></td></tr></table></figure>
<h3 id="2-5-分桶表"><a href="#2-5-分桶表" class="headerlink" title="2.5 分桶表"></a>2.5 分桶表</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">EXTERNAL</span> <span class="keyword">TABLE</span> emp_bucket(</span><br><span class="line">  empno <span class="built_in">INT</span>,</span><br><span class="line">  ename <span class="keyword">STRING</span>,</span><br><span class="line">  job <span class="keyword">STRING</span>,</span><br><span class="line">  mgr <span class="built_in">INT</span>,</span><br><span class="line">  hiredate <span class="built_in">TIMESTAMP</span>,</span><br><span class="line">  sal <span class="built_in">DECIMAL</span>(<span class="number">7</span>,<span class="number">2</span>),</span><br><span class="line">  comm <span class="built_in">DECIMAL</span>(<span class="number">7</span>,<span class="number">2</span>),</span><br><span class="line">  deptno <span class="built_in">INT</span>)</span><br><span class="line">  CLUSTERED <span class="keyword">BY</span>(empno) SORTED <span class="keyword">BY</span>(empno <span class="keyword">ASC</span>) <span class="keyword">INTO</span> <span class="number">4</span> BUCKETS  <span class="comment">--按照员工编号散列到四个 bucket 中</span></span><br><span class="line">  <span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">"\t"</span></span><br><span class="line">  LOCATION <span class="string">'/hive/emp_bucket'</span>;</span><br></pre></td></tr></table></figure>
<h3 id="2-6-倾斜表"><a href="#2-6-倾斜表" class="headerlink" title="2.6 倾斜表"></a>2.6 倾斜表</h3><p>通过指定一个或者多个列经常出现的值（严重偏斜），Hive 会自动将涉及到这些值的数据拆分为单独的文件。在查询时，如果涉及到倾斜值，它就直接从独立文件中获取数据，而不是扫描所有文件，这使得性能得到提升。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">EXTERNAL</span> <span class="keyword">TABLE</span> emp_skewed(</span><br><span class="line">  empno <span class="built_in">INT</span>,</span><br><span class="line">  ename <span class="keyword">STRING</span>,</span><br><span class="line">  job <span class="keyword">STRING</span>,</span><br><span class="line">  mgr <span class="built_in">INT</span>,</span><br><span class="line">  hiredate <span class="built_in">TIMESTAMP</span>,</span><br><span class="line">  sal <span class="built_in">DECIMAL</span>(<span class="number">7</span>,<span class="number">2</span>),</span><br><span class="line">  comm <span class="built_in">DECIMAL</span>(<span class="number">7</span>,<span class="number">2</span>)</span><br><span class="line">  )</span><br><span class="line">  SKEWED <span class="keyword">BY</span> (empno) <span class="keyword">ON</span> (<span class="number">66</span>,<span class="number">88</span>,<span class="number">100</span>)  <span class="comment">--指定 empno 的倾斜值 66,88,100</span></span><br><span class="line">  <span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">"\t"</span></span><br><span class="line">  LOCATION <span class="string">'/hive/emp_skewed'</span>;</span><br></pre></td></tr></table></figure>
<h3 id="2-7-临时表"><a href="#2-7-临时表" class="headerlink" title="2.7 临时表"></a>2.7 临时表</h3><p>临时表仅对当前 session 可见，临时表的数据将存储在用户的暂存目录中，并在会话结束后删除。如果临时表与永久表表名相同，则对该表名的任何引用都将解析为临时表，而不是永久表。临时表还具有以下两个限制：</p>
<ul>
<li>不支持分区列；</li>
<li>不支持创建索引。</li>
</ul>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TEMPORARY</span> <span class="keyword">TABLE</span> emp_temp(</span><br><span class="line">  empno <span class="built_in">INT</span>,</span><br><span class="line">  ename <span class="keyword">STRING</span>,</span><br><span class="line">  job <span class="keyword">STRING</span>,</span><br><span class="line">  mgr <span class="built_in">INT</span>,</span><br><span class="line">  hiredate <span class="built_in">TIMESTAMP</span>,</span><br><span class="line">  sal <span class="built_in">DECIMAL</span>(<span class="number">7</span>,<span class="number">2</span>),</span><br><span class="line">  comm <span class="built_in">DECIMAL</span>(<span class="number">7</span>,<span class="number">2</span>)</span><br><span class="line">  )</span><br><span class="line">  <span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">"\t"</span>;</span><br></pre></td></tr></table></figure>
<h3 id="2-8-CTAS创建表"><a href="#2-8-CTAS创建表" class="headerlink" title="2.8 CTAS创建表"></a>2.8 CTAS创建表</h3><p>支持从查询语句的结果创建表：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> emp_copy <span class="keyword">AS</span> <span class="keyword">SELECT</span> * <span class="keyword">FROM</span> emp <span class="keyword">WHERE</span> deptno=<span class="string">'20'</span>;</span><br></pre></td></tr></table></figure>
<h3 id="2-9-复制表结构"><a href="#2-9-复制表结构" class="headerlink" title="2.9 复制表结构"></a>2.9 复制表结构</h3><p>语法：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> [<span class="keyword">TEMPORARY</span>] [<span class="keyword">EXTERNAL</span>] <span class="keyword">TABLE</span> [<span class="keyword">IF</span> <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>] [db_name.]table_name  <span class="comment">--创建表表名</span></span><br><span class="line">   <span class="keyword">LIKE</span> existing_table_or_view_name  <span class="comment">--被复制表的表名</span></span><br><span class="line">   [LOCATION hdfs_path]; <span class="comment">--存储位置</span></span><br></pre></td></tr></table></figure>
<p>示例：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TEMPORARY</span> <span class="keyword">EXTERNAL</span> <span class="keyword">TABLE</span>  <span class="keyword">IF</span> <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>  emp_co  <span class="keyword">LIKE</span> emp</span><br></pre></td></tr></table></figure>
<h3 id="2-10-加载数据到表"><a href="#2-10-加载数据到表" class="headerlink" title="2.10 加载数据到表"></a>2.10 加载数据到表</h3><p>加载数据到表中属于 DML 操作，这里为了方便大家测试，先简单介绍一下加载本地数据到表中：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 加载数据到 emp 表中</span></span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">"/usr/file/emp.txt"</span> <span class="keyword">into</span> <span class="keyword">table</span> emp;</span><br></pre></td></tr></table></figure>
<p>其中 emp.txt 的内容如下，你可以直接复制使用，也可以到本仓库的<a href="https://github.com/heibaiying/BigData-Notes/tree/master/resources" target="_blank" rel="noopener">resources</a> 目录下载：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">7369	SMITH	CLERK	7902	1980-12-17 00:00:00	800.00		20</span><br><span class="line">7499	ALLEN	SALESMAN	7698	1981-02-20 00:00:00	1600.00	300.00	30</span><br><span class="line">7521	WARD	SALESMAN	7698	1981-02-22 00:00:00	1250.00	500.00	30</span><br><span class="line">7566	JONES	MANAGER	7839	1981-04-02 00:00:00	2975.00		20</span><br><span class="line">7654	MARTIN	SALESMAN	7698	1981-09-28 00:00:00	1250.00	1400.00	30</span><br><span class="line">7698	BLAKE	MANAGER	7839	1981-05-01 00:00:00	2850.00		30</span><br><span class="line">7782	CLARK	MANAGER	7839	1981-06-09 00:00:00	2450.00		10</span><br><span class="line">7788	SCOTT	ANALYST	7566	1987-04-19 00:00:00	1500.00		20</span><br><span class="line">7839	KING	PRESIDENT		1981-11-17 00:00:00	5000.00		10</span><br><span class="line">7844	TURNER	SALESMAN	7698	1981-09-08 00:00:00	1500.00	0.00	30</span><br><span class="line">7876	ADAMS	CLERK	7788	1987-05-23 00:00:00	1100.00		20</span><br><span class="line">7900	JAMES	CLERK	7698	1981-12-03 00:00:00	950.00		30</span><br><span class="line">7902	FORD	ANALYST	7566	1981-12-03 00:00:00	3000.00		20</span><br><span class="line">7934	MILLER	CLERK	7782	1982-01-23 00:00:00	1300.00		10</span><br></pre></td></tr></table></figure>
<p>加载后可查询表中数据：</p>
<div align="center"> <img width="700px" src="../pictures/hive-select-emp.png"> </div>



<h2 id="三、修改表"><a href="#三、修改表" class="headerlink" title="三、修改表"></a>三、修改表</h2><h3 id="3-1-重命名表"><a href="#3-1-重命名表" class="headerlink" title="3.1 重命名表"></a>3.1 重命名表</h3><p>语法：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> table_name <span class="keyword">RENAME</span> <span class="keyword">TO</span> new_table_name;</span><br></pre></td></tr></table></figure>
<p>示例：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> emp_temp <span class="keyword">RENAME</span> <span class="keyword">TO</span> new_emp; <span class="comment">--把 emp_temp 表重命名为 new_emp</span></span><br></pre></td></tr></table></figure>
<h3 id="3-2-修改列"><a href="#3-2-修改列" class="headerlink" title="3.2 修改列"></a>3.2 修改列</h3><p>语法：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> table_name [<span class="keyword">PARTITION</span> partition_spec] <span class="keyword">CHANGE</span> [<span class="keyword">COLUMN</span>] col_old_name col_new_name column_type</span><br><span class="line">  [<span class="keyword">COMMENT</span> col_comment] [<span class="keyword">FIRST</span>|<span class="keyword">AFTER</span> column_name] [<span class="keyword">CASCADE</span>|RESTRICT];</span><br></pre></td></tr></table></figure>
<p>示例：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 修改字段名和类型</span></span><br><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> emp_temp <span class="keyword">CHANGE</span> empno empno_new <span class="built_in">INT</span>;</span><br><span class="line"> </span><br><span class="line"><span class="comment">-- 修改字段 sal 的名称 并将其放置到 empno 字段后</span></span><br><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> emp_temp <span class="keyword">CHANGE</span> sal sal_new <span class="built_in">decimal</span>(<span class="number">7</span>,<span class="number">2</span>)  <span class="keyword">AFTER</span> ename;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 为字段增加注释</span></span><br><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> emp_temp <span class="keyword">CHANGE</span> mgr mgr_new <span class="built_in">INT</span> <span class="keyword">COMMENT</span> <span class="string">'this is column mgr'</span>;</span><br></pre></td></tr></table></figure>
<h3 id="3-3-新增列"><a href="#3-3-新增列" class="headerlink" title="3.3 新增列"></a>3.3 新增列</h3><p>示例：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> emp_temp <span class="keyword">ADD</span> <span class="keyword">COLUMNS</span> (address <span class="keyword">STRING</span> <span class="keyword">COMMENT</span> <span class="string">'home address'</span>);</span><br></pre></td></tr></table></figure>
<h2 id="四、清空表-删除表"><a href="#四、清空表-删除表" class="headerlink" title="四、清空表/删除表"></a>四、清空表/删除表</h2><h3 id="4-1-清空表"><a href="#4-1-清空表" class="headerlink" title="4.1 清空表"></a>4.1 清空表</h3><p>语法：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 清空整个表或表指定分区中的数据</span></span><br><span class="line"><span class="keyword">TRUNCATE</span> <span class="keyword">TABLE</span> table_name [<span class="keyword">PARTITION</span> (partition_column = partition_col_value,  ...)];</span><br></pre></td></tr></table></figure>
<ul>
<li>目前只有内部表才能执行 TRUNCATE 操作，外部表执行时会抛出异常 <code>Cannot truncate non-managed table XXXX</code>。</li>
</ul>
<p>示例：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">TRUNCATE</span> <span class="keyword">TABLE</span> emp_mgt_ptn <span class="keyword">PARTITION</span> (deptno=<span class="number">20</span>);</span><br></pre></td></tr></table></figure>
<h3 id="4-2-删除表"><a href="#4-2-删除表" class="headerlink" title="4.2 删除表"></a>4.2 删除表</h3><p>语法：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">DROP</span> <span class="keyword">TABLE</span> [<span class="keyword">IF</span> <span class="keyword">EXISTS</span>] table_name [<span class="keyword">PURGE</span>];</span><br></pre></td></tr></table></figure>
<ul>
<li>内部表：不仅会删除表的元数据，同时会删除 HDFS 上的数据；</li>
<li>外部表：只会删除表的元数据，不会删除 HDFS 上的数据；</li>
<li>删除视图引用的表时，不会给出警告（但视图已经无效了，必须由用户删除或重新创建）。</li>
</ul>
<h2 id="五、其他命令"><a href="#五、其他命令" class="headerlink" title="五、其他命令"></a>五、其他命令</h2><h3 id="5-1-Describe"><a href="#5-1-Describe" class="headerlink" title="5.1 Describe"></a>5.1 Describe</h3><p>查看数据库：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">DESCRIBE</span>|<span class="keyword">Desc</span> <span class="keyword">DATABASE</span> [<span class="keyword">EXTENDED</span>] db_name;  <span class="comment">--EXTENDED 是否显示额外属性</span></span><br></pre></td></tr></table></figure>
<p>查看表：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">DESCRIBE</span>|<span class="keyword">Desc</span> [<span class="keyword">EXTENDED</span>|FORMATTED] table_name <span class="comment">--FORMATTED 以友好的展现方式查看表详情</span></span><br></pre></td></tr></table></figure>
<h3 id="5-2-Show"><a href="#5-2-Show" class="headerlink" title="5.2 Show"></a>5.2 Show</h3><p><strong>1. 查看数据库列表</strong></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 语法</span></span><br><span class="line"><span class="keyword">SHOW</span> (<span class="keyword">DATABASES</span>|SCHEMAS) [<span class="keyword">LIKE</span> <span class="string">'identifier_with_wildcards'</span>];</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 示例：</span></span><br><span class="line"><span class="keyword">SHOW</span> <span class="keyword">DATABASES</span> <span class="keyword">like</span> <span class="string">'hive*'</span>;</span><br></pre></td></tr></table></figure>
<p>LIKE 子句允许使用正则表达式进行过滤，但是 SHOW 语句当中的 LIKE 子句只支持 <code>*</code>（通配符）和 <code>|</code>（条件或）两个符号。例如 <code>employees</code>，<code>emp *</code>，<code>emp * | * ees</code>，所有这些都将匹配名为 <code>employees</code> 的数据库。</p>
<p><strong>2. 查看表的列表</strong></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 语法</span></span><br><span class="line"><span class="keyword">SHOW</span> <span class="keyword">TABLES</span> [<span class="keyword">IN</span> database_name] [<span class="string">'identifier_with_wildcards'</span>];</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 示例</span></span><br><span class="line"><span class="keyword">SHOW</span> <span class="keyword">TABLES</span> <span class="keyword">IN</span> <span class="keyword">default</span>;</span><br></pre></td></tr></table></figure>
<p><strong>3. 查看视图列表</strong></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SHOW</span> VIEWS [<span class="keyword">IN</span>/<span class="keyword">FROM</span> database_name] [<span class="keyword">LIKE</span> <span class="string">'pattern_with_wildcards'</span>];   <span class="comment">--仅支持 Hive 2.2.0 +</span></span><br></pre></td></tr></table></figure>
<p><strong>4. 查看表的分区列表</strong></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SHOW</span> <span class="keyword">PARTITIONS</span> table_name;</span><br></pre></td></tr></table></figure>
<p><strong>5. 查看表/视图的创建语句</strong></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SHOW</span> <span class="keyword">CREATE</span> <span class="keyword">TABLE</span> ([db_name.]table_name|view_name);</span><br></pre></td></tr></table></figure>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL" target="_blank" rel="noopener">LanguageManual DDL</a></p>
]]></content>
      <categories>
        <category>notes</category>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>/2021/03/18/Hive%E5%88%86%E5%8C%BA%E8%A1%A8%E5%92%8C%E5%88%86%E6%A1%B6%E8%A1%A8/</url>
    <content><![CDATA[<h1 id="Hive分区表和分桶表"><a href="#Hive分区表和分桶表" class="headerlink" title="Hive分区表和分桶表"></a>Hive分区表和分桶表</h1><nav><br><a href="#一分区表">一、分区表</a><br><br><a href="#二分桶表">二、分桶表</a><br><br><a href="#三分区表和分桶表结合使用">三、分区表和分桶表结合使用</a><br><br></nav>


<h2 id="一、分区表"><a href="#一、分区表" class="headerlink" title="一、分区表"></a>一、分区表</h2><h3 id="1-1-概念"><a href="#1-1-概念" class="headerlink" title="1.1 概念"></a>1.1 概念</h3><p>Hive 中的表对应为 HDFS 上的指定目录，在查询数据时候，默认会对全表进行扫描，这样时间和性能的消耗都非常大。</p>
<p><strong>分区为 HDFS 上表目录的子目录</strong>，数据按照分区存储在子目录中。如果查询的 <code>where</code> 字句的中包含分区条件，则直接从该分区去查找，而不是扫描整个表目录，合理的分区设计可以极大提高查询速度和性能。</p>
<blockquote>
<p>这里说明一下分区表并 Hive 独有的概念，实际上这个概念非常常见。比如在我们常用的 Oracle 数据库中，当表中的数据量不断增大，查询数据的速度就会下降，这时也可以对表进行分区。表进行分区后，逻辑上表仍然是一张完整的表，只是将表中的数据存放到多个表空间（物理文件上），这样查询数据时，就不必要每次都扫描整张表，从而提升查询性能。</p>
</blockquote>
<h3 id="1-2-使用场景"><a href="#1-2-使用场景" class="headerlink" title="1.2  使用场景"></a>1.2  使用场景</h3><p>通常，在管理大规模数据集的时候都需要进行分区，比如将日志文件按天进行分区，从而保证数据细粒度的划分，使得查询性能得到提升。</p>
<h3 id="1-3-创建分区表"><a href="#1-3-创建分区表" class="headerlink" title="1.3 创建分区表"></a>1.3 创建分区表</h3><p>在 Hive 中可以使用 <code>PARTITIONED BY</code> 子句创建分区表。表可以包含一个或多个分区列，程序会为分区列中的每个不同值组合创建单独的数据目录。下面的我们创建一张雇员表作为测试：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">CREATE EXTERNAL TABLE emp_partition(</span><br><span class="line">   empno INT,</span><br><span class="line">   ename STRING,</span><br><span class="line">   job STRING,</span><br><span class="line">   mgr INT,</span><br><span class="line">   hiredate TIMESTAMP,</span><br><span class="line">   sal DECIMAL(7,2),</span><br><span class="line">   comm DECIMAL(7,2)</span><br><span class="line">   )</span><br><span class="line">   PARTITIONED BY (deptno INT)   -- 按照部门编号进行分区</span><br><span class="line">   ROW FORMAT DELIMITED FIELDS TERMINATED BY "\t"</span><br><span class="line">   LOCATION '/hive/emp_partition';</span><br></pre></td></tr></table></figure>
<h3 id="1-4-加载数据到分区表"><a href="#1-4-加载数据到分区表" class="headerlink" title="1.4 加载数据到分区表"></a>1.4 加载数据到分区表</h3><p>加载数据到分区表时候必须要指定数据所处的分区：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 加载部门编号为20的数据到表中</span></span><br><span class="line">LOAD DATA LOCAL INPATH "/usr/file/emp20.txt" OVERWRITE INTO TABLE emp_partition PARTITION (deptno=20)</span><br><span class="line"><span class="meta">#</span><span class="bash"> 加载部门编号为30的数据到表中</span></span><br><span class="line">LOAD DATA LOCAL INPATH "/usr/file/emp30.txt" OVERWRITE INTO TABLE emp_partition PARTITION (deptno=30)</span><br></pre></td></tr></table></figure>
<h3 id="1-5-查看分区目录"><a href="#1-5-查看分区目录" class="headerlink" title="1.5 查看分区目录"></a>1.5 查看分区目录</h3><p>这时候我们直接查看表目录，可以看到表目录下存在两个子目录，分别是 <code>deptno=20</code> 和 <code>deptno=30</code>,这就是分区目录，分区目录下才是我们加载的数据文件。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> hadoop fs -ls  hdfs://hadoop001:8020/hive/emp_partition/</span></span><br></pre></td></tr></table></figure>
<p>这时候当你的查询语句的 <code>where</code> 包含 <code>deptno=20</code>，则就去对应的分区目录下进行查找，而不用扫描全表。</p>
<div align="center"> <img src="../pictures/hive-hadoop-partitation.png"> </div>



<h2 id="二、分桶表"><a href="#二、分桶表" class="headerlink" title="二、分桶表"></a>二、分桶表</h2><h3 id="1-1-简介"><a href="#1-1-简介" class="headerlink" title="1.1 简介"></a>1.1 简介</h3><p>分区提供了一个隔离数据和优化查询的可行方案，但是并非所有的数据集都可以形成合理的分区，分区的数量也不是越多越好，过多的分区条件可能会导致很多分区上没有数据。同时 Hive 会限制动态分区可以创建的最大分区数，用来避免过多分区文件对文件系统产生负担。鉴于以上原因，Hive 还提供了一种更加细粒度的数据拆分方案：分桶表 (bucket Table)。</p>
<p>分桶表会将指定列的值进行哈希散列，并对 bucket（桶数量）取余，然后存储到对应的 bucket（桶）中。</p>
<h3 id="1-2-理解分桶表"><a href="#1-2-理解分桶表" class="headerlink" title="1.2 理解分桶表"></a>1.2 理解分桶表</h3><p>单从概念上理解分桶表可能会比较晦涩，其实和分区一样，分桶这个概念同样不是 Hive 独有的，对于 Java 开发人员而言，这可能是一个每天都会用到的概念，因为 Hive 中的分桶概念和 Java 数据结构中的 HashMap 的分桶概念是一致的。</p>
<p>当调用 HashMap 的 put() 方法存储数据时，程序会先对 key 值调用 hashCode() 方法计算出 hashcode，然后对数组长度取模计算出 index，最后将数据存储在数组 index 位置的链表上，链表达到一定阈值后会转换为红黑树 (JDK1.8+)。下图为 HashMap 的数据结构图：</p>
<div align="center"> <img width="600px" src="../pictures/HashMap-HashTable.png"> </div>

<blockquote>
<p>图片引用自：<a href="http://www.itcuties.com/java/hashmap-hashtable/" target="_blank" rel="noopener">HashMap vs. Hashtable</a></p>
</blockquote>
<h3 id="1-3-创建分桶表"><a href="#1-3-创建分桶表" class="headerlink" title="1.3 创建分桶表"></a>1.3 创建分桶表</h3><p>在 Hive 中，我们可以通过 <code>CLUSTERED BY</code> 指定分桶列，并通过 <code>SORTED BY</code> 指定桶中数据的排序参考列。下面为分桶表建表语句示例：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">EXTERNAL</span> <span class="keyword">TABLE</span> emp_bucket(</span><br><span class="line">  empno <span class="built_in">INT</span>,</span><br><span class="line">  ename <span class="keyword">STRING</span>,</span><br><span class="line">  job <span class="keyword">STRING</span>,</span><br><span class="line">  mgr <span class="built_in">INT</span>,</span><br><span class="line">  hiredate <span class="built_in">TIMESTAMP</span>,</span><br><span class="line">  sal <span class="built_in">DECIMAL</span>(<span class="number">7</span>,<span class="number">2</span>),</span><br><span class="line">  comm <span class="built_in">DECIMAL</span>(<span class="number">7</span>,<span class="number">2</span>),</span><br><span class="line">  deptno <span class="built_in">INT</span>)</span><br><span class="line">  CLUSTERED <span class="keyword">BY</span>(empno) SORTED <span class="keyword">BY</span>(empno <span class="keyword">ASC</span>) <span class="keyword">INTO</span> <span class="number">4</span> BUCKETS  <span class="comment">--按照员工编号散列到四个 bucket 中</span></span><br><span class="line">  <span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">"\t"</span></span><br><span class="line">  LOCATION <span class="string">'/hive/emp_bucket'</span>;</span><br></pre></td></tr></table></figure>
<h3 id="1-4-加载数据到分桶表"><a href="#1-4-加载数据到分桶表" class="headerlink" title="1.4 加载数据到分桶表"></a>1.4 加载数据到分桶表</h3><p>这里直接使用 <code>Load</code> 语句向分桶表加载数据，数据时可以加载成功的，但是数据并不会分桶。</p>
<p>这是由于分桶的实质是对指定字段做了 hash 散列然后存放到对应文件中，这意味着向分桶表中插入数据是必然要通过 MapReduce，且 Reducer 的数量必须等于分桶的数量。由于以上原因，分桶表的数据通常只能使用 CTAS(CREATE TABLE AS SELECT) 方式插入，因为 CTAS 操作会触发 MapReduce。加载数据步骤如下：</p>
<h4 id="1-设置强制分桶"><a href="#1-设置强制分桶" class="headerlink" title="1. 设置强制分桶"></a>1. 设置强制分桶</h4><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.enforce.bucketing = <span class="literal">true</span>; <span class="comment">--Hive 2.x 不需要这一步</span></span><br></pre></td></tr></table></figure>
<p>在 Hive 0.x and 1.x 版本，必须使用设置 <code>hive.enforce.bucketing = true</code>，表示强制分桶，允许程序根据表结构自动选择正确数量的 Reducer 和 cluster by  column 来进行分桶。</p>
<h4 id="2-CTAS导入数据"><a href="#2-CTAS导入数据" class="headerlink" title="2. CTAS导入数据"></a>2. CTAS导入数据</h4><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="keyword">TABLE</span> emp_bucket <span class="keyword">SELECT</span> *  <span class="keyword">FROM</span> emp;  <span class="comment">--这里的 emp 表就是一张普通的雇员表</span></span><br></pre></td></tr></table></figure>
<p>可以从执行日志看到 CTAS 触发 MapReduce 操作，且 Reducer 数量和建表时候指定 bucket 数量一致：</p>
<div align="center"> <img src="../pictures/hive-hadoop-mapreducer.png"> </div>

<h3 id="1-5-查看分桶文件"><a href="#1-5-查看分桶文件" class="headerlink" title="1.5 查看分桶文件"></a>1.5 查看分桶文件</h3><p>bucket(桶) 本质上就是表目录下的具体文件：</p>
<div align="center"> <img src="../pictures/hive-hadoop-bucket.png"> </div>



<h2 id="三、分区表和分桶表结合使用"><a href="#三、分区表和分桶表结合使用" class="headerlink" title="三、分区表和分桶表结合使用"></a>三、分区表和分桶表结合使用</h2><p>分区表和分桶表的本质都是将数据按照不同粒度进行拆分，从而使得在查询时候不必扫描全表，只需要扫描对应的分区或分桶，从而提升查询效率。两者可以结合起来使用，从而保证表数据在不同粒度上都能得到合理的拆分。下面是 Hive 官方给出的示例：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> page_view_bucketed(</span><br><span class="line">	viewTime <span class="built_in">INT</span>, </span><br><span class="line">    userid <span class="built_in">BIGINT</span>,</span><br><span class="line">    page_url <span class="keyword">STRING</span>, </span><br><span class="line">    referrer_url <span class="keyword">STRING</span>,</span><br><span class="line">    ip <span class="keyword">STRING</span> )</span><br><span class="line"> PARTITIONED <span class="keyword">BY</span>(dt <span class="keyword">STRING</span>)</span><br><span class="line"> CLUSTERED <span class="keyword">BY</span>(userid) SORTED <span class="keyword">BY</span>(viewTime) <span class="keyword">INTO</span> <span class="number">32</span> BUCKETS</span><br><span class="line"> <span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span></span><br><span class="line">   <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">'\001'</span></span><br><span class="line">   COLLECTION ITEMS <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">'\002'</span></span><br><span class="line">   <span class="keyword">MAP</span> <span class="keyword">KEYS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">'\003'</span></span><br><span class="line"> <span class="keyword">STORED</span> <span class="keyword">AS</span> SEQUENCEFILE;</span><br></pre></td></tr></table></figure>
<p>此时导入数据时需要指定分区：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">INSERT OVERWRITE page_view_bucketed</span><br><span class="line">PARTITION (dt='2009-02-25')</span><br><span class="line">SELECT * FROM page_view WHERE dt='2009-02-25';</span><br></pre></td></tr></table></figure>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol>
<li><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL+BucketedTables" target="_blank" rel="noopener">LanguageManual DDL BucketedTables</a></li>
</ol>
]]></content>
  </entry>
  <entry>
    <title>Hive 常用DML操作</title>
    <url>/2021/03/18/Hive%E5%B8%B8%E7%94%A8DML%E6%93%8D%E4%BD%9C/</url>
    <content><![CDATA[<h2 id="一、加载文件数据到表"><a href="#一、加载文件数据到表" class="headerlink" title="一、加载文件数据到表"></a>一、加载文件数据到表</h2><h3 id="1-1-语法"><a href="#1-1-语法" class="headerlink" title="1.1 语法"></a>1.1 语法</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">LOAD DATA [LOCAL] INPATH 'filepath' [OVERWRITE] </span><br><span class="line">INTO TABLE tablename [PARTITION (partcol1=val1, partcol2=val2 ...)]</span><br></pre></td></tr></table></figure>
<ul>
<li><code>LOCAL</code> 关键字代表从本地文件系统加载文件，省略则代表从 HDFS 上加载文件：</li>
</ul>
<ul>
<li><p>从本地文件系统加载文件时， <code>filepath</code> 可以是绝对路径也可以是相对路径 (建议使用绝对路径)；</p>
</li>
<li><p>从 HDFS 加载文件时候，<code>filepath</code> 为文件完整的 URL 地址：如 <code>hdfs://namenode:port/user/hive/project/ data1</code></p>
</li>
</ul>
<ul>
<li><p><code>filepath</code> 可以是文件路径 (在这种情况下 Hive 会将文件移动到表中)，也可以目录路径 (在这种情况下，Hive 会将该目录中的所有文件移动到表中)；</p>
</li>
<li><p>如果使用 OVERWRITE 关键字，则将删除目标表（或分区）的内容，使用新的数据填充；不使用此关键字，则数据以追加的方式加入；</p>
</li>
<li><p>加载的目标可以是表或分区。如果是分区表，则必须指定加载数据的分区；</p>
</li>
<li><p>加载文件的格式必须与建表时使用 <code>STORED AS</code> 指定的存储格式相同。</p>
</li>
</ul>
<blockquote>
<p>使用建议：</p>
<p><strong>不论是本地路径还是 URL 都建议使用完整的</strong>。虽然可以使用不完整的 URL 地址，此时 Hive 将使用 hadoop 中的 fs.default.name 配置来推断地址，但是为避免不必要的错误，建议使用完整的本地路径或 URL 地址；</p>
<p><strong>加载对象是分区表时建议显示指定分区</strong>。在 Hive 3.0 之后，内部将加载 (LOAD) 重写为 INSERT AS SELECT，此时如果不指定分区，INSERT AS SELECT 将假设最后一组列是分区列，如果该列不是表定义的分区，它将抛出错误。为避免错误，还是建议显示指定分区。</p>
</blockquote>
<h3 id="1-2-示例"><a href="#1-2-示例" class="headerlink" title="1.2 示例"></a>1.2 示例</h3><p>新建分区表：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> emp_ptn(</span><br><span class="line">  empno <span class="built_in">INT</span>,</span><br><span class="line">  ename <span class="keyword">STRING</span>,</span><br><span class="line">  job <span class="keyword">STRING</span>,</span><br><span class="line">  mgr <span class="built_in">INT</span>,</span><br><span class="line">  hiredate <span class="built_in">TIMESTAMP</span>,</span><br><span class="line">  sal <span class="built_in">DECIMAL</span>(<span class="number">7</span>,<span class="number">2</span>),</span><br><span class="line">  comm <span class="built_in">DECIMAL</span>(<span class="number">7</span>,<span class="number">2</span>)</span><br><span class="line">  )</span><br><span class="line">  PARTITIONED <span class="keyword">BY</span> (deptno <span class="built_in">INT</span>)   <span class="comment">-- 按照部门编号进行分区</span></span><br><span class="line">  <span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">"\t"</span>;</span><br></pre></td></tr></table></figure>
<p>从 HDFS 上加载数据到分区表：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">LOAD</span> <span class="keyword">DATA</span>  INPATH <span class="string">"hdfs://hadoop001:8020/mydir/emp.txt"</span> OVERWRITE <span class="keyword">INTO</span> <span class="keyword">TABLE</span> emp_ptn <span class="keyword">PARTITION</span> (deptno=<span class="number">20</span>);</span><br></pre></td></tr></table></figure>
<blockquote>
<p>emp.txt 文件可在本仓库的 resources 目录中下载</p>
</blockquote>
<p>加载后表中数据如下,分区列 deptno 全部赋值成 20：</p>
<div align="center"> <img src="../pictures/hive-emp-ptn.png"> </div>

<h2 id="二、查询结果插入到表"><a href="#二、查询结果插入到表" class="headerlink" title="二、查询结果插入到表"></a>二、查询结果插入到表</h2><h3 id="2-1-语法"><a href="#2-1-语法" class="headerlink" title="2.1 语法"></a>2.1 语法</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> tablename1 [<span class="keyword">PARTITION</span> (partcol1=val1, partcol2=val2 ...) [<span class="keyword">IF</span> <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>]]   </span><br><span class="line">select_statement1 <span class="keyword">FROM</span> from_statement;</span><br><span class="line"></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="keyword">TABLE</span> tablename1 [<span class="keyword">PARTITION</span> (partcol1=val1, partcol2=val2 ...)] </span><br><span class="line">select_statement1 <span class="keyword">FROM</span> from_statement;</span><br></pre></td></tr></table></figure>
<ul>
<li><p>Hive 0.13.0 开始，建表时可以通过使用 TBLPROPERTIES（“immutable”=“true”）来创建不可变表 (immutable table) ，如果不可以变表中存在数据，则 INSERT INTO 失败。（注：INSERT OVERWRITE 的语句不受 <code>immutable</code> 属性的影响）;</p>
</li>
<li><p>可以对表或分区执行插入操作。如果表已分区，则必须通过指定所有分区列的值来指定表的特定分区；</p>
</li>
<li><p>从 Hive 1.1.0 开始，TABLE 关键字是可选的；</p>
</li>
<li><p>从 Hive 1.2.0 开始 ，可以采用 INSERT INTO tablename(z，x，c1) 指明插入列；</p>
</li>
<li><p>可以将 SELECT 语句的查询结果插入多个表（或分区），称为多表插入。语法如下：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">FROM from_statement</span><br><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> tablename1 </span><br><span class="line">[<span class="keyword">PARTITION</span> (partcol1=val1, partcol2=val2 ...) [<span class="keyword">IF</span> <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>]] select_statement1</span><br><span class="line">[<span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> tablename2 [<span class="keyword">PARTITION</span> ... [<span class="keyword">IF</span> <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>]] select_statement2]</span><br><span class="line">[<span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="keyword">TABLE</span> tablename2 [<span class="keyword">PARTITION</span> ...] select_statement2] ...;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="2-2-动态插入分区"><a href="#2-2-动态插入分区" class="headerlink" title="2.2 动态插入分区"></a>2.2 动态插入分区</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> tablename <span class="keyword">PARTITION</span> (partcol1[=val1], partcol2[=val2] ...) </span><br><span class="line">select_statement <span class="keyword">FROM</span> from_statement;</span><br><span class="line"></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="keyword">TABLE</span> tablename <span class="keyword">PARTITION</span> (partcol1[=val1], partcol2[=val2] ...) </span><br><span class="line">select_statement <span class="keyword">FROM</span> from_statement;</span><br></pre></td></tr></table></figure>
<p>在向分区表插入数据时候，分区列名是必须的，但是列值是可选的。如果给出了分区列值，我们将其称为静态分区，否则它是动态分区。动态分区列必须在 SELECT 语句的列中最后指定，并且与它们在 PARTITION() 子句中出现的顺序相同。</p>
<p>注意：Hive 0.9.0 之前的版本动态分区插入是默认禁用的，而 0.9.0 之后的版本则默认启用。以下是动态分区的相关配置：</p>
<table>
<thead>
<tr>
<th>配置</th>
<th>默认值</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>hive.exec.dynamic.partition</code></td>
<td><code>true</code></td>
<td>需要设置为 true 才能启用动态分区插入</td>
</tr>
<tr>
<td><code>hive.exec.dynamic.partition.mode</code></td>
<td><code>strict</code></td>
<td>在严格模式 (strict) 下，用户必须至少指定一个静态分区，以防用户意外覆盖所有分区，在非严格模式下，允许所有分区都是动态的</td>
</tr>
<tr>
<td><code>hive.exec.max.dynamic.partitions.pernode</code></td>
<td>100</td>
<td>允许在每个 mapper/reducer 节点中创建的最大动态分区数</td>
</tr>
<tr>
<td><code>hive.exec.max.dynamic.partitions</code></td>
<td>1000</td>
<td>允许总共创建的最大动态分区数</td>
</tr>
<tr>
<td><code>hive.exec.max.created.files</code></td>
<td>100000</td>
<td>作业中所有 mapper/reducer 创建的 HDFS 文件的最大数量</td>
</tr>
<tr>
<td><code>hive.error.on.empty.partition</code></td>
<td><code>false</code></td>
<td>如果动态分区插入生成空结果，是否抛出异常</td>
</tr>
</tbody>
</table>
<h3 id="2-3-示例"><a href="#2-3-示例" class="headerlink" title="2.3 示例"></a>2.3 示例</h3><ol>
<li>新建 emp 表，作为查询对象表</li>
</ol>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> emp(</span><br><span class="line">    empno <span class="built_in">INT</span>,</span><br><span class="line">    ename <span class="keyword">STRING</span>,</span><br><span class="line">    job <span class="keyword">STRING</span>,</span><br><span class="line">    mgr <span class="built_in">INT</span>,</span><br><span class="line">    hiredate <span class="built_in">TIMESTAMP</span>,</span><br><span class="line">    sal <span class="built_in">DECIMAL</span>(<span class="number">7</span>,<span class="number">2</span>),</span><br><span class="line">    comm <span class="built_in">DECIMAL</span>(<span class="number">7</span>,<span class="number">2</span>),</span><br><span class="line">    deptno <span class="built_in">INT</span>)</span><br><span class="line">    <span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">"\t"</span>;</span><br><span class="line">    </span><br><span class="line"> <span class="comment">-- 加载数据到 emp 表中 这里直接从本地加载</span></span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">"/usr/file/emp.txt"</span> <span class="keyword">into</span> <span class="keyword">table</span> emp;</span><br></pre></td></tr></table></figure>
<p>​    完成后 <code>emp</code> 表中数据如下：</p>
<div align="center"> <img src="../pictures/hive-emp.png"> </div>

<ol start="2">
<li>为清晰演示，先清空 <code>emp_ptn</code> 表中加载的数据：</li>
</ol>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">TRUNCATE</span> <span class="keyword">TABLE</span> emp_ptn;</span><br></pre></td></tr></table></figure>
<ol start="3">
<li>静态分区演示：从 <code>emp</code> 表中查询部门编号为 20 的员工数据，并插入 <code>emp_ptn</code> 表中，语句如下：</li>
</ol>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> emp_ptn <span class="keyword">PARTITION</span> (deptno=<span class="number">20</span>) </span><br><span class="line"><span class="keyword">SELECT</span> empno,ename,job,mgr,hiredate,sal,comm <span class="keyword">FROM</span> emp <span class="keyword">WHERE</span> deptno=<span class="number">20</span>;</span><br></pre></td></tr></table></figure>
<p>​    完成后 <code>emp_ptn</code> 表中数据如下：</p>
<div align="center"> <img src="../pictures/hive-emp-deptno-20.png"> </div>

<ol start="4">
<li>接着演示动态分区：</li>
</ol>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 由于我们只有一个分区，且还是动态分区，所以需要关闭严格默认。因为在严格模式下，用户必须至少指定一个静态分区</span></span><br><span class="line"><span class="keyword">set</span> hive.exec.dynamic.partition.mode=nonstrict;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 动态分区   此时查询语句的最后一列为动态分区列，即 deptno</span></span><br><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> emp_ptn <span class="keyword">PARTITION</span> (deptno) </span><br><span class="line"><span class="keyword">SELECT</span> empno,ename,job,mgr,hiredate,sal,comm,deptno <span class="keyword">FROM</span> emp <span class="keyword">WHERE</span> deptno=<span class="number">30</span>;</span><br></pre></td></tr></table></figure>
<p>​    完成后 <code>emp_ptn</code> 表中数据如下：</p>
<div align="center"> <img src="../pictures/hive-emp-deptno-20-30.png"> </div>



<h2 id="三、使用SQL语句插入值"><a href="#三、使用SQL语句插入值" class="headerlink" title="三、使用SQL语句插入值"></a>三、使用SQL语句插入值</h2><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="keyword">TABLE</span> tablename [<span class="keyword">PARTITION</span> (partcol1[=val1], partcol2[=val2] ...)] </span><br><span class="line"><span class="keyword">VALUES</span> ( <span class="keyword">value</span> [, <span class="keyword">value</span> ...] )</span><br></pre></td></tr></table></figure>
<ul>
<li>使用时必须为表中的每个列都提供值。不支持只向部分列插入值（可以为缺省值的列提供空值来消除这个弊端）；</li>
<li>如果目标表表支持 ACID 及其事务管理器，则插入后自动提交；</li>
<li>不支持支持复杂类型 (array, map, struct, union) 的插入。</li>
</ul>
<h2 id="四、更新和删除数据"><a href="#四、更新和删除数据" class="headerlink" title="四、更新和删除数据"></a>四、更新和删除数据</h2><h3 id="4-1-语法"><a href="#4-1-语法" class="headerlink" title="4.1 语法"></a>4.1 语法</h3><p>更新和删除的语法比较简单，和关系型数据库一致。需要注意的是这两个操作都只能在支持 ACID 的表，也就是事务表上才能执行。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 更新</span></span><br><span class="line"><span class="keyword">UPDATE</span> tablename <span class="keyword">SET</span> <span class="keyword">column</span> = <span class="keyword">value</span> [, <span class="keyword">column</span> = <span class="keyword">value</span> ...] [<span class="keyword">WHERE</span> expression]</span><br><span class="line"></span><br><span class="line"><span class="comment">--删除</span></span><br><span class="line"><span class="keyword">DELETE</span> <span class="keyword">FROM</span> tablename [<span class="keyword">WHERE</span> expression]</span><br></pre></td></tr></table></figure>
<h3 id="4-2-示例"><a href="#4-2-示例" class="headerlink" title="4.2 示例"></a>4.2 示例</h3><p><strong>1. 修改配置</strong></p>
<p>首先需要更改 <code>hive-site.xml</code>，添加如下配置，开启事务支持，配置完成后需要重启 Hive 服务。</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.support.concurrency<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.enforce.bucketing<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.exec.dynamic.partition.mode<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>nonstrict<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.txn.manager<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.hive.ql.lockmgr.DbTxnManager<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.compactor.initiator.on<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.in.test<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p><strong>2. 创建测试表</strong></p>
<p>创建用于测试的事务表，建表时候指定属性 <code>transactional = true</code> 则代表该表是事务表。需要注意的是，按照<a href="https://cwiki.apache.org/confluence/display/Hive/Hive+Transactions" target="_blank" rel="noopener">官方文档</a> 的说明，目前 Hive 中的事务表有以下限制：</p>
<ul>
<li>必须是 buckets Table;</li>
<li>仅支持 ORC 文件格式；</li>
<li>不支持 LOAD DATA …语句。</li>
</ul>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> emp_ts(  </span><br><span class="line">  empno <span class="built_in">int</span>,  </span><br><span class="line">  ename <span class="keyword">String</span></span><br><span class="line">)</span><br><span class="line">CLUSTERED <span class="keyword">BY</span> (empno) <span class="keyword">INTO</span> <span class="number">2</span> BUCKETS <span class="keyword">STORED</span> <span class="keyword">AS</span> ORC</span><br><span class="line">TBLPROPERTIES (<span class="string">"transactional"</span>=<span class="string">"true"</span>);</span><br></pre></td></tr></table></figure>
<p><strong>3. 插入测试数据</strong></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="keyword">TABLE</span> emp_ts  <span class="keyword">VALUES</span> (<span class="number">1</span>,<span class="string">"ming"</span>),(<span class="number">2</span>,<span class="string">"hong"</span>);</span><br></pre></td></tr></table></figure>
<p>插入数据依靠的是 MapReduce 作业，执行成功后数据如下：</p>
<div align="center"> <img src="../pictures/hive-emp-ts.png"> </div>

<p><strong>4. 测试更新和删除</strong></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">--更新数据</span></span><br><span class="line"><span class="keyword">UPDATE</span> emp_ts <span class="keyword">SET</span> ename = <span class="string">"lan"</span>  <span class="keyword">WHERE</span>  empno=<span class="number">1</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">--删除数据</span></span><br><span class="line"><span class="keyword">DELETE</span> <span class="keyword">FROM</span> emp_ts <span class="keyword">WHERE</span> empno=<span class="number">2</span>;</span><br></pre></td></tr></table></figure>
<p>更新和删除数据依靠的也是 MapReduce 作业，执行成功后数据如下：</p>
<div align="center"> <img src="../pictures/hive-emp-ts-2.png"> </div>


<h2 id="五、查询结果写出到文件系统"><a href="#五、查询结果写出到文件系统" class="headerlink" title="五、查询结果写出到文件系统"></a>五、查询结果写出到文件系统</h2><h3 id="5-1-语法"><a href="#5-1-语法" class="headerlink" title="5.1 语法"></a>5.1 语法</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> OVERWRITE [<span class="keyword">LOCAL</span>] <span class="keyword">DIRECTORY</span> directory1</span><br><span class="line">  [<span class="keyword">ROW</span> <span class="keyword">FORMAT</span> row_format] [<span class="keyword">STORED</span> <span class="keyword">AS</span> file_format] </span><br><span class="line">  <span class="keyword">SELECT</span> ... <span class="keyword">FROM</span> ...</span><br></pre></td></tr></table></figure>
<ul>
<li><p>OVERWRITE 关键字表示输出文件存在时，先删除后再重新写入；</p>
</li>
<li><p>和 Load 语句一样，建议无论是本地路径还是 URL 地址都使用完整的；</p>
</li>
<li><p>写入文件系统的数据被序列化为文本，其中列默认由^A 分隔，行由换行符分隔。如果列不是基本类型，则将其序列化为 JSON 格式。其中行分隔符不允许自定义，但列分隔符可以自定义，如下：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 定义列分隔符为'\t' </span></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">local</span> <span class="keyword">directory</span> <span class="string">'./test-04'</span> </span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> </span><br><span class="line"><span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">'\t'</span></span><br><span class="line">COLLECTION ITEMS <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">','</span></span><br><span class="line"><span class="keyword">MAP</span> <span class="keyword">KEYS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">':'</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> src;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="5-2-示例"><a href="#5-2-示例" class="headerlink" title="5.2 示例"></a>5.2 示例</h3><p>这里我们将上面创建的 <code>emp_ptn</code> 表导出到本地文件系统，语句如下：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">LOCAL</span> <span class="keyword">DIRECTORY</span> <span class="string">'/usr/file/ouput'</span></span><br><span class="line"><span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span></span><br><span class="line"><span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">'\t'</span></span><br><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> emp_ptn;</span><br></pre></td></tr></table></figure>
<p>导出结果如下：</p>
<div align="center"> <img src="../pictures/hive-ouput.png"> </div>





<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol>
<li><a href="https://cwiki.apache.org/confluence/display/Hive/Hive+Transactions" target="_blank" rel="noopener">Hive Transactions</a></li>
<li><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DML" target="_blank" rel="noopener">Hive Data Manipulation Language</a></li>
</ol>
]]></content>
      <categories>
        <category>notes</category>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title>Hive数据查询详解</title>
    <url>/2021/03/18/Hive%E6%95%B0%E6%8D%AE%E6%9F%A5%E8%AF%A2%E8%AF%A6%E8%A7%A3/</url>
    <content><![CDATA[<h2 id="一、数据准备"><a href="#一、数据准备" class="headerlink" title="一、数据准备"></a>一、数据准备</h2><p>为了演示查询操作，这里需要预先创建三张表，并加载测试数据。</p>
<blockquote>
<p>数据文件 emp.txt 和 dept.txt 可以从本仓库的<a href="https://github.com/heibaiying/BigData-Notes/tree/master/resources" target="_blank" rel="noopener">resources</a> 目录下载。</p>
</blockquote>
<h3 id="1-1-员工表"><a href="#1-1-员工表" class="headerlink" title="1.1 员工表"></a>1.1 员工表</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"> <span class="comment">-- 建表语句</span></span><br><span class="line"> <span class="keyword">CREATE</span> <span class="keyword">TABLE</span> emp(</span><br><span class="line">     empno <span class="built_in">INT</span>,     <span class="comment">-- 员工表编号</span></span><br><span class="line">     ename <span class="keyword">STRING</span>,  <span class="comment">-- 员工姓名</span></span><br><span class="line">     job <span class="keyword">STRING</span>,    <span class="comment">-- 职位类型</span></span><br><span class="line">     mgr <span class="built_in">INT</span>,   </span><br><span class="line">     hiredate <span class="built_in">TIMESTAMP</span>,  <span class="comment">--雇佣日期</span></span><br><span class="line">     sal <span class="built_in">DECIMAL</span>(<span class="number">7</span>,<span class="number">2</span>),  <span class="comment">--工资</span></span><br><span class="line">     comm <span class="built_in">DECIMAL</span>(<span class="number">7</span>,<span class="number">2</span>),</span><br><span class="line">     deptno <span class="built_in">INT</span>)   <span class="comment">--部门编号</span></span><br><span class="line">    <span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">"\t"</span>;</span><br><span class="line"></span><br><span class="line">  <span class="comment">--加载数据</span></span><br><span class="line"><span class="keyword">LOAD</span> <span class="keyword">DATA</span> <span class="keyword">LOCAL</span> INPATH <span class="string">"/usr/file/emp.txt"</span> OVERWRITE <span class="keyword">INTO</span> <span class="keyword">TABLE</span> emp;</span><br></pre></td></tr></table></figure>
<h3 id="1-2-部门表"><a href="#1-2-部门表" class="headerlink" title="1.2 部门表"></a>1.2 部门表</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 建表语句</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> dept(</span><br><span class="line">    deptno <span class="built_in">INT</span>,   <span class="comment">--部门编号</span></span><br><span class="line">    dname <span class="keyword">STRING</span>,  <span class="comment">--部门名称</span></span><br><span class="line">    loc <span class="keyword">STRING</span>    <span class="comment">--部门所在的城市</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">"\t"</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">--加载数据</span></span><br><span class="line"><span class="keyword">LOAD</span> <span class="keyword">DATA</span> <span class="keyword">LOCAL</span> INPATH <span class="string">"/usr/file/dept.txt"</span> OVERWRITE <span class="keyword">INTO</span> <span class="keyword">TABLE</span> dept;</span><br></pre></td></tr></table></figure>
<h3 id="1-3-分区表"><a href="#1-3-分区表" class="headerlink" title="1.3 分区表"></a>1.3 分区表</h3><p>这里需要额外创建一张分区表，主要是为了演示分区查询：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">EXTERNAL</span> <span class="keyword">TABLE</span> emp_ptn(</span><br><span class="line">      empno <span class="built_in">INT</span>,</span><br><span class="line">      ename <span class="keyword">STRING</span>,</span><br><span class="line">      job <span class="keyword">STRING</span>,</span><br><span class="line">      mgr <span class="built_in">INT</span>,</span><br><span class="line">      hiredate <span class="built_in">TIMESTAMP</span>,</span><br><span class="line">      sal <span class="built_in">DECIMAL</span>(<span class="number">7</span>,<span class="number">2</span>),</span><br><span class="line">      comm <span class="built_in">DECIMAL</span>(<span class="number">7</span>,<span class="number">2</span>)</span><br><span class="line">  )</span><br><span class="line"> PARTITIONED <span class="keyword">BY</span> (deptno <span class="built_in">INT</span>)   <span class="comment">-- 按照部门编号进行分区</span></span><br><span class="line"> <span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">"\t"</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">--加载数据</span></span><br><span class="line"><span class="keyword">LOAD</span> <span class="keyword">DATA</span> <span class="keyword">LOCAL</span> INPATH <span class="string">"/usr/file/emp.txt"</span> OVERWRITE <span class="keyword">INTO</span> <span class="keyword">TABLE</span> emp_ptn <span class="keyword">PARTITION</span> (deptno=<span class="number">20</span>)</span><br><span class="line"><span class="keyword">LOAD</span> <span class="keyword">DATA</span> <span class="keyword">LOCAL</span> INPATH <span class="string">"/usr/file/emp.txt"</span> OVERWRITE <span class="keyword">INTO</span> <span class="keyword">TABLE</span> emp_ptn <span class="keyword">PARTITION</span> (deptno=<span class="number">30</span>)</span><br><span class="line"><span class="keyword">LOAD</span> <span class="keyword">DATA</span> <span class="keyword">LOCAL</span> INPATH <span class="string">"/usr/file/emp.txt"</span> OVERWRITE <span class="keyword">INTO</span> <span class="keyword">TABLE</span> emp_ptn <span class="keyword">PARTITION</span> (deptno=<span class="number">40</span>)</span><br><span class="line"><span class="keyword">LOAD</span> <span class="keyword">DATA</span> <span class="keyword">LOCAL</span> INPATH <span class="string">"/usr/file/emp.txt"</span> OVERWRITE <span class="keyword">INTO</span> <span class="keyword">TABLE</span> emp_ptn <span class="keyword">PARTITION</span> (deptno=<span class="number">50</span>)</span><br></pre></td></tr></table></figure>
<h2 id="二、单表查询"><a href="#二、单表查询" class="headerlink" title="二、单表查询"></a>二、单表查询</h2><h3 id="2-1-SELECT"><a href="#2-1-SELECT" class="headerlink" title="2.1 SELECT"></a>2.1 SELECT</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 查询表中全部数据</span></span><br><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> emp;</span><br></pre></td></tr></table></figure>
<h3 id="2-2-WHERE"><a href="#2-2-WHERE" class="headerlink" title="2.2 WHERE"></a>2.2 WHERE</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 查询 10 号部门中员工编号大于 7782 的员工信息 </span></span><br><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> emp <span class="keyword">WHERE</span> empno &gt; <span class="number">7782</span> <span class="keyword">AND</span> deptno = <span class="number">10</span>;</span><br></pre></td></tr></table></figure>
<h3 id="2-3-DISTINCT"><a href="#2-3-DISTINCT" class="headerlink" title="2.3  DISTINCT"></a>2.3  DISTINCT</h3><p>Hive 支持使用 DISTINCT 关键字去重。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 查询所有工作类型</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="keyword">DISTINCT</span> job <span class="keyword">FROM</span> emp;</span><br></pre></td></tr></table></figure>
<h3 id="2-4-分区查询"><a href="#2-4-分区查询" class="headerlink" title="2.4 分区查询"></a>2.4 分区查询</h3><p>分区查询 (Partition Based Queries)，可以指定某个分区或者分区范围。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 查询分区表中部门编号在[20,40]之间的员工</span></span><br><span class="line"><span class="keyword">SELECT</span> emp_ptn.* <span class="keyword">FROM</span> emp_ptn</span><br><span class="line"><span class="keyword">WHERE</span> emp_ptn.deptno &gt;= <span class="number">20</span> <span class="keyword">AND</span> emp_ptn.deptno &lt;= <span class="number">40</span>;</span><br></pre></td></tr></table></figure>
<h3 id="2-5-LIMIT"><a href="#2-5-LIMIT" class="headerlink" title="2.5 LIMIT"></a>2.5 LIMIT</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 查询薪资最高的 5 名员工</span></span><br><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> emp <span class="keyword">ORDER</span> <span class="keyword">BY</span> sal <span class="keyword">DESC</span> <span class="keyword">LIMIT</span> <span class="number">5</span>;</span><br></pre></td></tr></table></figure>
<h3 id="2-6-GROUP-BY"><a href="#2-6-GROUP-BY" class="headerlink" title="2.6 GROUP BY"></a>2.6 GROUP BY</h3><p>Hive 支持使用 GROUP BY 进行分组聚合操作。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.map.aggr=<span class="literal">true</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 查询各个部门薪酬综合</span></span><br><span class="line"><span class="keyword">SELECT</span> deptno,<span class="keyword">SUM</span>(sal) <span class="keyword">FROM</span> emp <span class="keyword">GROUP</span> <span class="keyword">BY</span> deptno;</span><br></pre></td></tr></table></figure>
<p><code>hive.map.aggr</code> 控制程序如何进行聚合。默认值为 false。如果设置为 true，Hive 会在 map 阶段就执行一次聚合。这可以提高聚合效率，但需要消耗更多内存。</p>
<h3 id="2-7-ORDER-AND-SORT"><a href="#2-7-ORDER-AND-SORT" class="headerlink" title="2.7 ORDER AND SORT"></a>2.7 ORDER AND SORT</h3><p>可以使用 ORDER BY 或者 Sort BY 对查询结果进行排序，排序字段可以是整型也可以是字符串：如果是整型，则按照大小排序；如果是字符串，则按照字典序排序。ORDER BY 和 SORT BY 的区别如下：</p>
<ul>
<li>使用 ORDER BY 时会有一个 Reducer 对全部查询结果进行排序，可以保证数据的全局有序性；</li>
<li>使用 SORT BY 时只会在每个 Reducer 中进行排序，这可以保证每个 Reducer 的输出数据是有序的，但不能保证全局有序。</li>
</ul>
<p>由于 ORDER BY 的时间可能很长，如果你设置了严格模式 (hive.mapred.mode = strict)，则其后面必须再跟一个 <code>limit</code> 子句。</p>
<blockquote>
<p>注 ：hive.mapred.mode 默认值是 nonstrict ，也就是非严格模式。</p>
</blockquote>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 查询员工工资，结果按照部门升序，按照工资降序排列</span></span><br><span class="line"><span class="keyword">SELECT</span> empno, deptno, sal <span class="keyword">FROM</span> emp <span class="keyword">ORDER</span> <span class="keyword">BY</span> deptno <span class="keyword">ASC</span>, sal <span class="keyword">DESC</span>;</span><br></pre></td></tr></table></figure>
<h3 id="2-8-HAVING"><a href="#2-8-HAVING" class="headerlink" title="2.8 HAVING"></a>2.8 HAVING</h3><p>可以使用 HAVING 对分组数据进行过滤。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 查询工资总和大于 9000 的所有部门</span></span><br><span class="line"><span class="keyword">SELECT</span> deptno,<span class="keyword">SUM</span>(sal) <span class="keyword">FROM</span> emp <span class="keyword">GROUP</span> <span class="keyword">BY</span> deptno <span class="keyword">HAVING</span> <span class="keyword">SUM</span>(sal)&gt;<span class="number">9000</span>;</span><br></pre></td></tr></table></figure>
<h3 id="2-9-DISTRIBUTE-BY"><a href="#2-9-DISTRIBUTE-BY" class="headerlink" title="2.9 DISTRIBUTE BY"></a>2.9 DISTRIBUTE BY</h3><p>默认情况下，MapReduce 程序会对 Map 输出结果的 Key 值进行散列，并均匀分发到所有 Reducer 上。如果想要把具有相同 Key 值的数据分发到同一个 Reducer 进行处理，这就需要使用 DISTRIBUTE BY 字句。</p>
<p>需要注意的是，DISTRIBUTE BY 虽然能保证具有相同 Key 值的数据分发到同一个 Reducer，但是不能保证数据在 Reducer 上是有序的。情况如下：</p>
<p>把以下 5 个数据发送到两个 Reducer 上进行处理：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">k1</span><br><span class="line">k2</span><br><span class="line">k4</span><br><span class="line">k3</span><br><span class="line">k1</span><br></pre></td></tr></table></figure>
<p>Reducer1 得到如下乱序数据：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">k1</span><br><span class="line">k2</span><br><span class="line">k1</span><br></pre></td></tr></table></figure>
<p>Reducer2 得到数据如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">k4</span><br><span class="line">k3</span><br></pre></td></tr></table></figure>
<p>如果想让 Reducer 上的数据时有序的，可以结合 <code>SORT BY</code> 使用 (示例如下)，或者使用下面我们将要介绍的 CLUSTER BY。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 将数据按照部门分发到对应的 Reducer 上处理</span></span><br><span class="line"><span class="keyword">SELECT</span> empno, deptno, sal <span class="keyword">FROM</span> emp <span class="keyword">DISTRIBUTE</span> <span class="keyword">BY</span> deptno <span class="keyword">SORT</span> <span class="keyword">BY</span> deptno <span class="keyword">ASC</span>;</span><br></pre></td></tr></table></figure>
<h3 id="2-10-CLUSTER-BY"><a href="#2-10-CLUSTER-BY" class="headerlink" title="2.10 CLUSTER BY"></a>2.10 CLUSTER BY</h3><p>如果 <code>SORT BY</code> 和 <code>DISTRIBUTE BY</code> 指定的是相同字段，且 SORT BY 排序规则是 ASC，此时可以使用 <code>CLUSTER BY</code> 进行替换，同时 <code>CLUSTER BY</code> 可以保证数据在全局是有序的。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> empno, deptno, sal <span class="keyword">FROM</span> emp CLUSTER  <span class="keyword">BY</span> deptno ;</span><br></pre></td></tr></table></figure>
<h2 id="三、多表联结查询"><a href="#三、多表联结查询" class="headerlink" title="三、多表联结查询"></a>三、多表联结查询</h2><p>Hive 支持内连接，外连接，左外连接，右外连接，笛卡尔连接，这和传统数据库中的概念是一致的，可以参见下图。</p>
<p>需要特别强调：JOIN 语句的关联条件必须用 ON 指定，不能用 WHERE 指定，否则就会先做笛卡尔积，再过滤，这会导致你得不到预期的结果 (下面的演示会有说明)。</p>
<div align="center"> <img width="600px" src="../pictures/sql-join.jpg"> </div>

<h3 id="3-1-INNER-JOIN"><a href="#3-1-INNER-JOIN" class="headerlink" title="3.1 INNER JOIN"></a>3.1 INNER JOIN</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 查询员工编号为 7369 的员工的详细信息</span></span><br><span class="line"><span class="keyword">SELECT</span> e.*,d.* <span class="keyword">FROM</span> </span><br><span class="line">emp e <span class="keyword">JOIN</span> dept d</span><br><span class="line"><span class="keyword">ON</span> e.deptno = d.deptno </span><br><span class="line"><span class="keyword">WHERE</span> empno=<span class="number">7369</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">--如果是三表或者更多表连接，语法如下</span></span><br><span class="line"><span class="keyword">SELECT</span> a.val, b.val, c.val <span class="keyword">FROM</span> a <span class="keyword">JOIN</span> b <span class="keyword">ON</span> (a.key = b.key1) <span class="keyword">JOIN</span> c <span class="keyword">ON</span> (c.key = b.key1)</span><br></pre></td></tr></table></figure>
<h3 id="3-2-LEFT-OUTER-JOIN"><a href="#3-2-LEFT-OUTER-JOIN" class="headerlink" title="3.2 LEFT OUTER  JOIN"></a>3.2 LEFT OUTER  JOIN</h3><p>LEFT OUTER  JOIN 和 LEFT  JOIN 是等价的。 </p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 左连接</span></span><br><span class="line"><span class="keyword">SELECT</span> e.*,d.*</span><br><span class="line"><span class="keyword">FROM</span> emp e <span class="keyword">LEFT</span> <span class="keyword">OUTER</span>  <span class="keyword">JOIN</span>  dept d</span><br><span class="line"><span class="keyword">ON</span> e.deptno = d.deptno;</span><br></pre></td></tr></table></figure>
<h3 id="3-3-RIGHT-OUTER-JOIN"><a href="#3-3-RIGHT-OUTER-JOIN" class="headerlink" title="3.3 RIGHT OUTER  JOIN"></a>3.3 RIGHT OUTER  JOIN</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">--右连接</span></span><br><span class="line"><span class="keyword">SELECT</span> e.*,d.*</span><br><span class="line"><span class="keyword">FROM</span> emp e <span class="keyword">RIGHT</span> <span class="keyword">OUTER</span> <span class="keyword">JOIN</span>  dept d</span><br><span class="line"><span class="keyword">ON</span> e.deptno = d.deptno;</span><br></pre></td></tr></table></figure>
<p>执行右连接后，由于 40 号部门下没有任何员工，所以此时员工信息为 NULL。这个查询可以很好的复述上面提到的——JOIN 语句的关联条件必须用 ON 指定，不能用 WHERE 指定。你可以把 ON 改成 WHERE，你会发现无论如何都查不出 40 号部门这条数据，因为笛卡尔运算不会有 (NULL, 40) 这种情况。</p>
<p><div align="center"> <img width="700px" src="../pictures/hive-right-join.png"> </div></p>
<h3 id="3-4-FULL-OUTER-JOIN"><a href="#3-4-FULL-OUTER-JOIN" class="headerlink" title="3.4 FULL OUTER  JOIN"></a>3.4 FULL OUTER  JOIN</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> e.*,d.*</span><br><span class="line"><span class="keyword">FROM</span> emp e <span class="keyword">FULL</span> <span class="keyword">OUTER</span> <span class="keyword">JOIN</span>  dept d</span><br><span class="line"><span class="keyword">ON</span> e.deptno = d.deptno;</span><br></pre></td></tr></table></figure>
<h3 id="3-5-LEFT-SEMI-JOIN"><a href="#3-5-LEFT-SEMI-JOIN" class="headerlink" title="3.5 LEFT SEMI JOIN"></a>3.5 LEFT SEMI JOIN</h3><p>LEFT SEMI JOIN （左半连接）是 IN/EXISTS 子查询的一种更高效的实现。</p>
<ul>
<li>JOIN 子句中右边的表只能在 ON 子句中设置过滤条件;</li>
<li>查询结果只包含左边表的数据，所以只能 SELECT 左表中的列。</li>
</ul>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 查询在纽约办公的所有员工信息</span></span><br><span class="line"><span class="keyword">SELECT</span> emp.*</span><br><span class="line"><span class="keyword">FROM</span> emp <span class="keyword">LEFT</span> <span class="keyword">SEMI</span> <span class="keyword">JOIN</span> dept </span><br><span class="line"><span class="keyword">ON</span> emp.deptno = dept.deptno <span class="keyword">AND</span> dept.loc=<span class="string">"NEW YORK"</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">--上面的语句就等价于</span></span><br><span class="line"><span class="keyword">SELECT</span> emp.* <span class="keyword">FROM</span> emp</span><br><span class="line"><span class="keyword">WHERE</span> emp.deptno <span class="keyword">IN</span> (<span class="keyword">SELECT</span> deptno <span class="keyword">FROM</span> dept <span class="keyword">WHERE</span> loc=<span class="string">"NEW YORK"</span>);</span><br></pre></td></tr></table></figure>
<h3 id="3-6-JOIN"><a href="#3-6-JOIN" class="headerlink" title="3.6 JOIN"></a>3.6 JOIN</h3><p>笛卡尔积连接，这个连接日常的开发中可能很少遇到，且性能消耗比较大，基于这个原因，如果在严格模式下 (hive.mapred.mode = strict)，Hive 会阻止用户执行此操作。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> emp <span class="keyword">JOIN</span> dept;</span><br></pre></td></tr></table></figure>
<h2 id="四、JOIN优化"><a href="#四、JOIN优化" class="headerlink" title="四、JOIN优化"></a>四、JOIN优化</h2><h3 id="4-1-STREAMTABLE"><a href="#4-1-STREAMTABLE" class="headerlink" title="4.1 STREAMTABLE"></a>4.1 STREAMTABLE</h3><p>在多表进行联结的时候，如果每个 ON 字句都使用到共同的列（如下面的 <code>b.key</code>），此时 Hive 会进行优化，将多表 JOIN 在同一个 map / reduce 作业上进行。同时假定查询的最后一个表（如下面的 c 表）是最大的一个表，在对每行记录进行 JOIN 操作时，它将尝试将其他的表缓存起来，然后扫描最后那个表进行计算。因此用户需要保证查询的表的大小从左到右是依次增加的。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">`<span class="keyword">SELECT</span> a.val, b.val, c.val <span class="keyword">FROM</span> a <span class="keyword">JOIN</span> b <span class="keyword">ON</span> (a.key = b.key) <span class="keyword">JOIN</span> c <span class="keyword">ON</span> (c.key = b.key)<span class="string">`</span></span><br></pre></td></tr></table></figure>
<p>然后，用户并非需要总是把最大的表放在查询语句的最后面，Hive 提供了 <code>/*+ STREAMTABLE() */</code> 标志，用于标识最大的表，示例如下：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="comment">/*+ STREAMTABLE(d) */</span>  e.*,d.* </span><br><span class="line"><span class="keyword">FROM</span> emp e <span class="keyword">JOIN</span> dept d</span><br><span class="line"><span class="keyword">ON</span> e.deptno = d.deptno</span><br><span class="line"><span class="keyword">WHERE</span> job=<span class="string">'CLERK'</span>;</span><br></pre></td></tr></table></figure>
<h3 id="4-2-MAPJOIN"><a href="#4-2-MAPJOIN" class="headerlink" title="4.2 MAPJOIN"></a>4.2 MAPJOIN</h3><p>如果所有表中只有一张表是小表，那么 Hive 把这张小表加载到内存中。这时候程序会在 map 阶段直接拿另外一个表的数据和内存中表数据做匹配，由于在 map 就进行了 JOIN 操作，从而可以省略 reduce 过程，这样效率可以提升很多。Hive 中提供了 <code>/*+ MAPJOIN() */</code> 来标记小表，示例如下：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="comment">/*+ MAPJOIN(d) */</span> e.*,d.* </span><br><span class="line"><span class="keyword">FROM</span> emp e <span class="keyword">JOIN</span> dept d</span><br><span class="line"><span class="keyword">ON</span> e.deptno = d.deptno</span><br><span class="line"><span class="keyword">WHERE</span> job=<span class="string">'CLERK'</span>;</span><br></pre></td></tr></table></figure>
<h2 id="五、SELECT的其他用途"><a href="#五、SELECT的其他用途" class="headerlink" title="五、SELECT的其他用途"></a>五、SELECT的其他用途</h2><p>查看当前数据库：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> current_database()</span><br></pre></td></tr></table></figure>
<h2 id="六、本地模式"><a href="#六、本地模式" class="headerlink" title="六、本地模式"></a>六、本地模式</h2><p>在上面演示的语句中，大多数都会触发 MapReduce, 少部分不会触发，比如 <code>select * from emp limit 5</code> 就不会触发 MR，此时 Hive 只是简单的读取数据文件中的内容，然后格式化后进行输出。在需要执行 MapReduce 的查询中，你会发现执行时间可能会很长，这时候你可以选择开启本地模式。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">--本地模式默认关闭，需要手动开启此功能</span></span><br><span class="line"><span class="keyword">SET</span> hive.exec.mode.local.auto=<span class="literal">true</span>;</span><br></pre></td></tr></table></figure>
<p>启用后，Hive 将分析查询中每个 map-reduce 作业的大小，如果满足以下条件，则可以在本地运行它：</p>
<ul>
<li>作业的总输入大小低于：hive.exec.mode.local.auto.inputbytes.max（默认为 128MB）；</li>
<li>map-tasks 的总数小于：hive.exec.mode.local.auto.tasks.max（默认为 4）；</li>
<li>所需的 reduce 任务总数为 1 或 0。</li>
</ul>
<p>因为我们测试的数据集很小，所以你再次去执行上面涉及 MR 操作的查询，你会发现速度会有显著的提升。</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol>
<li><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Select" target="_blank" rel="noopener">LanguageManual Select</a></li>
<li><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Joins" target="_blank" rel="noopener">LanguageManual Joins</a></li>
<li><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+GroupBy" target="_blank" rel="noopener">LanguageManual GroupBy</a></li>
<li><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+SortBy" target="_blank" rel="noopener">LanguageManual SortBy</a></li>
</ol>
]]></content>
      <categories>
        <category>notes</category>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title>Kafka消费者详解</title>
    <url>/2021/03/17/Kafka%E6%B6%88%E8%B4%B9%E8%80%85%E8%AF%A6%E8%A7%A3/</url>
    <content><![CDATA[<h2 id="一、消费者和消费者群组"><a href="#一、消费者和消费者群组" class="headerlink" title="一、消费者和消费者群组"></a>一、消费者和消费者群组</h2><p>在 Kafka 中，消费者通常是消费者群组的一部分，多个消费者群组共同读取同一个主题时，彼此之间互不影响。Kafka 之所以要引入消费者群组这个概念是因为 Kafka 消费者经常会做一些高延迟的操作，比如把数据写到数据库或 HDFS ，或者进行耗时的计算，在这些情况下，单个消费者无法跟上数据生成的速度。此时可以增加更多的消费者，让它们分担负载，分别处理部分分区的消息，这就是 Kafka 实现横向伸缩的主要手段。 </p>
<div align="center"> <img src="../pictures/kafka-consumer01.png"> </div>

<p>需要注意的是：同一个分区只能被同一个消费者群组里面的一个消费者读取，不可能存在同一个分区被同一个消费者群里多个消费者共同读取的情况，如图：</p>
<div align="center"> <img src="../pictures/kafka-consumer02.png"> </div>

<p>可以看到即便消费者 Consumer5 空闲了，但是也不会去读取任何一个分区的数据，这同时也提醒我们在使用时应该合理设置消费者的数量，以免造成闲置和额外开销。</p>
<h2 id="二、分区再均衡"><a href="#二、分区再均衡" class="headerlink" title="二、分区再均衡"></a>二、分区再均衡</h2><p>因为群组里的消费者共同读取主题的分区，所以当一个消费者被关闭或发生崩溃时，它就离开了群组，原本由它读取的分区将由群组里的其他消费者来读取。同时在主题发生变化时 ， 比如添加了新的分区，也会发生分区与消费者的重新分配，分区的所有权从一个消费者转移到另一个消费者，这样的行为被称为再均衡。正是因为再均衡，所以消费费者群组才能保证高可用性和伸缩性。</p>
<p>消费者通过向群组协调器所在的 broker 发送心跳来维持它们和群组的从属关系以及它们对分区的所有权。只要消费者以正常的时间间隔发送心跳，就被认为是活跃的，说明它还在读取分区里的消息。消费者会在轮询消息或提交偏移量时发送心跳。如果消费者停止发送心跳的时间足够长，会话就会过期，群组协调器认为它已经死亡，就会触发再均衡。</p>
<h2 id="三、创建Kafka消费者"><a href="#三、创建Kafka消费者" class="headerlink" title="三、创建Kafka消费者"></a>三、创建Kafka消费者</h2><p>在创建消费者的时候以下以下三个选项是必选的：</p>
<ul>
<li><strong>bootstrap.servers</strong> ：指定 broker 的地址清单，清单里不需要包含所有的 broker 地址，生产者会从给定的 broker 里查找 broker 的信息。不过建议至少要提供两个 broker 的信息作为容错；</li>
<li><strong>key.deserializer</strong> ：指定键的反序列化器；</li>
<li><strong>value.deserializer</strong> ：指定值的反序列化器。</li>
</ul>
<p>除此之外你还需要指明你需要想订阅的主题，可以使用如下两个 API :</p>
<ul>
<li><strong>consumer.subscribe(Collection\<string> topics)</string></strong>  ：指明需要订阅的主题的集合；</li>
<li><strong>consumer.subscribe(Pattern pattern)</strong>  ：使用正则来匹配需要订阅的集合。</li>
</ul>
<p>最后只需要通过轮询 API(<code>poll</code>) 向服务器定时请求数据。一旦消费者订阅了主题，轮询就会处理所有的细节，包括群组协调、分区再均衡、发送心跳和获取数据，这使得开发者只需要关注从分区返回的数据，然后进行业务处理。 示例如下：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="type">String</span> topic = <span class="string">"Hello-Kafka"</span>;</span><br><span class="line"><span class="type">String</span> group = <span class="string">"group1"</span>;</span><br><span class="line"><span class="type">Properties</span> props = <span class="keyword">new</span> <span class="type">Properties</span>();</span><br><span class="line">props.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"hadoop001:9092"</span>);</span><br><span class="line"><span class="comment">/*指定分组 ID*/</span></span><br><span class="line">props.put(<span class="string">"group.id"</span>, group);</span><br><span class="line">props.put(<span class="string">"key.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">props.put(<span class="string">"value.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line"><span class="type">KafkaConsumer</span>&lt;<span class="type">String</span>, <span class="type">String</span>&gt; consumer = <span class="keyword">new</span> <span class="type">KafkaConsumer</span>&lt;&gt;(props);</span><br><span class="line"></span><br><span class="line"><span class="comment">/*订阅主题 (s)*/</span></span><br><span class="line">consumer.subscribe(<span class="type">Collections</span>.singletonList(topic));</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">        <span class="comment">/*轮询获取数据*/</span></span><br><span class="line">        <span class="type">ConsumerRecords</span>&lt;<span class="type">String</span>, <span class="type">String</span>&gt; records = consumer.poll(<span class="type">Duration</span>.of(<span class="number">100</span>, <span class="type">ChronoUnit</span>.<span class="type">MILLIS</span>));</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">ConsumerRecord</span>&lt;<span class="type">String</span>, <span class="type">String</span>&gt; record : records) &#123;</span><br><span class="line">            <span class="type">System</span>.out.printf(<span class="string">"topic = %s,partition = %d, key = %s, value = %s, offset = %d,\n"</span>,</span><br><span class="line">           record.topic(), record.partition(), record.key(), record.value(), record.offset());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">    consumer.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>本篇文章的所有示例代码可以从 Github 上进行下载：<a href="https://github.com/heibaiying/BigData-Notes/tree/master/code/Kafka/kafka-basis" target="_blank" rel="noopener">kafka-basis</a></p>
</blockquote>
<h2 id="三、-自动提交偏移量"><a href="#三、-自动提交偏移量" class="headerlink" title="三、 自动提交偏移量"></a>三、 自动提交偏移量</h2><h3 id="3-1-偏移量的重要性"><a href="#3-1-偏移量的重要性" class="headerlink" title="3.1 偏移量的重要性"></a>3.1 偏移量的重要性</h3><p>Kafka 的每一条消息都有一个偏移量属性，记录了其在分区中的位置，偏移量是一个单调递增的整数。消费者通过往一个叫作 <code>＿consumer_offset</code> 的特殊主题发送消息，消息里包含每个分区的偏移量。 如果消费者一直处于运行状态，那么偏移量就没有<br>什么用处。不过，如果有消费者退出或者新分区加入，此时就会触发再均衡。完成再均衡之后，每个消费者可能分配到新的分区，而不是之前处理的那个。为了能够继续之前的工作，消费者需要读取每个分区最后一次提交的偏移量，然后从偏移量指定的地方继续处理。 因为这个原因，所以如果不能正确提交偏移量，就可能会导致数据丢失或者重复出现消费，比如下面情况：</p>
<ul>
<li>如果提交的偏移量小于客户端处理的最后一个消息的偏移量 ，那么处于两个偏移量之间的消息就会被重复消费；</li>
<li>如果提交的偏移量大于客户端处理的最后一个消息的偏移量，那么处于两个偏移量之间的消息将会丢失。</li>
</ul>
<h3 id="3-2-自动提交偏移量"><a href="#3-2-自动提交偏移量" class="headerlink" title="3.2 自动提交偏移量"></a>3.2 自动提交偏移量</h3><p>Kafka 支持自动提交和手动提交偏移量两种方式。这里先介绍比较简单的自动提交：</p>
<p>只需要将消费者的 <code>enable.auto.commit</code> 属性配置为 <code>true</code> 即可完成自动提交的配置。 此时每隔固定的时间，消费者就会把 <code>poll()</code> 方法接收到的最大偏移量进行提交，提交间隔由 <code>auto.commit.interval.ms</code> 属性进行配置，默认值是 5s。</p>
<p>使用自动提交是存在隐患的，假设我们使用默认的 5s 提交时间间隔，在最近一次提交之后的 3s 发生了再均衡，再均衡之后，消费者从最后一次提交的偏移量位置开始读取消息。这个时候偏移量已经落后了 3s ，所以在这 3s 内到达的消息会被重复处理。可以通过修改提交时间间隔来更频繁地提交偏移量，减小可能出现重复消息的时间窗，不过这种情况是无法完全避免的。基于这个原因，Kafka 也提供了手动提交偏移量的 API，使得用户可以更为灵活的提交偏移量。</p>
<h2 id="四、手动提交偏移量"><a href="#四、手动提交偏移量" class="headerlink" title="四、手动提交偏移量"></a>四、手动提交偏移量</h2><p>用户可以通过将 <code>enable.auto.commit</code> 设为 <code>false</code>，然后手动提交偏移量。基于用户需求手动提交偏移量可以分为两大类：</p>
<ul>
<li>手动提交当前偏移量：即手动提交当前轮询的最大偏移量；</li>
<li>手动提交固定偏移量：即按照业务需求，提交某一个固定的偏移量。</li>
</ul>
<p>而按照 Kafka API，手动提交偏移量又可以分为同步提交和异步提交。</p>
<h3 id="4-1-同步提交"><a href="#4-1-同步提交" class="headerlink" title="4.1 同步提交"></a>4.1 同步提交</h3><p>通过调用 <code>consumer.commitSync()</code> 来进行同步提交，不传递任何参数时提交的是当前轮询的最大偏移量。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">    ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.of(<span class="number">100</span>, ChronoUnit.MILLIS));</span><br><span class="line">    <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records) &#123;</span><br><span class="line">        System.out.println(record);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">/*同步提交*/</span></span><br><span class="line">    consumer.commitSync();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>如果某个提交失败，同步提交还会进行重试，这可以保证数据能够最大限度提交成功，但是同时也会降低程序的吞吐量。基于这个原因，Kafka 还提供了异步提交的 API。</p>
<h3 id="4-2-异步提交"><a href="#4-2-异步提交" class="headerlink" title="4.2 异步提交"></a>4.2 异步提交</h3><p>异步提交可以提高程序的吞吐量，因为此时你可以尽管请求数据，而不用等待 Broker 的响应。代码如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">    ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.of(<span class="number">100</span>, ChronoUnit.MILLIS));</span><br><span class="line">    <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records) &#123;</span><br><span class="line">        System.out.println(record);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">/*异步提交并定义回调*/</span></span><br><span class="line">    consumer.commitAsync(<span class="keyword">new</span> OffsetCommitCallback() &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onComplete</span><span class="params">(Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets, Exception exception)</span> </span>&#123;</span><br><span class="line">          <span class="keyword">if</span> (exception != <span class="keyword">null</span>) &#123;</span><br><span class="line">             System.out.println(<span class="string">"错误处理"</span>);</span><br><span class="line">             offsets.forEach((x, y) -&gt; System.out.printf(<span class="string">"topic = %s,partition = %d, offset = %s \n"</span>,</span><br><span class="line">                                                            x.topic(), x.partition(), y.offset()));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>异步提交存在的问题是，在提交失败的时候不会进行自动重试，实际上也不能进行自动重试。假设程序同时提交了 200 和 300 的偏移量，此时 200 的偏移量失败的，但是紧随其后的 300 的偏移量成功了，此时如果重试就会存在 200 覆盖 300 偏移量的可能。同步提交就不存在这个问题，因为在同步提交的情况下，300 的提交请求必须等待服务器返回 200 提交请求的成功反馈后才会发出。基于这个原因，某些情况下，需要同时组合同步和异步两种提交方式。</p>
<blockquote>
<p>注：虽然程序不能在失败时候进行自动重试，但是我们是可以手动进行重试的，你可以通过一个 Map&lt;TopicPartition, Integer&gt; offsets 来维护你提交的每个分区的偏移量，然后当失败时候，你可以判断失败的偏移量是否小于你维护的同主题同分区的最后提交的偏移量，如果小于则代表你已经提交了更大的偏移量请求，此时不需要重试，否则就可以进行手动重试。</p>
</blockquote>
<h3 id="4-3-同步加异步提交"><a href="#4-3-同步加异步提交" class="headerlink" title="4.3  同步加异步提交"></a>4.3  同步加异步提交</h3><p>下面这种情况，在正常的轮询中使用异步提交来保证吞吐量，但是因为在最后即将要关闭消费者了，所以此时需要用同步提交来保证最大限度的提交成功。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">        <span class="type">ConsumerRecords</span>&lt;<span class="type">String</span>, <span class="type">String</span>&gt; records = consumer.poll(<span class="type">Duration</span>.of(<span class="number">100</span>, <span class="type">ChronoUnit</span>.<span class="type">MILLIS</span>));</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">ConsumerRecord</span>&lt;<span class="type">String</span>, <span class="type">String</span>&gt; record : records) &#123;</span><br><span class="line">            <span class="type">System</span>.out.println(record);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 异步提交</span></span><br><span class="line">        consumer.commitAsync();</span><br><span class="line">    &#125;</span><br><span class="line">&#125; <span class="keyword">catch</span> (<span class="type">Exception</span> e) &#123;</span><br><span class="line">    e.printStackTrace();</span><br><span class="line">&#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="comment">// 因为即将要关闭消费者，所以要用同步提交保证提交成功</span></span><br><span class="line">        consumer.commitSync();</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">        consumer.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="4-4-提交特定偏移量"><a href="#4-4-提交特定偏移量" class="headerlink" title="4.4 提交特定偏移量"></a>4.4 提交特定偏移量</h3><p>在上面同步和异步提交的 API 中，实际上我们都没有对 commit 方法传递参数，此时默认提交的是当前轮询的最大偏移量，如果你需要提交特定的偏移量，可以调用它们的重载方法。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/*同步提交特定偏移量*/</span></span><br><span class="line">commitSync(Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets) </span><br><span class="line"><span class="comment">/*异步提交特定偏移量*/</span>    </span><br><span class="line">commitAsync(Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets, OffsetCommitCallback callback)</span><br></pre></td></tr></table></figure>
<p>需要注意的是，因为你可以订阅多个主题，所以 <code>offsets</code> 中必须要包含所有主题的每个分区的偏移量，示例代码如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">        ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.of(<span class="number">100</span>, ChronoUnit.MILLIS));</span><br><span class="line">        <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records) &#123;</span><br><span class="line">            System.out.println(record);</span><br><span class="line">            <span class="comment">/*记录每个主题的每个分区的偏移量*/</span></span><br><span class="line">            TopicPartition topicPartition = <span class="keyword">new</span> TopicPartition(record.topic(), record.partition());</span><br><span class="line">            OffsetAndMetadata offsetAndMetadata = <span class="keyword">new</span> OffsetAndMetadata(record.offset()+<span class="number">1</span>, <span class="string">"no metaData"</span>);</span><br><span class="line">            <span class="comment">/*TopicPartition 重写过 hashCode 和 equals 方法，所以能够保证同一主题和分区的实例不会被重复添加*/</span></span><br><span class="line">            offsets.put(topicPartition, offsetAndMetadata);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">/*提交特定偏移量*/</span></span><br><span class="line">        consumer.commitAsync(offsets, <span class="keyword">null</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">    consumer.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="五、监听分区再均衡"><a href="#五、监听分区再均衡" class="headerlink" title="五、监听分区再均衡"></a>五、监听分区再均衡</h2><p>因为分区再均衡会导致分区与消费者的重新划分，有时候你可能希望在再均衡前执行一些操作：比如提交已经处理但是尚未提交的偏移量，关闭数据库连接等。此时可以在订阅主题时候，调用 <code>subscribe</code> 的重载方法传入自定义的分区再均衡监听器。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"> <span class="comment">/*订阅指定集合内的所有主题*/</span></span><br><span class="line">subscribe(Collection&lt;String&gt; topics, ConsumerRebalanceListener listener)</span><br><span class="line"> <span class="comment">/*使用正则匹配需要订阅的主题*/</span>    </span><br><span class="line">subscribe(Pattern pattern, ConsumerRebalanceListener listener)</span><br></pre></td></tr></table></figure>
<p>代码示例如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line"></span><br><span class="line">consumer.subscribe(Collections.singletonList(topic), <span class="keyword">new</span> ConsumerRebalanceListener() &#123;</span><br><span class="line">    <span class="comment">/*该方法会在消费者停止读取消息之后，再均衡开始之前就调用*/</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onPartitionsRevoked</span><span class="params">(Collection&lt;TopicPartition&gt; partitions)</span> </span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"再均衡即将触发"</span>);</span><br><span class="line">        <span class="comment">// 提交已经处理的偏移量</span></span><br><span class="line">        consumer.commitSync(offsets);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/*该方法会在重新分配分区之后，消费者开始读取消息之前被调用*/</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onPartitionsAssigned</span><span class="params">(Collection&lt;TopicPartition&gt; partitions)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">        ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.of(<span class="number">100</span>, ChronoUnit.MILLIS));</span><br><span class="line">        <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records) &#123;</span><br><span class="line">            System.out.println(record);</span><br><span class="line">            TopicPartition topicPartition = <span class="keyword">new</span> TopicPartition(record.topic(), record.partition());</span><br><span class="line">            OffsetAndMetadata offsetAndMetadata = <span class="keyword">new</span> OffsetAndMetadata(record.offset() + <span class="number">1</span>, <span class="string">"no metaData"</span>);</span><br><span class="line">            <span class="comment">/*TopicPartition 重写过 hashCode 和 equals 方法，所以能够保证同一主题和分区的实例不会被重复添加*/</span></span><br><span class="line">            offsets.put(topicPartition, offsetAndMetadata);</span><br><span class="line">        &#125;</span><br><span class="line">        consumer.commitAsync(offsets, <span class="keyword">null</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">    consumer.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="六-、退出轮询"><a href="#六-、退出轮询" class="headerlink" title="六 、退出轮询"></a>六 、退出轮询</h2><p>Kafka 提供了 <code>consumer.wakeup()</code> 方法用于退出轮询，它通过抛出 <code>WakeupException</code> 异常来跳出循环。需要注意的是，在退出线程时最好显示的调用 <code>consumer.close()</code> , 此时消费者会提交任何还没有提交的东西，并向群组协调器发送消息，告知自己要离开群组，接下来就会触发再均衡 ，而不需要等待会话超时。 </p>
<p>下面的示例代码为监听控制台输出，当输入 <code>exit</code> 时结束轮询，关闭消费者并退出程序：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/*调用 wakeup 优雅的退出*/</span></span><br><span class="line"><span class="keyword">final</span> Thread mainThread = Thread.currentThread();</span><br><span class="line"><span class="keyword">new</span> Thread(() -&gt; &#123;</span><br><span class="line">    Scanner sc = <span class="keyword">new</span> Scanner(System.in);</span><br><span class="line">    <span class="keyword">while</span> (sc.hasNext()) &#123;</span><br><span class="line">        <span class="keyword">if</span> (<span class="string">"exit"</span>.equals(sc.next())) &#123;</span><br><span class="line">            consumer.wakeup();</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                <span class="comment">/*等待主线程完成提交偏移量、关闭消费者等操作*/</span></span><br><span class="line">                mainThread.join();</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">                e.printStackTrace();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;).start();</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">        ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.of(<span class="number">100</span>, ChronoUnit.MILLIS));</span><br><span class="line">        <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; rd : records) &#123;</span><br><span class="line">            System.out.printf(<span class="string">"topic = %s,partition = %d, key = %s, value = %s, offset = %d,\n"</span>,</span><br><span class="line">                              rd.topic(), rd.partition(), rd.key(), rd.value(), rd.offset());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125; <span class="keyword">catch</span> (WakeupException e) &#123;</span><br><span class="line">    <span class="comment">//对于 wakeup() 调用引起的 WakeupException 异常可以不必处理</span></span><br><span class="line">&#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">    consumer.close();</span><br><span class="line">    System.out.println(<span class="string">"consumer 关闭"</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="七、独立的消费者"><a href="#七、独立的消费者" class="headerlink" title="七、独立的消费者"></a>七、独立的消费者</h2><p>因为 Kafka 的设计目标是高吞吐和低延迟，所以在 Kafka 中，消费者通常都是从属于某个群组的，这是因为单个消费者的处理能力是有限的。但是某些时候你的需求可能很简单，比如可能只需要一个消费者从一个主题的所有分区或者某个特定的分区读取数据，这个时候就不需要消费者群组和再均衡了， 只需要把主题或者分区分配给消费者，然后开始读取消息井提交偏移量即可。</p>
<p>在这种情况下，就不需要订阅主题， 取而代之的是消费者为自己分配分区。 一个消费者可以订阅主题（井加入消费者群组），或者为自己分配分区，但不能同时做这两件事情。 分配分区的示例代码如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">List&lt;TopicPartition&gt; partitions = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">List&lt;PartitionInfo&gt; partitionInfos = consumer.partitionsFor(topic);</span><br><span class="line"></span><br><span class="line"><span class="comment">/*可以指定读取哪些分区 如这里假设只读取主题的 0 分区*/</span></span><br><span class="line"><span class="keyword">for</span> (PartitionInfo partition : partitionInfos) &#123;</span><br><span class="line">    <span class="keyword">if</span> (partition.partition()==<span class="number">0</span>)&#123;</span><br><span class="line">        partitions.add(<span class="keyword">new</span> TopicPartition(partition.topic(), partition.partition()));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 为消费者指定分区</span></span><br><span class="line">consumer.assign(partitions);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">    ConsumerRecords&lt;Integer, String&gt; records = consumer.poll(Duration.of(<span class="number">100</span>, ChronoUnit.MILLIS));</span><br><span class="line">    <span class="keyword">for</span> (ConsumerRecord&lt;Integer, String&gt; record : records) &#123;</span><br><span class="line">        System.out.printf(<span class="string">"partition = %s, key = %d, value = %s\n"</span>,</span><br><span class="line">                          record.partition(), record.key(), record.value());</span><br><span class="line">    &#125;</span><br><span class="line">    consumer.commitSync();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="附录-Kafka消费者可选属性"><a href="#附录-Kafka消费者可选属性" class="headerlink" title="附录 : Kafka消费者可选属性"></a>附录 : Kafka消费者可选属性</h2><h3 id="1-fetch-min-byte"><a href="#1-fetch-min-byte" class="headerlink" title="1. fetch.min.byte"></a>1. fetch.min.byte</h3><p>消费者从服务器获取记录的最小字节数。如果可用的数据量小于设置值，broker 会等待有足够的可用数据时才会把它返回给消费者。</p>
<h3 id="2-fetch-max-wait-ms"><a href="#2-fetch-max-wait-ms" class="headerlink" title="2. fetch.max.wait.ms"></a>2. fetch.max.wait.ms</h3><p>broker 返回给消费者数据的等待时间，默认是 500ms。</p>
<h3 id="3-max-partition-fetch-bytes"><a href="#3-max-partition-fetch-bytes" class="headerlink" title="3. max.partition.fetch.bytes"></a>3. max.partition.fetch.bytes</h3><p>该属性指定了服务器从每个分区返回给消费者的最大字节数，默认为 1MB。</p>
<h3 id="4-session-timeout-ms"><a href="#4-session-timeout-ms" class="headerlink" title="4. session.timeout.ms"></a>4. session.timeout.ms</h3><p>消费者在被认为死亡之前可以与服务器断开连接的时间，默认是 3s。</p>
<h3 id="5-auto-offset-reset"><a href="#5-auto-offset-reset" class="headerlink" title="5. auto.offset.reset"></a>5. auto.offset.reset</h3><p>该属性指定了消费者在读取一个没有偏移量的分区或者偏移量无效的情况下该作何处理：</p>
<ul>
<li>latest (默认值) ：在偏移量无效的情况下，消费者将从最新的记录开始读取数据（在消费者启动之后生成的最新记录）;</li>
<li>earliest ：在偏移量无效的情况下，消费者将从起始位置读取分区的记录。</li>
</ul>
<h3 id="6-enable-auto-commit"><a href="#6-enable-auto-commit" class="headerlink" title="6. enable.auto.commit"></a>6. enable.auto.commit</h3><p>是否自动提交偏移量，默认值是 true。为了避免出现重复消费和数据丢失，可以把它设置为 false。</p>
<h3 id="7-client-id"><a href="#7-client-id" class="headerlink" title="7. client.id"></a>7. client.id</h3><p>客户端 id，服务器用来识别消息的来源。</p>
<h3 id="8-max-poll-records"><a href="#8-max-poll-records" class="headerlink" title="8. max.poll.records"></a>8. max.poll.records</h3><p>单次调用 <code>poll()</code> 方法能够返回的记录数量。</p>
<h3 id="9-receive-buffer-bytes-amp-send-buffer-byte"><a href="#9-receive-buffer-bytes-amp-send-buffer-byte" class="headerlink" title="9. receive.buffer.bytes &amp; send.buffer.byte"></a>9. receive.buffer.bytes &amp; send.buffer.byte</h3><p>这两个参数分别指定 TCP socket 接收和发送数据包缓冲区的大小，-1 代表使用操作系统的默认值。</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol>
<li>Neha Narkhede, Gwen Shapira ,Todd Palino(著) , 薛命灯 (译) . Kafka 权威指南 . 人民邮电出版社 . 2017-12-26</li>
</ol>
]]></content>
      <categories>
        <category>技术</category>
        <category>kafka</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title>Hive 视图和索引</title>
    <url>/2021/03/18/Hive%E8%A7%86%E5%9B%BE%E5%92%8C%E7%B4%A2%E5%BC%95/</url>
    <content><![CDATA[<h2 id="一、视图"><a href="#一、视图" class="headerlink" title="一、视图"></a>一、视图</h2><h3 id="1-1-简介"><a href="#1-1-简介" class="headerlink" title="1.1 简介"></a>1.1 简介</h3><p>Hive 中的视图和 RDBMS 中视图的概念一致，都是一组数据的逻辑表示，本质上就是一条 SELECT 语句的结果集。视图是纯粹的逻辑对象，没有关联的存储 (Hive 3.0.0 引入的物化视图除外)，当查询引用视图时，Hive 可以将视图的定义与查询结合起来，例如将查询中的过滤器推送到视图中。</p>
<h3 id="1-2-创建视图"><a href="#1-2-创建视图" class="headerlink" title="1.2 创建视图"></a>1.2 创建视图</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">VIEW</span> [<span class="keyword">IF</span> <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>] [db_name.]view_name   <span class="comment">-- 视图名称</span></span><br><span class="line">  [(column_name [<span class="keyword">COMMENT</span> column_comment], ...) ]    <span class="comment">--列名</span></span><br><span class="line">  [<span class="keyword">COMMENT</span> view_comment]  <span class="comment">--视图注释</span></span><br><span class="line">  [TBLPROPERTIES (property_name = property_value, ...)]  <span class="comment">--额外信息</span></span><br><span class="line">  <span class="keyword">AS</span> <span class="keyword">SELECT</span> ...;</span><br></pre></td></tr></table></figure>
<p>在 Hive 中可以使用 <code>CREATE VIEW</code> 创建视图，如果已存在具有相同名称的表或视图，则会抛出异常，建议使用 <code>IF NOT EXISTS</code> 预做判断。在使用视图时候需要注意以下事项：</p>
<ul>
<li><p>视图是只读的，不能用作 LOAD / INSERT / ALTER 的目标；</p>
</li>
<li><p>在创建视图时候视图就已经固定，对基表的后续更改（如添加列）将不会反映在视图；</p>
</li>
<li><p>删除基表并不会删除视图，需要手动删除视图；</p>
</li>
<li><p>视图可能包含 ORDER BY 和 LIMIT 子句。如果引用视图的查询语句也包含这类子句，其执行优先级低于视图对应字句。例如，视图 <code>custom_view</code> 指定 LIMIT 5，查询语句为 <code>select * from custom_view  LIMIT 10</code>，此时结果最多返回 5 行。</p>
</li>
<li><p>创建视图时，如果未提供列名，则将从 SELECT 语句中自动派生列名；</p>
</li>
<li><p>创建视图时，如果 SELECT 语句中包含其他表达式，例如 x + y，则列名称将以_C0，_C1 等形式生成；</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">VIEW</span>  <span class="keyword">IF</span> <span class="keyword">NOT</span> <span class="keyword">EXISTS</span> custom_view <span class="keyword">AS</span> <span class="keyword">SELECT</span> empno, empno+deptno , <span class="number">1</span>+<span class="number">2</span> <span class="keyword">FROM</span> emp;</span><br></pre></td></tr></table></figure>
<div align="center"> <img src="../pictures/hive-1-2-view.png"> </div>



</li>
</ul>
<h3 id="1-3-查看视图"><a href="#1-3-查看视图" class="headerlink" title="1.3 查看视图"></a>1.3 查看视图</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 查看所有视图： 没有单独查看视图列表的语句，只能使用 show tables</span></span><br><span class="line"><span class="keyword">show</span> <span class="keyword">tables</span>;</span><br><span class="line"><span class="comment">-- 查看某个视图</span></span><br><span class="line">desc view_name;</span><br><span class="line"><span class="comment">-- 查看某个视图详细信息</span></span><br><span class="line">desc formatted view_name;</span><br></pre></td></tr></table></figure>
<h3 id="1-4-删除视图"><a href="#1-4-删除视图" class="headerlink" title="1.4 删除视图"></a>1.4 删除视图</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">DROP</span> <span class="keyword">VIEW</span> [<span class="keyword">IF</span> <span class="keyword">EXISTS</span>] [db_name.]view_name;</span><br></pre></td></tr></table></figure>
<p>删除视图时，如果被删除的视图被其他视图所引用，这时候程序不会发出警告，但是引用该视图其他视图已经失效，需要进行重建或者删除。</p>
<h3 id="1-5-修改视图"><a href="#1-5-修改视图" class="headerlink" title="1.5 修改视图"></a>1.5 修改视图</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">VIEW</span> [db_name.]view_name <span class="keyword">AS</span> select_statement;</span><br></pre></td></tr></table></figure>
<p> 被更改的视图必须存在，且视图不能具有分区，如果视图具有分区，则修改失败。  </p>
<h3 id="1-6-修改视图属性"><a href="#1-6-修改视图属性" class="headerlink" title="1.6 修改视图属性"></a>1.6 修改视图属性</h3><p>语法：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">VIEW</span> [db_name.]view_name <span class="keyword">SET</span> TBLPROPERTIES table_properties;</span><br><span class="line"> </span><br><span class="line">table_properties:</span><br><span class="line">  : (property_name = property_value, property_name = property_value, ...)</span><br></pre></td></tr></table></figure>
<p>示例：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">VIEW</span> custom_view <span class="keyword">SET</span> TBLPROPERTIES (<span class="string">'create'</span>=<span class="string">'heibaiying'</span>,<span class="string">'date'</span>=<span class="string">'2019-05-05'</span>);</span><br></pre></td></tr></table></figure>
<div align="center"> <img width="600px" src="../pictures/hive-view-properties.png"> </div>





<h2 id="二、索引"><a href="#二、索引" class="headerlink" title="二、索引"></a>二、索引</h2><h3 id="2-1-简介"><a href="#2-1-简介" class="headerlink" title="2.1 简介"></a>2.1 简介</h3><p>Hive 在 0.7.0 引入了索引的功能，索引的设计目标是提高表某些列的查询速度。如果没有索引，带有谓词的查询（如’WHERE table1.column = 10’）会加载整个表或分区并处理所有行。但是如果 column 存在索引，则只需要加载和处理文件的一部分。</p>
<h3 id="2-2-索引原理"><a href="#2-2-索引原理" class="headerlink" title="2.2 索引原理"></a>2.2 索引原理</h3><p>在指定列上建立索引，会产生一张索引表（表结构如下），里面的字段包括：索引列的值、该值对应的 HDFS 文件路径、该值在文件中的偏移量。在查询涉及到索引字段时，首先到索引表查找索引列值对应的 HDFS 文件路径及偏移量，这样就避免了全表扫描。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">+--------------+----------------+----------+--+</span><br><span class="line">|   col_name   |   data_type    | comment     |</span><br><span class="line">+--------------+----------------+----------+--+</span><br><span class="line">| empno        | int            |  建立索引的列  |   </span><br><span class="line">| _bucketname  | string         |  HDFS 文件路径  |</span><br><span class="line">| _offsets     | array&lt;bigint&gt;  |  偏移量       |</span><br><span class="line">+--------------+----------------+----------+--+</span><br></pre></td></tr></table></figure>
<h3 id="2-3-创建索引"><a href="#2-3-创建索引" class="headerlink" title="2.3 创建索引"></a>2.3 创建索引</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">INDEX</span> index_name     <span class="comment">--索引名称</span></span><br><span class="line">  <span class="keyword">ON</span> <span class="keyword">TABLE</span> base_table_name (col_name, ...)  <span class="comment">--建立索引的列</span></span><br><span class="line">  <span class="keyword">AS</span> index_type    <span class="comment">--索引类型</span></span><br><span class="line">  [<span class="keyword">WITH</span> <span class="keyword">DEFERRED</span> <span class="keyword">REBUILD</span>]    <span class="comment">--重建索引</span></span><br><span class="line">  [IDXPROPERTIES (property_name=property_value, ...)]  <span class="comment">--索引额外属性</span></span><br><span class="line">  [<span class="keyword">IN</span> <span class="keyword">TABLE</span> index_table_name]    <span class="comment">--索引表的名字</span></span><br><span class="line">  [</span><br><span class="line">     [ <span class="keyword">ROW</span> <span class="keyword">FORMAT</span> ...] <span class="keyword">STORED</span> <span class="keyword">AS</span> ...  </span><br><span class="line">     | <span class="keyword">STORED</span> <span class="keyword">BY</span> ...</span><br><span class="line">  ]   <span class="comment">--索引表行分隔符 、 存储格式</span></span><br><span class="line">  [LOCATION hdfs_path]  <span class="comment">--索引表存储位置</span></span><br><span class="line">  [TBLPROPERTIES (...)]   <span class="comment">--索引表表属性</span></span><br><span class="line">  [<span class="keyword">COMMENT</span> <span class="string">"index comment"</span>];  <span class="comment">--索引注释</span></span><br></pre></td></tr></table></figure>
<h3 id="2-4-查看索引"><a href="#2-4-查看索引" class="headerlink" title="2.4 查看索引"></a>2.4 查看索引</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">--显示表上所有列的索引</span></span><br><span class="line"><span class="keyword">SHOW</span> FORMATTED <span class="keyword">INDEX</span> <span class="keyword">ON</span> table_name;</span><br></pre></td></tr></table></figure>
<h3 id="2-4-删除索引"><a href="#2-4-删除索引" class="headerlink" title="2.4 删除索引"></a>2.4 删除索引</h3><p>删除索引会删除对应的索引表。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">DROP</span> <span class="keyword">INDEX</span> [<span class="keyword">IF</span> <span class="keyword">EXISTS</span>] index_name <span class="keyword">ON</span> table_name;</span><br></pre></td></tr></table></figure>
<p>如果存在索引的表被删除了，其对应的索引和索引表都会被删除。如果被索引表的某个分区被删除了，那么分区对应的分区索引也会被删除。</p>
<h3 id="2-5-重建索引"><a href="#2-5-重建索引" class="headerlink" title="2.5 重建索引"></a>2.5 重建索引</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">INDEX</span> index_name <span class="keyword">ON</span> table_name [<span class="keyword">PARTITION</span> partition_spec] <span class="keyword">REBUILD</span>;</span><br></pre></td></tr></table></figure>
<p>重建索引。如果指定了 PARTITION，则仅重建该分区的索引。</p>
<h2 id="三、索引案例"><a href="#三、索引案例" class="headerlink" title="三、索引案例"></a>三、索引案例</h2><h3 id="3-1-创建索引"><a href="#3-1-创建索引" class="headerlink" title="3.1 创建索引"></a>3.1 创建索引</h3><p>在 emp 表上针对 <code>empno</code> 字段创建名为 <code>emp_index</code>,索引数据存储在 <code>emp_index_table</code> 索引表中</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">index</span> emp_index <span class="keyword">on</span> <span class="keyword">table</span> emp(empno) <span class="keyword">as</span>  </span><br><span class="line"><span class="string">'org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler'</span> </span><br><span class="line"><span class="keyword">with</span> <span class="keyword">deferred</span> <span class="keyword">rebuild</span> </span><br><span class="line"><span class="keyword">in</span> <span class="keyword">table</span> emp_index_table ;</span><br></pre></td></tr></table></figure>
<p>此时索引表中是没有数据的，需要重建索引才会有索引的数据。</p>
<h3 id="3-2-重建索引"><a href="#3-2-重建索引" class="headerlink" title="3.2 重建索引"></a>3.2 重建索引</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">index</span> emp_index <span class="keyword">on</span> emp <span class="keyword">rebuild</span>;</span><br></pre></td></tr></table></figure>
<p>Hive 会启动 MapReduce 作业去建立索引，建立好后查看索引表数据如下。三个表字段分别代表：索引列的值、该值对应的 HDFS 文件路径、该值在文件中的偏移量。</p>
<div align="center"> <img width="700px" src="../pictures/hive-index-table.png"> </div>

<h3 id="3-3-自动使用索引"><a href="#3-3-自动使用索引" class="headerlink" title="3.3 自动使用索引"></a>3.3 自动使用索引</h3><p>默认情况下，虽然建立了索引，但是 Hive 在查询时候是不会自动去使用索引的，需要开启相关配置。开启配置后，涉及到索引列的查询就会使用索引功能去优化查询。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SET</span> hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;</span><br><span class="line"><span class="keyword">SET</span> hive.optimize.index.filter=<span class="literal">true</span>;</span><br><span class="line"><span class="keyword">SET</span> hive.optimize.index.filter.compact.minsize=<span class="number">0</span>;</span><br></pre></td></tr></table></figure>
<h3 id="3-4-查看索引"><a href="#3-4-查看索引" class="headerlink" title="3.4 查看索引"></a>3.4 查看索引</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SHOW</span> <span class="keyword">INDEX</span> <span class="keyword">ON</span> emp;</span><br></pre></td></tr></table></figure>
<div align="center"> <img src="../pictures/hive-index-show.png"> </div>





<h2 id="四、索引的缺陷"><a href="#四、索引的缺陷" class="headerlink" title="四、索引的缺陷"></a>四、索引的缺陷</h2><p>索引表最主要的一个缺陷在于：索引表无法自动 rebuild，这也就意味着如果表中有数据新增或删除，则必须手动 rebuild，重新执行 MapReduce 作业，生成索引表数据。</p>
<p>同时按照<a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Indexing" target="_blank" rel="noopener">官方文档</a> 的说明，Hive 会从 3.0 开始移除索引功能，主要基于以下两个原因：</p>
<ul>
<li>具有自动重写的物化视图 (Materialized View) 可以产生与索引相似的效果（Hive 2.3.0 增加了对物化视图的支持，在 3.0 之后正式引入）。</li>
<li>使用列式存储文件格式（Parquet，ORC）进行存储时，这些格式支持选择性扫描，可以跳过不需要的文件或块。</li>
</ul>
<blockquote>
<p>ORC 内置的索引功能可以参阅这篇文章：<a href="http://lxw1234.com/archives/2016/04/632.htm" target="_blank" rel="noopener">Hive 性能优化之 ORC 索引–Row Group Index vs Bloom Filter Index</a></p>
</blockquote>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol>
<li><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualDDL-Create/Drop/AlterView" target="_blank" rel="noopener">Create/Drop/Alter View</a></li>
<li><a href="https://cwiki.apache.org/confluence/display/Hive/Materialized+views" target="_blank" rel="noopener">Materialized views</a></li>
<li><a href="http://lxw1234.com/archives/2015/05/207.htm" target="_blank" rel="noopener">Hive 索引</a></li>
<li><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Indexing" target="_blank" rel="noopener">Overview of Hive Indexes</a></li>
</ol>
]]></content>
      <categories>
        <category>notes</category>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title>Hive简介及核心概念</title>
    <url>/2021/03/18/Hive%E7%AE%80%E4%BB%8B%E5%8F%8A%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5/</url>
    <content><![CDATA[<h2 id="一、简介"><a href="#一、简介" class="headerlink" title="一、简介"></a>一、简介</h2><p>Hive 是一个构建在 Hadoop 之上的数据仓库，它可以将结构化的数据文件映射成表，并提供类 SQL 查询功能，用于查询的 SQL 语句会被转化为 MapReduce 作业，然后提交到 Hadoop 上运行。</p>
<p><strong>特点</strong>：</p>
<ol>
<li>简单、容易上手 (提供了类似 sql 的查询语言 hql)，使得精通 sql 但是不了解 Java 编程的人也能很好地进行大数据分析；</li>
<li>灵活性高，可以自定义用户函数 (UDF) 和存储格式；</li>
<li>为超大的数据集设计的计算和存储能力，集群扩展容易;</li>
<li>统一的元数据管理，可与 presto／impala／sparksql 等共享数据；</li>
<li>执行延迟高，不适合做数据的实时处理，但适合做海量数据的离线处理。</li>
</ol>
<h2 id="二、Hive的体系架构"><a href="#二、Hive的体系架构" class="headerlink" title="二、Hive的体系架构"></a>二、Hive的体系架构</h2><div align="center"> <img width="600px" src="../pictures/hive体系架构.png"> </div>

<h3 id="2-1-command-line-shell-amp-thrift-jdbc"><a href="#2-1-command-line-shell-amp-thrift-jdbc" class="headerlink" title="2.1 command-line shell &amp; thrift/jdbc"></a>2.1 command-line shell &amp; thrift/jdbc</h3><p>可以用 command-line shell 和 thrift／jdbc 两种方式来操作数据：</p>
<ul>
<li><strong>command-line shell</strong>：通过 hive 命令行的的方式来操作数据；</li>
<li><strong>thrift／jdbc</strong>：通过 thrift 协议按照标准的 JDBC 的方式操作数据。</li>
</ul>
<h3 id="2-2-Metastore"><a href="#2-2-Metastore" class="headerlink" title="2.2 Metastore"></a>2.2 Metastore</h3><p>在 Hive 中，表名、表结构、字段名、字段类型、表的分隔符等统一被称为元数据。所有的元数据默认存储在 Hive 内置的 derby 数据库中，但由于 derby 只能有一个实例，也就是说不能有多个命令行客户端同时访问，所以在实际生产环境中，通常使用 MySQL 代替 derby。</p>
<p>Hive 进行的是统一的元数据管理，就是说你在 Hive 上创建了一张表，然后在 presto／impala／sparksql 中都是可以直接使用的，它们会从 Metastore 中获取统一的元数据信息，同样的你在 presto／impala／sparksql 中创建一张表，在 Hive 中也可以直接使用。</p>
<h3 id="2-3-HQL的执行流程"><a href="#2-3-HQL的执行流程" class="headerlink" title="2.3 HQL的执行流程"></a>2.3 HQL的执行流程</h3><p>Hive 在执行一条 HQL 的时候，会经过以下步骤：</p>
<ol>
<li>语法解析：Antlr 定义 SQL 的语法规则，完成 SQL 词法，语法解析，将 SQL 转化为抽象 语法树 AST Tree；</li>
<li>语义解析：遍历 AST Tree，抽象出查询的基本组成单元 QueryBlock；</li>
<li>生成逻辑执行计划：遍历 QueryBlock，翻译为执行操作树 OperatorTree；</li>
<li>优化逻辑执行计划：逻辑层优化器进行 OperatorTree 变换，合并不必要的 ReduceSinkOperator，减少 shuffle 数据量；</li>
<li>生成物理执行计划：遍历 OperatorTree，翻译为 MapReduce 任务；</li>
<li>优化物理执行计划：物理层优化器进行 MapReduce 任务的变换，生成最终的执行计划。</li>
</ol>
<blockquote>
<p>关于 Hive SQL 的详细执行流程可以参考美团技术团队的文章：<a href="https://tech.meituan.com/2014/02/12/hive-sql-to-mapreduce.html" target="_blank" rel="noopener">Hive SQL 的编译过程</a></p>
</blockquote>
<h2 id="三、数据类型"><a href="#三、数据类型" class="headerlink" title="三、数据类型"></a>三、数据类型</h2><h3 id="3-1-基本数据类型"><a href="#3-1-基本数据类型" class="headerlink" title="3.1 基本数据类型"></a>3.1 基本数据类型</h3><p>Hive 表中的列支持以下基本数据类型：</p>
<table>
<thead>
<tr>
<th>大类</th>
<th>类型</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Integers（整型）</strong></td>
<td>TINYINT—1 字节的有符号整数 <br>SMALLINT—2 字节的有符号整数<br> INT—4 字节的有符号整数<br> BIGINT—8 字节的有符号整数</td>
</tr>
<tr>
<td><strong>Boolean（布尔型）</strong></td>
<td>BOOLEAN—TRUE/FALSE</td>
</tr>
<tr>
<td><strong>Floating point numbers（浮点型）</strong></td>
<td>FLOAT— 单精度浮点型 <br>DOUBLE—双精度浮点型</td>
</tr>
<tr>
<td><strong>Fixed point numbers（定点数）</strong></td>
<td>DECIMAL—用户自定义精度定点数，比如 DECIMAL(7,2)</td>
</tr>
<tr>
<td><strong>String types（字符串）</strong></td>
<td>STRING—指定字符集的字符序列<br> VARCHAR—具有最大长度限制的字符序列 <br>CHAR—固定长度的字符序列</td>
</tr>
<tr>
<td><strong>Date and time types（日期时间类型）</strong></td>
<td>TIMESTAMP —  时间戳 <br>TIMESTAMP WITH LOCAL TIME ZONE — 时间戳，纳秒精度<br> DATE—日期类型</td>
</tr>
<tr>
<td><strong>Binary types（二进制类型）</strong></td>
<td>BINARY—字节序列</td>
</tr>
</tbody>
</table>
<blockquote>
<p>TIMESTAMP 和 TIMESTAMP WITH LOCAL TIME ZONE 的区别如下：</p>
<ul>
<li><strong>TIMESTAMP WITH LOCAL TIME ZONE</strong>：用户提交时间给数据库时，会被转换成数据库所在的时区来保存。查询时则按照查询客户端的不同，转换为查询客户端所在时区的时间。</li>
<li><strong>TIMESTAMP</strong> ：提交什么时间就保存什么时间，查询时也不做任何转换。</li>
</ul>
</blockquote>
<h3 id="3-2-隐式转换"><a href="#3-2-隐式转换" class="headerlink" title="3.2 隐式转换"></a>3.2 隐式转换</h3><p>Hive 中基本数据类型遵循以下的层次结构，按照这个层次结构，子类型到祖先类型允许隐式转换。例如 INT 类型的数据允许隐式转换为 BIGINT 类型。额外注意的是：按照类型层次结构允许将 STRING 类型隐式转换为 DOUBLE 类型。</p>
<div align="center"> <img src="../pictures/hive-data-type.png"> </div>



<h3 id="3-3-复杂类型"><a href="#3-3-复杂类型" class="headerlink" title="3.3 复杂类型"></a>3.3 复杂类型</h3><table>
<thead>
<tr>
<th>类型</th>
<th>描述</th>
<th>示例</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>STRUCT</strong></td>
<td>类似于对象，是字段的集合，字段的类型可以不同，可以使用 <code>名称.字段名</code> 方式进行访问</td>
<td>STRUCT (‘xiaoming’, 12 , ‘2018-12-12’)</td>
</tr>
<tr>
<td><strong>MAP</strong></td>
<td>键值对的集合，可以使用 <code>名称[key]</code> 的方式访问对应的值</td>
<td>map(‘a’, 1, ‘b’, 2)</td>
</tr>
<tr>
<td><strong>ARRAY</strong></td>
<td>数组是一组具有相同类型和名称的变量的集合，可以使用 <code>名称[index]</code> 访问对应的值</td>
<td>ARRAY(‘a’, ‘b’, ‘c’, ‘d’)</td>
</tr>
</tbody>
</table>
<h3 id="3-4-示例"><a href="#3-4-示例" class="headerlink" title="3.4 示例"></a>3.4 示例</h3><p>如下给出一个基本数据类型和复杂数据类型的使用示例：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> students(</span><br><span class="line">  <span class="keyword">name</span>      <span class="keyword">STRING</span>,   <span class="comment">-- 姓名</span></span><br><span class="line">  age       <span class="built_in">INT</span>,      <span class="comment">-- 年龄</span></span><br><span class="line">  subject   <span class="built_in">ARRAY</span>&lt;<span class="keyword">STRING</span>&gt;,   <span class="comment">--学科</span></span><br><span class="line">  score     <span class="keyword">MAP</span>&lt;<span class="keyword">STRING</span>,<span class="built_in">FLOAT</span>&gt;,  <span class="comment">--各个学科考试成绩</span></span><br><span class="line">  address   <span class="keyword">STRUCT</span>&lt;houseNumber:<span class="built_in">int</span>, street:<span class="keyword">STRING</span>, city:<span class="keyword">STRING</span>, province：<span class="keyword">STRING</span>&gt;  <span class="comment">--家庭居住地址</span></span><br><span class="line">) <span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">"\t"</span>;</span><br></pre></td></tr></table></figure>
<h2 id="四、内容格式"><a href="#四、内容格式" class="headerlink" title="四、内容格式"></a>四、内容格式</h2><p>当数据存储在文本文件中，必须按照一定格式区别行和列，如使用逗号作为分隔符的 CSV 文件 (Comma-Separated Values) 或者使用制表符作为分隔值的 TSV 文件 (Tab-Separated Values)。但此时也存在一个缺点，就是正常的文件内容中也可能出现逗号或者制表符。</p>
<p>所以 Hive 默认使用了几个平时很少出现的字符，这些字符一般不会作为内容出现在文件中。Hive 默认的行和列分隔符如下表所示。</p>
<table>
<thead>
<tr>
<th>分隔符</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>\n</strong></td>
<td>对于文本文件来说，每行是一条记录，所以可以使用换行符来分割记录</td>
</tr>
<tr>
<td><strong>^A (Ctrl+A)</strong></td>
<td>分割字段 (列)，在 CREATE TABLE 语句中也可以使用八进制编码 <code>\001</code> 来表示</td>
</tr>
<tr>
<td><strong>^B</strong></td>
<td>用于分割 ARRAY 或者 STRUCT 中的元素，或者用于 MAP 中键值对之间的分割，<br>在 CREATE TABLE 语句中也可以使用八进制编码 <code>\002</code> 表示</td>
</tr>
<tr>
<td><strong>^C</strong></td>
<td>用于 MAP 中键和值之间的分割，在 CREATE TABLE 语句中也可以使用八进制编码 <code>\003</code> 表示</td>
</tr>
</tbody>
</table>
<p>使用示例如下：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> page_view(viewTime <span class="built_in">INT</span>, userid <span class="built_in">BIGINT</span>)</span><br><span class="line"> <span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span></span><br><span class="line">   <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">'\001'</span></span><br><span class="line">   COLLECTION ITEMS <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">'\002'</span></span><br><span class="line">   <span class="keyword">MAP</span> <span class="keyword">KEYS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">'\003'</span></span><br><span class="line"> <span class="keyword">STORED</span> <span class="keyword">AS</span> SEQUENCEFILE;</span><br></pre></td></tr></table></figure>
<h2 id="五、存储格式"><a href="#五、存储格式" class="headerlink" title="五、存储格式"></a>五、存储格式</h2><h3 id="5-1-支持的存储格式"><a href="#5-1-支持的存储格式" class="headerlink" title="5.1 支持的存储格式"></a>5.1 支持的存储格式</h3><p>Hive 会在 HDFS 为每个数据库上创建一个目录，数据库中的表是该目录的子目录，表中的数据会以文件的形式存储在对应的表目录下。Hive 支持以下几种文件存储格式：</p>
<table>
<thead>
<tr>
<th>格式</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>TextFile</strong></td>
<td>存储为纯文本文件。 这是 Hive 默认的文件存储格式。这种存储方式数据不做压缩，磁盘开销大，数据解析开销大。</td>
</tr>
<tr>
<td><strong>SequenceFile</strong></td>
<td>SequenceFile 是 Hadoop API 提供的一种二进制文件，它将数据以&lt;key,value&gt;的形式序列化到文件中。这种二进制文件内部使用 Hadoop 的标准的 Writable 接口实现序列化和反序列化。它与 Hadoop API 中的 MapFile 是互相兼容的。Hive 中的 SequenceFile 继承自 Hadoop API 的 SequenceFile，不过它的 key 为空，使用 value 存放实际的值，这样是为了避免 MR 在运行 map 阶段进行额外的排序操作。</td>
</tr>
<tr>
<td><strong>RCFile</strong></td>
<td>RCFile 文件格式是 FaceBook 开源的一种 Hive 的文件存储格式，首先将表分为几个行组，对每个行组内的数据按列存储，每一列的数据都是分开存储。</td>
</tr>
<tr>
<td><strong>ORC Files</strong></td>
<td>ORC 是在一定程度上扩展了 RCFile，是对 RCFile 的优化。</td>
</tr>
<tr>
<td><strong>Avro Files</strong></td>
<td>Avro 是一个数据序列化系统，设计用于支持大批量数据交换的应用。它的主要特点有：支持二进制序列化方式，可以便捷，快速地处理大量数据；动态语言友好，Avro 提供的机制使动态语言可以方便地处理 Avro 数据。</td>
</tr>
<tr>
<td><strong>Parquet</strong></td>
<td>Parquet 是基于 Dremel 的数据模型和算法实现的，面向分析型业务的列式存储格式。它通过按列进行高效压缩和特殊的编码技术，从而在降低存储空间的同时提高了 IO 效率。</td>
</tr>
</tbody>
</table>
<blockquote>
<p>以上压缩格式中 ORC 和 Parquet 的综合性能突出，使用较为广泛，推荐使用这两种格式。</p>
</blockquote>
<h3 id="5-2-指定存储格式"><a href="#5-2-指定存储格式" class="headerlink" title="5.2 指定存储格式"></a>5.2 指定存储格式</h3><p>通常在创建表的时候使用 <code>STORED AS</code> 参数指定：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> page_view(viewTime <span class="built_in">INT</span>, userid <span class="built_in">BIGINT</span>)</span><br><span class="line"> <span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span></span><br><span class="line">   <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">'\001'</span></span><br><span class="line">   COLLECTION ITEMS <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">'\002'</span></span><br><span class="line">   <span class="keyword">MAP</span> <span class="keyword">KEYS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">'\003'</span></span><br><span class="line"> <span class="keyword">STORED</span> <span class="keyword">AS</span> SEQUENCEFILE;</span><br></pre></td></tr></table></figure>
<p>各个存储文件类型指定方式如下：</p>
<ul>
<li>STORED AS TEXTFILE</li>
<li>STORED AS SEQUENCEFILE</li>
<li>STORED AS ORC</li>
<li>STORED AS PARQUET</li>
<li>STORED AS AVRO</li>
<li>STORED AS RCFILE</li>
</ul>
<h2 id="六、内部表和外部表"><a href="#六、内部表和外部表" class="headerlink" title="六、内部表和外部表"></a>六、内部表和外部表</h2><p>内部表又叫做管理表 (Managed/Internal Table)，创建表时不做任何指定，默认创建的就是内部表。想要创建外部表 (External Table)，则需要使用 External 进行修饰。 内部表和外部表主要区别如下：</p>
<table>
<thead>
<tr>
<th></th>
<th>内部表</th>
<th>外部表</th>
</tr>
</thead>
<tbody>
<tr>
<td>数据存储位置</td>
<td>内部表数据存储的位置由 hive.metastore.warehouse.dir 参数指定，默认情况下表的数据存储在 HDFS 的 <code>/user/hive/warehouse/数据库名.db/表名/</code>  目录下</td>
<td>外部表数据的存储位置创建表时由 <code>Location</code> 参数指定；</td>
</tr>
<tr>
<td>导入数据</td>
<td>在导入数据到内部表，内部表将数据移动到自己的数据仓库目录下，数据的生命周期由 Hive 来进行管理</td>
<td>外部表不会将数据移动到自己的数据仓库目录下，只是在元数据中存储了数据的位置</td>
</tr>
<tr>
<td>删除表</td>
<td>删除元数据（metadata）和文件</td>
<td>只删除元数据（metadata）</td>
</tr>
</tbody>
</table>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol>
<li><a href="https://cwiki.apache.org/confluence/display/Hive/GettingStarted" target="_blank" rel="noopener">Hive Getting Started</a></li>
<li><a href="https://tech.meituan.com/2014/02/12/hive-sql-to-mapreduce.html" target="_blank" rel="noopener">Hive SQL 的编译过程</a></li>
<li><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL" target="_blank" rel="noopener">LanguageManual DDL</a></li>
<li><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Types" target="_blank" rel="noopener">LanguageManual Types</a></li>
<li><a href="https://cwiki.apache.org/confluence/display/Hive/Managed+vs.+External+Tables" target="_blank" rel="noopener">Managed vs. External Tables</a></li>
</ol>
]]></content>
      <categories>
        <category>notes</category>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title>深入理解Kafka副本机制</title>
    <url>/2021/03/17/Kafka%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3%E5%88%86%E5%8C%BA%E5%89%AF%E6%9C%AC%E6%9C%BA%E5%88%B6/</url>
    <content><![CDATA[<h2 id="一、Kafka集群"><a href="#一、Kafka集群" class="headerlink" title="一、Kafka集群"></a>一、Kafka集群</h2><p>Kafka 使用 Zookeeper 来维护集群成员 (brokers) 的信息。每个 broker 都有一个唯一标识 <code>broker.id</code>，用于标识自己在集群中的身份，可以在配置文件 <code>server.properties</code> 中进行配置，或者由程序自动生成。下面是 Kafka brokers 集群自动创建的过程：</p>
<ul>
<li>每一个 broker 启动的时候，它会在 Zookeeper 的 <code>/brokers/ids</code> 路径下创建一个 <code>临时节点</code>，并将自己的 <code>broker.id</code> 写入，从而将自身注册到集群；</li>
<li>当有多个 broker 时，所有 broker 会竞争性地在 Zookeeper 上创建 <code>/controller</code> 节点，由于 Zookeeper 上的节点不会重复，所以必然只会有一个 broker 创建成功，此时该 broker 称为 controller broker。它除了具备其他 broker 的功能外，<strong>还负责管理主题分区及其副本的状态</strong>。</li>
<li>当 broker 出现宕机或者主动退出从而导致其持有的 Zookeeper 会话超时时，会触发注册在 Zookeeper 上的 watcher 事件，此时 Kafka 会进行相应的容错处理；如果宕机的是 controller broker 时，还会触发新的 controller 选举。</li>
</ul>
<h2 id="二、副本机制"><a href="#二、副本机制" class="headerlink" title="二、副本机制"></a>二、副本机制</h2><p>为了保证高可用，kafka 的分区是多副本的，如果一个副本丢失了，那么还可以从其他副本中获取分区数据。但是这要求对应副本的数据必须是完整的，这是 Kafka 数据一致性的基础，所以才需要使用 <code>controller broker</code> 来进行专门的管理。下面将详解介绍 Kafka 的副本机制。</p>
<h3 id="2-1-分区和副本"><a href="#2-1-分区和副本" class="headerlink" title="2.1 分区和副本"></a>2.1 分区和副本</h3><p>Kafka 的主题被分为多个分区 ，分区是 Kafka 最基本的存储单位。每个分区可以有多个副本 (可以在创建主题时使用 <code>replication-factor</code> 参数进行指定)。其中一个副本是首领副本 (Leader replica)，所有的事件都直接发送给首领副本；其他副本是跟随者副本 (Follower replica)，需要通过复制来保持与首领副本数据一致，当首领副本不可用时，其中一个跟随者副本将成为新首领。 </p>
<div align="center"> <img src="../pictures/kafka-cluster.png"> </div>

<h3 id="2-2-ISR机制"><a href="#2-2-ISR机制" class="headerlink" title="2.2 ISR机制"></a>2.2 ISR机制</h3><p>每个分区都有一个 ISR(in-sync Replica) 列表，用于维护所有同步的、可用的副本。首领副本必然是同步副本，而对于跟随者副本来说，它需要满足以下条件才能被认为是同步副本：</p>
<ul>
<li>与 Zookeeper 之间有一个活跃的会话，即必须定时向 Zookeeper 发送心跳；</li>
<li>在规定的时间内从首领副本那里低延迟地获取过消息。</li>
</ul>
<p>如果副本不满足上面条件的话，就会被从 ISR 列表中移除，直到满足条件才会被再次加入。</p>
<p>这里给出一个主题创建的示例：使用 <code>--replication-factor</code> 指定副本系数为 3，创建成功后使用 <code>--describe</code> 命令可以看到分区 0 的有 0,1,2 三个副本，且三个副本都在 ISR 列表中，其中 1 为首领副本。</p>
<div align="center"> <img src="../pictures/kafka-分区副本.png"> </div>

<h3 id="2-3-不完全的首领选举"><a href="#2-3-不完全的首领选举" class="headerlink" title="2.3 不完全的首领选举"></a>2.3 不完全的首领选举</h3><p>对于副本机制，在 broker 级别有一个可选的配置参数 <code>unclean.leader.election.enable</code>，默认值为 fasle，代表禁止不完全的首领选举。这是针对当首领副本挂掉且 ISR 中没有其他可用副本时，是否允许某个不完全同步的副本成为首领副本，这可能会导致数据丢失或者数据不一致，在某些对数据一致性要求较高的场景 (如金融领域)，这可能无法容忍的，所以其默认值为 false，如果你能够允许部分数据不一致的话，可以配置为 true。</p>
<h3 id="2-4-最少同步副本"><a href="#2-4-最少同步副本" class="headerlink" title="2.4 最少同步副本"></a>2.4 最少同步副本</h3><p>ISR 机制的另外一个相关参数是 <code>min.insync.replicas</code> , 可以在 broker 或者主题级别进行配置，代表 ISR 列表中至少要有几个可用副本。这里假设设置为 2，那么当可用副本数量小于该值时，就认为整个分区处于不可用状态。此时客户端再向分区写入数据时候就会抛出异常 <code>org.apache.kafka.common.errors.NotEnoughReplicasExceptoin: Messages are rejected since there are fewer in-sync replicas than required。</code></p>
<h3 id="2-5-发送确认"><a href="#2-5-发送确认" class="headerlink" title="2.5 发送确认"></a>2.5 发送确认</h3><p>Kafka 在生产者上有一个可选的参数 ack，该参数指定了必须要有多少个分区副本收到消息，生产者才会认为消息写入成功：</p>
<ul>
<li><strong>acks=0</strong> ：消息发送出去就认为已经成功了，不会等待任何来自服务器的响应；</li>
<li><strong>acks=1</strong> ： 只要集群的首领节点收到消息，生产者就会收到一个来自服务器成功响应；</li>
<li><strong>acks=all</strong> ：只有当所有参与复制的节点全部收到消息时，生产者才会收到一个来自服务器的成功响应。</li>
</ul>
<h2 id="三、数据请求"><a href="#三、数据请求" class="headerlink" title="三、数据请求"></a>三、数据请求</h2><h3 id="3-1-元数据请求机制"><a href="#3-1-元数据请求机制" class="headerlink" title="3.1 元数据请求机制"></a>3.1 元数据请求机制</h3><p>在所有副本中，只有领导副本才能进行消息的读写处理。由于不同分区的领导副本可能在不同的 broker 上，如果某个 broker 收到了一个分区请求，但是该分区的领导副本并不在该 broker 上，那么它就会向客户端返回一个 <code>Not a Leader for Partition</code> 的错误响应。 为了解决这个问题，Kafka 提供了元数据请求机制。</p>
<p>首先集群中的每个 broker 都会缓存所有主题的分区副本信息，客户端会定期发送发送元数据请求，然后将获取的元数据进行缓存。定时刷新元数据的时间间隔可以通过为客户端配置 <code>metadata.max.age.ms</code> 来进行指定。有了元数据信息后，客户端就知道了领导副本所在的 broker，之后直接将读写请求发送给对应的 broker 即可。</p>
<p>如果在定时请求的时间间隔内发生的分区副本的选举，则意味着原来缓存的信息可能已经过时了，此时还有可能会收到 <code>Not a Leader for Partition</code> 的错误响应，这种情况下客户端会再次求发出元数据请求，然后刷新本地缓存，之后再去正确的 broker 上执行对应的操作，过程如下图：</p>
<div align="center"> <img src="../pictures/kafka-元数据请求.png"> </div>

<h3 id="3-2-数据可见性"><a href="#3-2-数据可见性" class="headerlink" title="3.2 数据可见性"></a>3.2 数据可见性</h3><p>需要注意的是，并不是所有保存在分区首领上的数据都可以被客户端读取到，为了保证数据一致性，只有被所有同步副本 (ISR 中所有副本) 都保存了的数据才能被客户端读取到。</p>
<div align="center"> <img src="../pictures/kafka-数据可见性.png"> </div>

<h3 id="3-3-零拷贝"><a href="#3-3-零拷贝" class="headerlink" title="3.3 零拷贝"></a>3.3 零拷贝</h3><p>Kafka 所有数据的写入和读取都是通过零拷贝来实现的。传统拷贝与零拷贝的区别如下：</p>
<h4 id="传统模式下的四次拷贝与四次上下文切换"><a href="#传统模式下的四次拷贝与四次上下文切换" class="headerlink" title="传统模式下的四次拷贝与四次上下文切换"></a>传统模式下的四次拷贝与四次上下文切换</h4><p>以将磁盘文件通过网络发送为例。传统模式下，一般使用如下伪代码所示的方法先将文件数据读入内存，然后通过 Socket 将内存中的数据发送出去。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">buffer = File.read</span><br><span class="line">Socket.send(buffer)</span><br></pre></td></tr></table></figure>
<p>这一过程实际上发生了四次数据拷贝。首先通过系统调用将文件数据读入到内核态 Buffer（DMA 拷贝），然后应用程序将内存态 Buffer 数据读入到用户态 Buffer（CPU 拷贝），接着用户程序通过 Socket 发送数据时将用户态 Buffer 数据拷贝到内核态 Buffer（CPU 拷贝），最后通过 DMA 拷贝将数据拷贝到 NIC Buffer。同时，还伴随着四次上下文切换，如下图所示：</p>
<div align="center"> <img src="../pictures/kafka-BIO.png"> </div>

<h4 id="sendfile和transferTo实现零拷贝"><a href="#sendfile和transferTo实现零拷贝" class="headerlink" title="sendfile和transferTo实现零拷贝"></a>sendfile和transferTo实现零拷贝</h4><p>Linux 2.4+ 内核通过 <code>sendfile</code> 系统调用，提供了零拷贝。数据通过 DMA 拷贝到内核态 Buffer 后，直接通过 DMA 拷贝到 NIC Buffer，无需 CPU 拷贝。这也是零拷贝这一说法的来源。除了减少数据拷贝外，因为整个读文件到网络发送由一个 <code>sendfile</code> 调用完成，整个过程只有两次上下文切换，因此大大提高了性能。零拷贝过程如下图所示：</p>
<div align="center"> <img src="../pictures/kafka-零拷贝.png"> </div>

<p>从具体实现来看，Kafka 的数据传输通过 TransportLayer 来完成，其子类 <code>PlaintextTransportLayer</code> 的 <code>transferFrom</code> 方法通过调用 Java NIO 中 FileChannel 的 <code>transferTo</code> 方法实现零拷贝，如下所示：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">transferFrom</span><span class="params">(FileChannel fileChannel, <span class="keyword">long</span> position, <span class="keyword">long</span> count)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> fileChannel.transferTo(position, count, socketChannel);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>注：</strong> <code>transferTo</code> 和 <code>transferFrom</code> 并不保证一定能使用零拷贝。实际上是否能使用零拷贝与操作系统相关，如果操作系统提供 <code>sendfile</code> 这样的零拷贝系统调用，则这两个方法会通过这样的系统调用充分利用零拷贝的优势，否则并不能通过这两个方法本身实现零拷贝。</p>
<h2 id="四、物理存储"><a href="#四、物理存储" class="headerlink" title="四、物理存储"></a>四、物理存储</h2><h3 id="4-1-分区分配"><a href="#4-1-分区分配" class="headerlink" title="4.1 分区分配"></a>4.1 分区分配</h3><p>在创建主题时，Kafka 会首先决定如何在 broker 间分配分区副本，它遵循以下原则：</p>
<ul>
<li>在所有 broker 上均匀地分配分区副本；</li>
<li>确保分区的每个副本分布在不同的 broker 上；</li>
<li>如果使用了 <code>broker.rack</code> 参数为 broker 指定了机架信息，那么会尽可能的把每个分区的副本分配到不同机架的 broker 上，以避免一个机架不可用而导致整个分区不可用。</li>
</ul>
<p>基于以上原因，如果你在一个单节点上创建一个 3 副本的主题，通常会抛出下面的异常：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Error while executing topic command : org.apache.kafka.common.errors.InvalidReplicationFactor   </span><br><span class="line">Exception: Replication factor: 3 larger than available brokers: 1.</span><br></pre></td></tr></table></figure>
<h3 id="4-2-分区数据保留规则"><a href="#4-2-分区数据保留规则" class="headerlink" title="4.2 分区数据保留规则"></a>4.2 分区数据保留规则</h3><p>保留数据是 Kafka 的一个基本特性， 但是 Kafka 不会一直保留数据，也不会等到所有消费者都读取了消息之后才删除消息。相反， Kafka 为每个主题配置了数据保留期限，规定数据被删除之前可以保留多长时间，或者清理数据之前可以保留的数据量大小。分别对应以下四个参数： </p>
<ul>
<li><code>log.retention.bytes</code> ：删除数据前允许的最大数据量；默认值-1，代表没有限制；</li>
<li><code>log.retention.ms</code>：保存数据文件的毫秒数，如果未设置，则使用 <code>log.retention.minutes</code> 中的值，默认为 null；</li>
<li><code>log.retention.minutes</code>：保留数据文件的分钟数，如果未设置，则使用 <code>log.retention.hours</code> 中的值，默认为 null；</li>
<li><code>log.retention.hours</code>：保留数据文件的小时数，默认值为 168，也就是一周。</li>
</ul>
<p>因为在一个大文件里查找和删除消息是很费时的，也很容易出错，所以 Kafka 把分区分成若干个片段，当前正在写入数据的片段叫作活跃片段。活动片段永远不会被删除。如果按照默认值保留数据一周，而且每天使用一个新片段，那么你就会看到，在每天使用一个新片段的同时会删除一个最老的片段，所以大部分时间该分区会有 7 个片段存在。 </p>
<h3 id="4-3-文件格式"><a href="#4-3-文件格式" class="headerlink" title="4.3 文件格式"></a>4.3 文件格式</h3><p>通常保存在磁盘上的数据格式与生产者发送过来消息格式是一样的。 如果生产者发送的是压缩过的消息，那么同一个批次的消息会被压缩在一起，被当作“包装消息”进行发送 (格式如下所示) ，然后保存到磁盘上。之后消费者读取后再自己解压这个包装消息，获取每条消息的具体信息。</p>
<div align="center"> <img src="../pictures/kafka-compress-message.png"> </div>



<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol>
<li>Neha Narkhede, Gwen Shapira ,Todd Palino(著) , 薛命灯 (译) . Kafka 权威指南 . 人民邮电出版社 . 2017-12-26</li>
<li><a href="http://www.jasongj.com/kafka/high_throughput/" target="_blank" rel="noopener">Kafka 高性能架构之道</a></li>
</ol>
]]></content>
      <categories>
        <category>技术</category>
        <category>kafka</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title>Kafka简介</title>
    <url>/2021/03/17/Kafka%E7%AE%80%E4%BB%8B/</url>
    <content><![CDATA[<h2 id="一、简介"><a href="#一、简介" class="headerlink" title="一、简介"></a>一、简介</h2><p>ApacheKafka 是一个分布式的流处理平台。它具有以下特点：</p>
<ul>
<li>支持消息的发布和订阅，类似于 RabbtMQ、ActiveMQ 等消息队列；</li>
<li>支持数据实时处理；</li>
<li>能保证消息的可靠性投递；</li>
<li>支持消息的持久化存储，并通过多副本分布式的存储方案来保证消息的容错；</li>
<li>高吞吐率，单 Broker 可以轻松处理数千个分区以及每秒百万级的消息量。</li>
</ul>
<h2 id="二、基本概念"><a href="#二、基本概念" class="headerlink" title="二、基本概念"></a>二、基本概念</h2><h3 id="2-1-Messages-And-Batches"><a href="#2-1-Messages-And-Batches" class="headerlink" title="2.1 Messages And Batches"></a>2.1 Messages And Batches</h3><p>Kafka 的基本数据单元被称为 message(消息)，为减少网络开销，提高效率，多个消息会被放入同一批次 (Batch) 中后再写入。</p>
<h3 id="2-2-Topics-And-Partitions"><a href="#2-2-Topics-And-Partitions" class="headerlink" title="2.2 Topics And Partitions"></a>2.2 Topics And Partitions</h3><p>Kafka 的消息通过 Topics(主题) 进行分类，一个主题可以被分为若干个 Partitions(分区)，一个分区就是一个提交日志 (commit log)。消息以追加的方式写入分区，然后以先入先出的顺序读取。Kafka 通过分区来实现数据的冗余和伸缩性，分区可以分布在不同的服务器上，这意味着一个 Topic 可以横跨多个服务器，以提供比单个服务器更强大的性能。</p>
<p>由于一个 Topic 包含多个分区，因此无法在整个 Topic 范围内保证消息的顺序性，但可以保证消息在单个分区内的顺序性。</p>
<div align="center"> <img src="../pictures/kafka-topic.png"> </div>

<h3 id="2-3-Producers-And-Consumers"><a href="#2-3-Producers-And-Consumers" class="headerlink" title="2.3 Producers And Consumers"></a>2.3 Producers And Consumers</h3><h4 id="1-生产者"><a href="#1-生产者" class="headerlink" title="1. 生产者"></a>1. 生产者</h4><p>生产者负责创建消息。一般情况下，生产者在把消息均衡地分布到在主题的所有分区上，而并不关心消息会被写到哪个分区。如果我们想要把消息写到指定的分区，可以通过自定义分区器来实现。</p>
<h4 id="2-消费者"><a href="#2-消费者" class="headerlink" title="2. 消费者"></a>2. 消费者</h4><p>消费者是消费者群组的一部分，消费者负责消费消息。消费者可以订阅一个或者多个主题，并按照消息生成的顺序来读取它们。消费者通过检查消息的偏移量 (offset) 来区分读取过的消息。偏移量是一个不断递增的数值，在创建消息时，Kafka 会把它添加到其中，在给定的分区里，每个消息的偏移量都是唯一的。消费者把每个分区最后读取的偏移量保存在 Zookeeper 或 Kafka 上，如果消费者关闭或者重启，它还可以重新获取该偏移量，以保证读取状态不会丢失。</p>
<div align="center"> <img src="../pictures/kafka-producer-consumer.png"> </div>

<p>一个分区只能被同一个消费者群组里面的一个消费者读取，但可以被不同消费者群组中所组成的多个消费者共同读取。多个消费者群组中消费者共同读取同一个主题时，彼此之间互不影响。</p>
<div align="center"> <img src="../pictures/kafka消费者.png"> </div>

<h3 id="2-4-Brokers-And-Clusters"><a href="#2-4-Brokers-And-Clusters" class="headerlink" title="2.4 Brokers And Clusters"></a>2.4 Brokers And Clusters</h3><p>一个独立的 Kafka 服务器被称为 Broker。Broker 接收来自生产者的消息，为消息设置偏移量，并提交消息到磁盘保存。Broker 为消费者提供服务，对读取分区的请求做出响应，返回已经提交到磁盘的消息。</p>
<p>Broker 是集群 (Cluster) 的组成部分。每一个集群都会选举出一个 Broker 作为集群控制器 (Controller)，集群控制器负责管理工作，包括将分区分配给 Broker 和监控 Broker。</p>
<p>在集群中，一个分区 (Partition) 从属一个 Broker，该 Broker 被称为分区的首领 (Leader)。一个分区可以分配给多个 Brokers，这个时候会发生分区复制。这种复制机制为分区提供了消息冗余，如果有一个 Broker 失效，其他 Broker 可以接管领导权。</p>
<div align="center"> <img src="../pictures/kafka-cluster.png"> </div>



<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>Neha Narkhede, Gwen Shapira ,Todd Palino(著) , 薛命灯 (译) . Kafka 权威指南 . 人民邮电出版社 . 2017-12-26</p>
]]></content>
      <categories>
        <category>技术</category>
        <category>kafka</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title>Kafka生产者详解</title>
    <url>/2021/03/17/Kafka%E7%94%9F%E4%BA%A7%E8%80%85%E8%AF%A6%E8%A7%A3/</url>
    <content><![CDATA[<h2 id="一、生产者发送消息的过程"><a href="#一、生产者发送消息的过程" class="headerlink" title="一、生产者发送消息的过程"></a>一、生产者发送消息的过程</h2><p>首先介绍一下 Kafka 生产者发送消息的过程：</p>
<ul>
<li>Kafka 会将发送消息包装为 ProducerRecord 对象， ProducerRecord 对象包含了目标主题和要发送的内容，同时还可以指定键和分区。在发送 ProducerRecord 对象前，生产者会先把键和值对象序列化成字节数组，这样它们才能够在网络上传输。</li>
<li>接下来，数据被传给分区器。如果之前已经在 ProducerRecord 对象里指定了分区，那么分区器就不会再做任何事情。如果没有指定分区 ，那么分区器会根据 ProducerRecord 对象的键来选择一个分区，紧接着，这条记录被添加到一个记录批次里，这个批次里的所有消息会被发送到相同的主题和分区上。有一个独立的线程负责把这些记录批次发送到相应的 broker 上。</li>
<li>服务器在收到这些消息时会返回一个响应。如果消息成功写入 Kafka，就返回一个 RecordMetaData 对象，它包含了主题和分区信息，以及记录在分区里的偏移量。如果写入失败，则会返回一个错误。生产者在收到错误之后会尝试重新发送消息，如果达到指定的重试次数后还没有成功，则直接抛出异常，不再重试。</li>
</ul>
<div align="center"> <img src="../pictures/kafka-send-messgaes.png"> </div>

<h2 id="二、创建生产者"><a href="#二、创建生产者" class="headerlink" title="二、创建生产者"></a>二、创建生产者</h2><h3 id="2-1-项目依赖"><a href="#2-1-项目依赖" class="headerlink" title="2.1 项目依赖"></a>2.1 项目依赖</h3><p>本项目采用 Maven 构建，想要调用 Kafka 生产者 API，需要导入 <code>kafka-clients</code> 依赖，如下：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.kafka<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>kafka-clients<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.2.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h3 id="2-2-创建生产者"><a href="#2-2-创建生产者" class="headerlink" title="2.2 创建生产者"></a>2.2 创建生产者</h3><p>创建 Kafka 生产者时，以下三个属性是必须指定的：</p>
<ul>
<li><strong>bootstrap.servers</strong> ：指定 broker 的地址清单，清单里不需要包含所有的 broker 地址，生产者会从给定的 broker 里查找 broker 的信息。不过建议至少要提供两个 broker 的信息作为容错；</li>
<li><strong>key.serializer</strong> ：指定键的序列化器；</li>
<li><strong>value.serializer</strong> ：指定值的序列化器。</li>
</ul>
<p>创建的示例代码如下：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">public <span class="class"><span class="keyword">class</span> <span class="title">SimpleProducer</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    public static void main(<span class="type">String</span>[] args) &#123;</span><br><span class="line"></span><br><span class="line">        <span class="type">String</span> topicName = <span class="string">"Hello-Kafka"</span>;</span><br><span class="line"></span><br><span class="line">        <span class="type">Properties</span> props = <span class="keyword">new</span> <span class="type">Properties</span>();</span><br><span class="line">        props.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"hadoop001:9092"</span>);</span><br><span class="line">        props.put(<span class="string">"key.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line">        props.put(<span class="string">"value.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line">        <span class="comment">/*创建生产者*/</span></span><br><span class="line">        <span class="type">Producer</span>&lt;<span class="type">String</span>, <span class="type">String</span>&gt; producer = <span class="keyword">new</span> <span class="type">KafkaProducer</span>&lt;&gt;(props);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (int i = <span class="number">0</span>; i &lt; <span class="number">10</span>; i++) &#123;</span><br><span class="line">            <span class="type">ProducerRecord</span>&lt;<span class="type">String</span>, <span class="type">String</span>&gt; record = <span class="keyword">new</span> <span class="type">ProducerRecord</span>&lt;&gt;(topicName, <span class="string">"hello"</span> + i, </span><br><span class="line">                                                                         <span class="string">"world"</span> + i);</span><br><span class="line">            <span class="comment">/* 发送消息*/</span></span><br><span class="line">            producer.send(record);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">/*关闭生产者*/</span></span><br><span class="line">        producer.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>本篇文章的所有示例代码可以从 Github 上进行下载：<a href="https://github.com/heibaiying/BigData-Notes/tree/master/code/Kafka/kafka-basis" target="_blank" rel="noopener">kafka-basis</a></p>
</blockquote>
<h3 id="2-3-测试"><a href="#2-3-测试" class="headerlink" title="2.3 测试"></a>2.3 测试</h3><h4 id="1-启动Kakfa"><a href="#1-启动Kakfa" class="headerlink" title="1. 启动Kakfa"></a>1. 启动Kakfa</h4><p>Kafka 的运行依赖于 zookeeper，需要预先启动，可以启动 Kafka 内置的 zookeeper，也可以启动自己安装的：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> zookeeper启动命令</span></span><br><span class="line">bin/zkServer.sh start</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 内置zookeeper启动命令</span></span><br><span class="line">bin/zookeeper-server-start.sh config/zookeeper.properties</span><br></pre></td></tr></table></figure>
<p>启动单节点 kafka 用于测试：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> bin/kafka-server-start.sh config/server.properties</span></span><br></pre></td></tr></table></figure>
<h4 id="2-创建topic"><a href="#2-创建topic" class="headerlink" title="2. 创建topic"></a>2. 创建topic</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 创建用于测试主题</span></span><br><span class="line">bin/kafka-topics.sh --create \</span><br><span class="line">                    --bootstrap-server hadoop001:9092 \</span><br><span class="line">                     --replication-factor 1 --partitions 1 \</span><br><span class="line">                     --topic Hello-Kafka</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看所有主题</span></span><br><span class="line"> bin/kafka-topics.sh --list --bootstrap-server hadoop001:9092</span><br></pre></td></tr></table></figure>
<h4 id="3-启动消费者"><a href="#3-启动消费者" class="headerlink" title="3. 启动消费者"></a>3. 启动消费者</h4><p> 启动一个控制台消费者用于观察写入情况，启动命令如下：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> bin/kafka-console-consumer.sh --bootstrap-server hadoop001:9092 --topic Hello-Kafka --from-beginning</span></span><br></pre></td></tr></table></figure>
<h4 id="4-运行项目"><a href="#4-运行项目" class="headerlink" title="4. 运行项目"></a>4. 运行项目</h4><p>此时可以看到消费者控制台，输出如下，这里 <code>kafka-console-consumer</code> 只会打印出值信息，不会打印出键信息。</p>
<div align="center"> <img src="../pictures/kafka-simple-producer.png"> </div>



<h3 id="2-4-可能出现的问题"><a href="#2-4-可能出现的问题" class="headerlink" title="2.4 可能出现的问题"></a>2.4 可能出现的问题</h3><p>在这里可能出现的一个问题是：生产者程序在启动后，一直处于等待状态。这通常出现在你使用默认配置启动 Kafka 的情况下，此时需要对 <code>server.properties</code> 文件中的 <code>listeners</code> 配置进行更改：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> hadoop001 为我启动kafka服务的主机名，你可以换成自己的主机名或者ip地址</span></span><br><span class="line">listeners=PLAINTEXT://hadoop001:9092</span><br></pre></td></tr></table></figure>
<h2 id="二、发送消息"><a href="#二、发送消息" class="headerlink" title="二、发送消息"></a>二、发送消息</h2><p>上面的示例程序调用了 <code>send</code> 方法发送消息后没有做任何操作，在这种情况下，我们没有办法知道消息发送的结果。想要知道消息发送的结果，可以使用同步发送或者异步发送来实现。</p>
<h3 id="2-1-同步发送"><a href="#2-1-同步发送" class="headerlink" title="2.1 同步发送"></a>2.1 同步发送</h3><p>在调用 <code>send</code> 方法后可以接着调用 <code>get()</code> 方法，<code>send</code> 方法的返回值是一个 Future\<recordmetadata>对象，RecordMetadata 里面包含了发送消息的主题、分区、偏移量等信息。改写后的代码如下：</recordmetadata></p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> (int i = <span class="number">0</span>; i &lt; <span class="number">10</span>; i++) &#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="type">ProducerRecord</span>&lt;<span class="type">String</span>, <span class="type">String</span>&gt; record = <span class="keyword">new</span> <span class="type">ProducerRecord</span>&lt;&gt;(topicName, <span class="string">"k"</span> + i, <span class="string">"world"</span> + i);</span><br><span class="line">        <span class="comment">/*同步发送消息*/</span></span><br><span class="line">        <span class="type">RecordMetadata</span> metadata = producer.send(record).get();</span><br><span class="line">        <span class="type">System</span>.out.printf(<span class="string">"topic=%s, partition=%d, offset=%s \n"</span>,</span><br><span class="line">                metadata.topic(), metadata.partition(), metadata.offset());</span><br><span class="line">    &#125; <span class="keyword">catch</span> (<span class="type">InterruptedException</span> | <span class="type">ExecutionException</span> e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>此时得到的输出如下：偏移量和调用次数有关，所有记录都分配到了 0 分区，这是因为在创建 <code>Hello-Kafka</code> 主题时候，使用 <code>--partitions</code> 指定其分区数为 1，即只有一个分区。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">topic=Hello-Kafka, partition=0, offset=40 </span><br><span class="line">topic=Hello-Kafka, partition=0, offset=41 </span><br><span class="line">topic=Hello-Kafka, partition=0, offset=42 </span><br><span class="line">topic=Hello-Kafka, partition=0, offset=43 </span><br><span class="line">topic=Hello-Kafka, partition=0, offset=44 </span><br><span class="line">topic=Hello-Kafka, partition=0, offset=45 </span><br><span class="line">topic=Hello-Kafka, partition=0, offset=46 </span><br><span class="line">topic=Hello-Kafka, partition=0, offset=47 </span><br><span class="line">topic=Hello-Kafka, partition=0, offset=48 </span><br><span class="line">topic=Hello-Kafka, partition=0, offset=49</span><br></pre></td></tr></table></figure>
<h3 id="2-2-异步发送"><a href="#2-2-异步发送" class="headerlink" title="2.2 异步发送"></a>2.2 异步发送</h3><p>通常我们并不关心发送成功的情况，更多关注的是失败的情况，因此 Kafka 提供了异步发送和回调函数。 代码如下：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> (int i = <span class="number">0</span>; i &lt; <span class="number">10</span>; i++) &#123;</span><br><span class="line">    <span class="type">ProducerRecord</span>&lt;<span class="type">String</span>, <span class="type">String</span>&gt; record = <span class="keyword">new</span> <span class="type">ProducerRecord</span>&lt;&gt;(topicName, <span class="string">"k"</span> + i, <span class="string">"world"</span> + i);</span><br><span class="line">    <span class="comment">/*异步发送消息，并监听回调*/</span></span><br><span class="line">    producer.send(record, <span class="keyword">new</span> <span class="type">Callback</span>() &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        public void onCompletion(<span class="type">RecordMetadata</span> metadata, <span class="type">Exception</span> exception) &#123;</span><br><span class="line">            <span class="keyword">if</span> (exception != <span class="literal">null</span>) &#123;</span><br><span class="line">                <span class="type">System</span>.out.println(<span class="string">"进行异常处理"</span>);</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="type">System</span>.out.printf(<span class="string">"topic=%s, partition=%d, offset=%s \n"</span>,</span><br><span class="line">                        metadata.topic(), metadata.partition(), metadata.offset());</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="三、自定义分区器"><a href="#三、自定义分区器" class="headerlink" title="三、自定义分区器"></a>三、自定义分区器</h2><p>Kafka 有着默认的分区机制：</p>
<ul>
<li>如果键值为 null， 则使用轮询 (Round Robin) 算法将消息均衡地分布到各个分区上；</li>
<li>如果键值不为 null，那么 Kafka 会使用内置的散列算法对键进行散列，然后分布到各个分区上。</li>
</ul>
<p>某些情况下，你可能有着自己的分区需求，这时候可以采用自定义分区器实现。这里给出一个自定义分区器的示例：</p>
<h3 id="3-1-自定义分区器"><a href="#3-1-自定义分区器" class="headerlink" title="3.1 自定义分区器"></a>3.1 自定义分区器</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 自定义分区器</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomPartitioner</span> <span class="keyword">implements</span> <span class="title">Partitioner</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> passLine;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(Map&lt;String, ?&gt; configs)</span> </span>&#123;</span><br><span class="line">        <span class="comment">/*从生产者配置中获取分数线*/</span></span><br><span class="line">        passLine = (Integer) configs.get(<span class="string">"pass.line"</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">partition</span><span class="params">(String topic, Object key, <span class="keyword">byte</span>[] keyBytes, Object value, </span></span></span><br><span class="line"><span class="function"><span class="params">                         <span class="keyword">byte</span>[] valueBytes, Cluster cluster)</span> </span>&#123;</span><br><span class="line">        <span class="comment">/*key 值为分数，当分数大于分数线时候，分配到 1 分区，否则分配到 0 分区*/</span></span><br><span class="line">        <span class="keyword">return</span> (Integer) key &gt;= passLine ? <span class="number">1</span> : <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"分区器关闭"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>需要在创建生产者时指定分区器，和分区器所需要的配置参数：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ProducerWithPartitioner</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        String topicName = <span class="string">"Kafka-Partitioner-Test"</span>;</span><br><span class="line"></span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        props.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"hadoop001:9092"</span>);</span><br><span class="line">        props.put(<span class="string">"key.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.IntegerSerializer"</span>);</span><br><span class="line">        props.put(<span class="string">"value.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">/*传递自定义分区器*/</span></span><br><span class="line">        props.put(<span class="string">"partitioner.class"</span>, <span class="string">"com.heibaiying.producers.partitioners.CustomPartitioner"</span>);</span><br><span class="line">        <span class="comment">/*传递分区器所需的参数*/</span></span><br><span class="line">        props.put(<span class="string">"pass.line"</span>, <span class="number">6</span>);</span><br><span class="line"></span><br><span class="line">        Producer&lt;Integer, String&gt; producer = <span class="keyword">new</span> KafkaProducer&lt;&gt;(props);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt;= <span class="number">10</span>; i++) &#123;</span><br><span class="line">            String score = <span class="string">"score:"</span> + i;</span><br><span class="line">            ProducerRecord&lt;Integer, String&gt; record = <span class="keyword">new</span> ProducerRecord&lt;&gt;(topicName, i, score);</span><br><span class="line">            <span class="comment">/*异步发送消息*/</span></span><br><span class="line">            producer.send(record, (metadata, exception) -&gt;</span><br><span class="line">                    System.out.printf(<span class="string">"%s, partition=%d, \n"</span>, score, metadata.partition()));</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        producer.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="3-2-测试"><a href="#3-2-测试" class="headerlink" title="3.2 测试"></a>3.2 测试</h3><p>需要创建一个至少有两个分区的主题：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">bin/kafka-topics.sh --create \</span><br><span class="line">                   --bootstrap-server hadoop001:9092 \</span><br><span class="line">                    --replication-factor 1 --partitions 2 \</span><br><span class="line">                    --topic Kafka-Partitioner-Test</span><br></pre></td></tr></table></figure>
<p>此时输入如下，可以看到分数大于等于 6 分的都被分到 1 分区，而小于 6 分的都被分到了 0 分区。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">score:6, partition=1, </span><br><span class="line">score:7, partition=1, </span><br><span class="line">score:8, partition=1, </span><br><span class="line">score:9, partition=1, </span><br><span class="line">score:10, partition=1, </span><br><span class="line">score:0, partition=0, </span><br><span class="line">score:1, partition=0, </span><br><span class="line">score:2, partition=0, </span><br><span class="line">score:3, partition=0, </span><br><span class="line">score:4, partition=0, </span><br><span class="line">score:5, partition=0, </span><br><span class="line">分区器关闭</span><br></pre></td></tr></table></figure>
<h2 id="四、生产者其他属性"><a href="#四、生产者其他属性" class="headerlink" title="四、生产者其他属性"></a>四、生产者其他属性</h2><p>上面生产者的创建都仅指定了服务地址，键序列化器、值序列化器，实际上 Kafka 的生产者还有很多可配置属性，如下：</p>
<h3 id="1-acks"><a href="#1-acks" class="headerlink" title="1. acks"></a>1. acks</h3><p>acks 参数指定了必须要有多少个分区副本收到消息，生产者才会认为消息写入是成功的：</p>
<ul>
<li><strong>acks=0</strong> ： 消息发送出去就认为已经成功了，不会等待任何来自服务器的响应；</li>
<li><strong>acks=1</strong> ： 只要集群的首领节点收到消息，生产者就会收到一个来自服务器成功响应；</li>
<li><strong>acks=all</strong> ：只有当所有参与复制的节点全部收到消息时，生产者才会收到一个来自服务器的成功响应。</li>
</ul>
<h3 id="2-buffer-memory"><a href="#2-buffer-memory" class="headerlink" title="2. buffer.memory"></a>2. buffer.memory</h3><p>设置生产者内存缓冲区的大小。</p>
<h3 id="3-compression-type"><a href="#3-compression-type" class="headerlink" title="3. compression.type"></a>3. compression.type</h3><p>默认情况下，发送的消息不会被压缩。如果想要进行压缩，可以配置此参数，可选值有 snappy，gzip，lz4。</p>
<h3 id="4-retries"><a href="#4-retries" class="headerlink" title="4. retries"></a>4. retries</h3><p>发生错误后，消息重发的次数。如果达到设定值，生产者就会放弃重试并返回错误。</p>
<h3 id="5-batch-size"><a href="#5-batch-size" class="headerlink" title="5. batch.size"></a>5. batch.size</h3><p>当有多个消息需要被发送到同一个分区时，生产者会把它们放在同一个批次里。该参数指定了一个批次可以使用的内存大小，按照字节数计算。</p>
<h3 id="6-linger-ms"><a href="#6-linger-ms" class="headerlink" title="6. linger.ms"></a>6. linger.ms</h3><p>该参数制定了生产者在发送批次之前等待更多消息加入批次的时间。</p>
<h3 id="7-clent-id"><a href="#7-clent-id" class="headerlink" title="7. clent.id"></a>7. clent.id</h3><p>客户端 id,服务器用来识别消息的来源。</p>
<h3 id="8-max-in-flight-requests-per-connection"><a href="#8-max-in-flight-requests-per-connection" class="headerlink" title="8. max.in.flight.requests.per.connection"></a>8. max.in.flight.requests.per.connection</h3><p>指定了生产者在收到服务器响应之前可以发送多少个消息。它的值越高，就会占用越多的内存，不过也会提升吞吐量，把它设置为 1 可以保证消息是按照发送的顺序写入服务器，即使发生了重试。</p>
<h3 id="9-timeout-ms-request-timeout-ms-amp-metadata-fetch-timeout-ms"><a href="#9-timeout-ms-request-timeout-ms-amp-metadata-fetch-timeout-ms" class="headerlink" title="9. timeout.ms, request.timeout.ms &amp; metadata.fetch.timeout.ms"></a>9. timeout.ms, request.timeout.ms &amp; metadata.fetch.timeout.ms</h3><ul>
<li>timeout.ms 指定了 borker 等待同步副本返回消息的确认时间；</li>
<li>request.timeout.ms 指定了生产者在发送数据时等待服务器返回响应的时间；</li>
<li>metadata.fetch.timeout.ms 指定了生产者在获取元数据（比如分区首领是谁）时等待服务器返回响应的时间。</li>
</ul>
<h3 id="10-max-block-ms"><a href="#10-max-block-ms" class="headerlink" title="10. max.block.ms"></a>10. max.block.ms</h3><p>指定了在调用 <code>send()</code> 方法或使用 <code>partitionsFor()</code> 方法获取元数据时生产者的阻塞时间。当生产者的发送缓冲区已满，或者没有可用的元数据时，这些方法会阻塞。在阻塞时间达到 max.block.ms 时，生产者会抛出超时异常。</p>
<h3 id="11-max-request-size"><a href="#11-max-request-size" class="headerlink" title="11. max.request.size"></a>11. max.request.size</h3><p>该参数用于控制生产者发送的请求大小。它可以指发送的单个消息的最大值，也可以指单个请求里所有消息总的大小。例如，假设这个值为 1000K ，那么可以发送的单个最大消息为 1000K ，或者生产者可以在单个请求里发送一个批次，该批次包含了 1000 个消息，每个消息大小为 1K。 </p>
<h3 id="12-receive-buffer-bytes-amp-send-buffer-byte"><a href="#12-receive-buffer-bytes-amp-send-buffer-byte" class="headerlink" title="12. receive.buffer.bytes &amp; send.buffer.byte"></a>12. receive.buffer.bytes &amp; send.buffer.byte</h3><p>这两个参数分别指定 TCP socket 接收和发送数据包缓冲区的大小，-1 代表使用操作系统的默认值。</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol>
<li>Neha Narkhede, Gwen Shapira ,Todd Palino(著) , 薛命灯 (译) . Kafka 权威指南 . 人民邮电出版社 . 2017-12-26</li>
</ol>
]]></content>
      <categories>
        <category>技术</category>
        <category>kafka</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title>函数和闭包</title>
    <url>/2021/03/18/Scala%E5%87%BD%E6%95%B0%E5%92%8C%E9%97%AD%E5%8C%85/</url>
    <content><![CDATA[<h2 id="一、函数"><a href="#一、函数" class="headerlink" title="一、函数"></a>一、函数</h2><h3 id="1-1-函数与方法"><a href="#1-1-函数与方法" class="headerlink" title="1.1 函数与方法"></a>1.1 函数与方法</h3><p>Scala 中函数与方法的区别非常小，如果函数作为某个对象的成员，这样的函数被称为方法，否则就是一个正常的函数。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 定义方法</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">multi1</span></span>(x:<span class="type">Int</span>) = &#123;x * x&#125;</span><br><span class="line"><span class="comment">// 定义函数</span></span><br><span class="line"><span class="keyword">val</span> multi2 = (x: <span class="type">Int</span>) =&gt; &#123;x * x&#125;</span><br><span class="line"></span><br><span class="line">println(multi1(<span class="number">3</span>)) <span class="comment">//输出 9</span></span><br><span class="line">println(multi2(<span class="number">3</span>)) <span class="comment">//输出 9</span></span><br></pre></td></tr></table></figure>
<p>也可以使用 <code>def</code> 定义函数：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">multi3</span> </span>= (x: <span class="type">Int</span>) =&gt; &#123;x * x&#125;</span><br><span class="line">println(multi3(<span class="number">3</span>))  <span class="comment">//输出 9</span></span><br></pre></td></tr></table></figure>
<p><code>multi2</code> 和 <code>multi3</code> 本质上没有区别，这是因为函数是一等公民，<code>val multi2 = (x: Int) =&gt; {x * x}</code> 这个语句相当于是使用 <code>def</code> 预先定义了函数，之后赋值给变量 <code>multi2</code>。</p>
<h3 id="1-2-函数类型"><a href="#1-2-函数类型" class="headerlink" title="1.2 函数类型"></a>1.2 函数类型</h3><p>上面我们说过 <code>multi2</code> 和 <code>multi3</code> 本质上是一样的，那么作为函数它们是什么类型的？两者的类型实际上都是 <code>Int =&gt; Int</code>，前面一个 Int 代表输入参数类型，后面一个 Int 代表返回值类型。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> multi2 = (x: <span class="type">Int</span>) =&gt; &#123;x * x&#125;</span><br><span class="line">multi2: <span class="type">Int</span> =&gt; <span class="type">Int</span> = $$<span class="type">Lambda</span>$<span class="number">1092</span>/<span class="number">594363215</span>@<span class="number">1</span>dd1a777</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="function"><span class="keyword">def</span> <span class="title">multi3</span> </span>= (x: <span class="type">Int</span>) =&gt; &#123;x * x&#125;</span><br><span class="line">multi3: <span class="type">Int</span> =&gt; <span class="type">Int</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 如果有多个参数，则类型为：（参数类型，参数类型 ...）=&gt;返回值类型</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> multi4 = (x: <span class="type">Int</span>,name: <span class="type">String</span>) =&gt; &#123;name + x * x &#125;</span><br><span class="line">multi4: (<span class="type">Int</span>, <span class="type">String</span>) =&gt; <span class="type">String</span> = $$<span class="type">Lambda</span>$<span class="number">1093</span>/<span class="number">1039732747</span>@<span class="number">2</span>eb4fe7</span><br></pre></td></tr></table></figure>
<h3 id="1-3-一等公民-amp-匿名函数"><a href="#1-3-一等公民-amp-匿名函数" class="headerlink" title="1.3 一等公民&amp;匿名函数"></a>1.3 一等公民&amp;匿名函数</h3><p>在 Scala 中函数是一等公民，这意味着不仅可以定义函数并调用它们，还可以将它们作为值进行传递：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> scala.math.ceil</span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaApp</span> <span class="keyword">extends</span> <span class="title">App</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 将函数 ceil 赋值给变量 fun,使用下划线 (_) 指明是 ceil 函数但不传递参数</span></span><br><span class="line">  <span class="keyword">val</span> fun = ceil _</span><br><span class="line">  println(fun(<span class="number">2.3456</span>))  <span class="comment">//输出 3.0</span></span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在 Scala 中你不必给每一个函数都命名，如 <code>(x: Int) =&gt; 3 * x</code> 就是一个匿名函数：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaApp</span> <span class="keyword">extends</span> <span class="title">App</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 1.匿名函数</span></span><br><span class="line">  (x: <span class="type">Int</span>) =&gt; <span class="number">3</span> * x</span><br><span class="line">  <span class="comment">// 2.具名函数</span></span><br><span class="line">  <span class="keyword">val</span> fun = (x: <span class="type">Int</span>) =&gt; <span class="number">3</span> * x</span><br><span class="line">  <span class="comment">// 3.直接使用匿名函数</span></span><br><span class="line">  <span class="keyword">val</span> array01 = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>).map((x: <span class="type">Int</span>) =&gt; <span class="number">3</span> * x)  </span><br><span class="line">  <span class="comment">// 4.使用占位符简写匿名函数</span></span><br><span class="line">  <span class="keyword">val</span> array02 = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>).map(_ * <span class="number">3</span>)</span><br><span class="line">  <span class="comment">// 5.使用具名函数</span></span><br><span class="line">  <span class="keyword">val</span> array03 = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>).map(fun)</span><br><span class="line">  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="1-4-特殊的函数表达式"><a href="#1-4-特殊的函数表达式" class="headerlink" title="1.4 特殊的函数表达式"></a>1.4 特殊的函数表达式</h3><h4 id="1-可变长度参数列表"><a href="#1-可变长度参数列表" class="headerlink" title="1. 可变长度参数列表"></a>1. 可变长度参数列表</h4><p>在 Java 中如果你想要传递可变长度的参数，需要使用 <code>String ...args</code> 这种形式，Scala 中等效的表达为 <code>args: String*</code>。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaApp</span> <span class="keyword">extends</span> <span class="title">App</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">echo</span></span>(args: <span class="type">String</span>*): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">for</span> (arg &lt;- args) println(arg)</span><br><span class="line">  &#125;</span><br><span class="line">  echo(<span class="string">"spark"</span>,<span class="string">"hadoop"</span>,<span class="string">"flink"</span>)</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 输出</span></span><br><span class="line">spark</span><br><span class="line">hadoop</span><br><span class="line">flink</span><br></pre></td></tr></table></figure>
<h4 id="2-传递具名参数"><a href="#2-传递具名参数" class="headerlink" title="2. 传递具名参数"></a>2. 传递具名参数</h4><p>向函数传递参数时候可以指定具体的参数名。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaApp</span> <span class="keyword">extends</span> <span class="title">App</span> </span>&#123;</span><br><span class="line">  </span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">detail</span></span>(name: <span class="type">String</span>, age: <span class="type">Int</span>): <span class="type">Unit</span> = println(name + <span class="string">":"</span> + age)</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// 1.按照参数定义的顺序传入</span></span><br><span class="line">  detail(<span class="string">"heibaiying"</span>, <span class="number">12</span>)</span><br><span class="line">  <span class="comment">// 2.传递参数的时候指定具体的名称,则不必遵循定义的顺序</span></span><br><span class="line">  detail(age = <span class="number">12</span>, name = <span class="string">"heibaiying"</span>)</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="3-默认值参数"><a href="#3-默认值参数" class="headerlink" title="3. 默认值参数"></a>3. 默认值参数</h4><p>在定义函数时，可以为参数指定默认值。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaApp</span> <span class="keyword">extends</span> <span class="title">App</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">detail</span></span>(name: <span class="type">String</span>, age: <span class="type">Int</span> = <span class="number">88</span>): <span class="type">Unit</span> = println(name + <span class="string">":"</span> + age)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 如果没有传递 age 值,则使用默认值</span></span><br><span class="line">  detail(<span class="string">"heibaiying"</span>)</span><br><span class="line">  detail(<span class="string">"heibaiying"</span>, <span class="number">12</span>)</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="二、闭包"><a href="#二、闭包" class="headerlink" title="二、闭包"></a>二、闭包</h2><h3 id="2-1-闭包的定义"><a href="#2-1-闭包的定义" class="headerlink" title="2.1 闭包的定义"></a>2.1 闭包的定义</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">var</span> more = <span class="number">10</span></span><br><span class="line"><span class="comment">// addMore 一个闭包函数:因为其捕获了自由变量 more 从而闭合了该函数字面量</span></span><br><span class="line"><span class="keyword">val</span> addMore = (x: <span class="type">Int</span>) =&gt; x + more</span><br></pre></td></tr></table></figure>
<p>如上函数 <code>addMore</code> 中有两个变量 x 和 more:</p>
<ul>
<li><strong>x</strong> : 是一个绑定变量 (bound variable)，因为其是该函数的入参，在函数的上下文中有明确的定义；</li>
<li><strong>more</strong> : 是一个自由变量 (free variable)，因为函数字面量本生并没有给 more 赋予任何含义。</li>
</ul>
<p>按照定义：在创建函数时，如果需要捕获自由变量，那么包含指向被捕获变量的引用的函数就被称为闭包函数。</p>
<h3 id="2-2-修改自由变量"><a href="#2-2-修改自由变量" class="headerlink" title="2.2 修改自由变量"></a>2.2 修改自由变量</h3><p>这里需要注意的是，闭包捕获的是变量本身，即是对变量本身的引用，这意味着：</p>
<ul>
<li>闭包外部对自由变量的修改，在闭包内部是可见的；</li>
<li>闭包内部对自由变量的修改，在闭包外部也是可见的。</li>
</ul>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 声明 more 变量</span></span><br><span class="line">scala&gt; <span class="keyword">var</span> more = <span class="number">10</span></span><br><span class="line">more: <span class="type">Int</span> = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// more 变量必须已经被声明，否则下面的语句会报错</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> addMore = (x: <span class="type">Int</span>) =&gt; &#123;x + more&#125;</span><br><span class="line">addMore: <span class="type">Int</span> =&gt; <span class="type">Int</span> = $$<span class="type">Lambda</span>$<span class="number">1076</span>/<span class="number">1844473121</span>@<span class="number">876</span>c4f0</span><br><span class="line"></span><br><span class="line">scala&gt; addMore(<span class="number">10</span>)</span><br><span class="line">res7: <span class="type">Int</span> = <span class="number">20</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 注意这里是给 more 变量赋值，而不是重新声明 more 变量</span></span><br><span class="line">scala&gt; more=<span class="number">1000</span></span><br><span class="line">more: <span class="type">Int</span> = <span class="number">1000</span></span><br><span class="line"></span><br><span class="line">scala&gt; addMore(<span class="number">10</span>)</span><br><span class="line">res8: <span class="type">Int</span> = <span class="number">1010</span></span><br></pre></td></tr></table></figure>
<h3 id="2-3-自由变量多副本"><a href="#2-3-自由变量多副本" class="headerlink" title="2.3 自由变量多副本"></a>2.3 自由变量多副本</h3><p>自由变量可能随着程序的改变而改变，从而产生多个副本，但是闭包永远指向创建时候有效的那个变量副本。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 第一次声明 more 变量</span></span><br><span class="line">scala&gt; <span class="keyword">var</span> more = <span class="number">10</span></span><br><span class="line">more: <span class="type">Int</span> = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 创建闭包函数</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> addMore10 = (x: <span class="type">Int</span>) =&gt; &#123;x + more&#125;</span><br><span class="line">addMore10: <span class="type">Int</span> =&gt; <span class="type">Int</span> = $$<span class="type">Lambda</span>$<span class="number">1077</span>/<span class="number">1144251618</span>@<span class="number">1</span>bdaa13c</span><br><span class="line"></span><br><span class="line"><span class="comment">// 调用闭包函数</span></span><br><span class="line">scala&gt; addMore10(<span class="number">9</span>)</span><br><span class="line">res9: <span class="type">Int</span> = <span class="number">19</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 重新声明 more 变量</span></span><br><span class="line">scala&gt; <span class="keyword">var</span> more = <span class="number">100</span></span><br><span class="line">more: <span class="type">Int</span> = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 创建新的闭包函数</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> addMore100 = (x: <span class="type">Int</span>) =&gt; &#123;x + more&#125;</span><br><span class="line">addMore100: <span class="type">Int</span> =&gt; <span class="type">Int</span> = $$<span class="type">Lambda</span>$<span class="number">1078</span>/<span class="number">626955849</span>@<span class="number">4</span>d0be2ac</span><br><span class="line"></span><br><span class="line"><span class="comment">// 引用的是重新声明 more 变量</span></span><br><span class="line">scala&gt; addMore100(<span class="number">9</span>)</span><br><span class="line">res10: <span class="type">Int</span> = <span class="number">109</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 引用的还是第一次声明的 more 变量</span></span><br><span class="line">scala&gt; addMore10(<span class="number">9</span>)</span><br><span class="line">res11: <span class="type">Int</span> = <span class="number">19</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 对于全局而言 more 还是 100</span></span><br><span class="line">scala&gt; more</span><br><span class="line">res12: <span class="type">Int</span> = <span class="number">100</span></span><br></pre></td></tr></table></figure>
<p>从上面的示例可以看出重新声明 <code>more</code> 后，全局的 <code>more</code> 的值是 100，但是对于闭包函数 <code>addMore10</code> 还是引用的是值为 10 的 <code>more</code>，这是由虚拟机来实现的，虚拟机会保证 <code>more</code> 变量在重新声明后，原来的被捕获的变量副本继续在堆上保持存活。</p>
<h2 id="三、高阶函数"><a href="#三、高阶函数" class="headerlink" title="三、高阶函数"></a>三、高阶函数</h2><h3 id="3-1-使用函数作为参数"><a href="#3-1-使用函数作为参数" class="headerlink" title="3.1 使用函数作为参数"></a>3.1 使用函数作为参数</h3><p>定义函数时候支持传入函数作为参数，此时新定义的函数被称为高阶函数。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaApp</span> <span class="keyword">extends</span> <span class="title">App</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 1.定义函数</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">square</span> </span>= (x: <span class="type">Int</span>) =&gt; &#123;</span><br><span class="line">    x * x</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 2.定义高阶函数: 第一个参数是类型为 Int =&gt; Int 的函数</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">multi</span></span>(fun: <span class="type">Int</span> =&gt; <span class="type">Int</span>, x: <span class="type">Int</span>) = &#123;</span><br><span class="line">    fun(x) * <span class="number">100</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 3.传入具名函数</span></span><br><span class="line">  println(multi(square, <span class="number">5</span>)) <span class="comment">// 输出 2500</span></span><br><span class="line">    </span><br><span class="line">  <span class="comment">// 4.传入匿名函数</span></span><br><span class="line">  println(multi(_ * <span class="number">100</span>, <span class="number">5</span>)) <span class="comment">// 输出 50000</span></span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="3-2-函数柯里化"><a href="#3-2-函数柯里化" class="headerlink" title="3.2 函数柯里化"></a>3.2 函数柯里化</h3><p>我们上面定义的函数都只支持一个参数列表，而柯里化函数则支持多个参数列表。柯里化指的是将原来接受两个参数的函数变成接受一个参数的函数的过程。新的函数以原有第二个参数作为参数。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaApp</span> <span class="keyword">extends</span> <span class="title">App</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 定义柯里化函数</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">curriedSum</span></span>(x: <span class="type">Int</span>)(y: <span class="type">Int</span>) = x + y</span><br><span class="line">  println(curriedSum(<span class="number">2</span>)(<span class="number">3</span>)) <span class="comment">//输出 5</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里当你调用 curriedSum 时候，实际上是连着做了两次传统的函数调用，实际执行的柯里化过程如下：</p>
<ul>
<li>第一次调用接收一个名为 <code>x</code> 的 Int 型参数，返回一个用于第二次调用的函数，假设 <code>x</code> 为 2，则返回函数 <code>2+y</code>；</li>
<li>返回的函数接收参数 <code>y</code>，并计算并返回值 <code>2+3</code> 的值。</li>
</ul>
<p>想要获得柯里化的中间返回的函数其实也比较简单：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaApp</span> <span class="keyword">extends</span> <span class="title">App</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 定义柯里化函数</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">curriedSum</span></span>(x: <span class="type">Int</span>)(y: <span class="type">Int</span>) = x + y</span><br><span class="line">  println(curriedSum(<span class="number">2</span>)(<span class="number">3</span>)) <span class="comment">//输出 5</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// 获取传入值为 10 返回的中间函数 10 + y</span></span><br><span class="line">  <span class="keyword">val</span> plus: <span class="type">Int</span> =&gt; <span class="type">Int</span> = curriedSum(<span class="number">10</span>)_</span><br><span class="line">  println(plus(<span class="number">3</span>)) <span class="comment">//输出值 13</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>柯里化支持多个参数列表，多个参数按照从左到右的顺序依次执行柯里化操作：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaApp</span> <span class="keyword">extends</span> <span class="title">App</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 定义柯里化函数</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">curriedSum</span></span>(x: <span class="type">Int</span>)(y: <span class="type">Int</span>)(z: <span class="type">String</span>) = x + y + z</span><br><span class="line">  println(curriedSum(<span class="number">2</span>)(<span class="number">3</span>)(<span class="string">"name"</span>)) <span class="comment">// 输出 5name</span></span><br><span class="line">  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol>
<li>Martin Odersky . Scala 编程 (第 3 版)[M] . 电子工业出版社 . 2018-1-1  </li>
<li>凯.S.霍斯特曼  . 快学 Scala(第 2 版)[M] . 电子工业出版社 . 2017-7</li>
</ol>
]]></content>
      <categories>
        <category>notes</category>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>Scala</tag>
      </tags>
  </entry>
  <entry>
    <title>Scala基本数据类型和运算符</title>
    <url>/2021/03/18/Scala%E5%9F%BA%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E5%92%8C%E8%BF%90%E7%AE%97%E7%AC%A6/</url>
    <content><![CDATA[<h2 id="一、数据类型"><a href="#一、数据类型" class="headerlink" title="一、数据类型"></a>一、数据类型</h2><h3 id="1-1-类型支持"><a href="#1-1-类型支持" class="headerlink" title="1.1 类型支持"></a>1.1 类型支持</h3><p>Scala 拥有下表所示的数据类型，其中 Byte、Short、Int、Long 和 Char 类型统称为整数类型，整数类型加上 Float 和 Double 统称为数值类型。Scala 数值类型的取值范围和 Java 对应类型的取值范围相同。</p>
<table>
<thead>
<tr>
<th>数据类型</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>Byte</td>
<td>8 位有符号补码整数。数值区间为 -128 到 127</td>
</tr>
<tr>
<td>Short</td>
<td>16 位有符号补码整数。数值区间为 -32768 到 32767</td>
</tr>
<tr>
<td>Int</td>
<td>32 位有符号补码整数。数值区间为 -2147483648 到 2147483647</td>
</tr>
<tr>
<td>Long</td>
<td>64 位有符号补码整数。数值区间为 -9223372036854775808 到 9223372036854775807</td>
</tr>
<tr>
<td>Float</td>
<td>32 位, IEEE 754 标准的单精度浮点数</td>
</tr>
<tr>
<td>Double</td>
<td>64 位 IEEE 754 标准的双精度浮点数</td>
</tr>
<tr>
<td>Char</td>
<td>16 位无符号 Unicode 字符, 区间值为 U+0000 到 U+FFFF</td>
</tr>
<tr>
<td>String</td>
<td>字符序列</td>
</tr>
<tr>
<td>Boolean</td>
<td>true 或 false</td>
</tr>
<tr>
<td>Unit</td>
<td>表示无值，等同于 Java 中的 void。用作不返回任何结果的方法的结果类型。Unit 只有一个实例值，写成 ()。</td>
</tr>
<tr>
<td>Null</td>
<td>null 或空引用</td>
</tr>
<tr>
<td>Nothing</td>
<td>Nothing 类型在 Scala 的类层级的最低端；它是任何其他类型的子类型。</td>
</tr>
<tr>
<td>Any</td>
<td>Any 是所有其他类的超类</td>
</tr>
<tr>
<td>AnyRef</td>
<td>AnyRef 类是 Scala 里所有引用类 (reference class) 的基类</td>
</tr>
</tbody>
</table>
<h3 id="1-2-定义变量"><a href="#1-2-定义变量" class="headerlink" title="1.2 定义变量"></a>1.2 定义变量</h3><p>Scala 的变量分为两种，val 和 var，其区别如下：</p>
<ul>
<li><strong>val</strong> ： 类似于 Java 中的 final 变量，一旦初始化就不能被重新赋值；</li>
<li><strong>var</strong> ：类似于 Java 中的非 final 变量，在整个声明周期内 var 可以被重新赋值；</li>
</ul>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> a=<span class="number">1</span></span><br><span class="line">a: <span class="type">Int</span> = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">scala&gt; a=<span class="number">2</span></span><br><span class="line">&lt;console&gt;:<span class="number">8</span>: error: reassignment to <span class="keyword">val</span> <span class="comment">// 不允许重新赋值</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">var</span> b=<span class="number">1</span></span><br><span class="line">b: <span class="type">Int</span> = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">scala&gt; b=<span class="number">2</span></span><br><span class="line">b: <span class="type">Int</span> = <span class="number">2</span></span><br></pre></td></tr></table></figure>
<h3 id="1-3-类型推断"><a href="#1-3-类型推断" class="headerlink" title="1.3 类型推断"></a>1.3 类型推断</h3><p>在上面的演示中，并没有声明 a 是 Int 类型，但是程序还是把 a 当做 Int 类型，这就是 Scala 的类型推断。在大多数情况下，你都无需指明变量的类型，程序会自动进行推断。如果你想显式的声明类型，可以在变量后面指定，如下：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt;  <span class="keyword">val</span> c:<span class="type">String</span>=<span class="string">"hello scala"</span></span><br><span class="line">c: <span class="type">String</span> = hello scala</span><br></pre></td></tr></table></figure>
<h3 id="1-4-Scala解释器"><a href="#1-4-Scala解释器" class="headerlink" title="1.4 Scala解释器"></a>1.4 Scala解释器</h3><p>在 scala 命令行中，如果没有对输入的值指定赋值的变量，则输入的值默认会赋值给 <code>resX</code>(其中 X 是一个从 0 开始递增的整数)，<code>res</code> 是 result 的缩写，这个变量可以在后面的语句中进行引用。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="number">5</span></span><br><span class="line">res0: <span class="type">Int</span> = <span class="number">5</span></span><br><span class="line"></span><br><span class="line">scala&gt; res0*<span class="number">6</span></span><br><span class="line">res1: <span class="type">Int</span> = <span class="number">30</span></span><br><span class="line"></span><br><span class="line">scala&gt; println(res1)</span><br><span class="line"><span class="number">30</span></span><br></pre></td></tr></table></figure>
<h2 id="二、字面量"><a href="#二、字面量" class="headerlink" title="二、字面量"></a>二、字面量</h2><p>Scala 和 Java 字面量在使用上很多相似，比如都使用 F 或 f 表示浮点型，都使用 L 或 l 表示 Long 类型。下文主要介绍两者差异部分。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="number">1.2</span></span><br><span class="line">res0: <span class="type">Double</span> = <span class="number">1.2</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="number">1.2</span>f</span><br><span class="line">res1: <span class="type">Float</span> = <span class="number">1.2</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="number">1.4</span>F</span><br><span class="line">res2: <span class="type">Float</span> = <span class="number">1.4</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="number">1</span></span><br><span class="line">res3: <span class="type">Int</span> = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="number">1</span>l</span><br><span class="line">res4: <span class="type">Long</span> = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="number">1</span>L</span><br><span class="line">res5: <span class="type">Long</span> = <span class="number">1</span></span><br></pre></td></tr></table></figure>
<h3 id="2-1-整数字面量"><a href="#2-1-整数字面量" class="headerlink" title="2.1 整数字面量"></a>2.1 整数字面量</h3><p>Scala 支持 10 进制和 16 进制，但不支持八进制字面量和以 0 开头的整数字面量。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="number">012</span></span><br><span class="line">&lt;console&gt;:<span class="number">1</span>: error: <span class="type">Decimal</span> integer literals may not have a leading zero. (<span class="type">Octal</span> syntax is obsolete.)</span><br></pre></td></tr></table></figure>
<h3 id="2-2-字符串字面量"><a href="#2-2-字符串字面量" class="headerlink" title="2.2 字符串字面量"></a>2.2 字符串字面量</h3><h4 id="1-字符字面量"><a href="#1-字符字面量" class="headerlink" title="1. 字符字面量"></a>1. 字符字面量</h4><p>字符字面量由一对单引号和中间的任意 Unicode 字符组成。你可以显式的给出原字符、也可以使用字符的 Unicode 码来表示，还可以包含特殊的转义字符。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; '\u0041'</span><br><span class="line">res0: <span class="type">Char</span> = <span class="type">A</span></span><br><span class="line"></span><br><span class="line">scala&gt; 'a'</span><br><span class="line">res1: <span class="type">Char</span> = a</span><br><span class="line"></span><br><span class="line">scala&gt; '\n'</span><br><span class="line">res2: <span class="type">Char</span> =</span><br></pre></td></tr></table></figure>
<h4 id="2-字符串字面量"><a href="#2-字符串字面量" class="headerlink" title="2. 字符串字面量"></a>2. 字符串字面量</h4><p>字符串字面量由双引号包起来的字符组成。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="string">"hello world"</span></span><br><span class="line">res3: <span class="type">String</span> = hello world</span><br></pre></td></tr></table></figure>
<h4 id="3-原生字符串"><a href="#3-原生字符串" class="headerlink" title="3.原生字符串"></a>3.原生字符串</h4><p>Scala 提供了 <code>&quot;&quot;&quot; ... &quot;&quot;&quot;</code> 语法，通过三个双引号来表示原生字符串和多行字符串，使用该种方式，原生字符串中的特殊字符不会被转义。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="string">"hello \tool"</span></span><br><span class="line">res4: <span class="type">String</span> = hello    ool</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="string">""</span><span class="string">"hello \tool"</span><span class="string">""</span></span><br><span class="line">res5: <span class="type">String</span> = hello \tool</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="string">""</span><span class="string">"hello</span></span><br><span class="line"><span class="string">     | world"</span><span class="string">""</span></span><br><span class="line">res6: <span class="type">String</span> =</span><br><span class="line">hello</span><br><span class="line">world</span><br></pre></td></tr></table></figure>
<h3 id="2-3-符号字面量"><a href="#2-3-符号字面量" class="headerlink" title="2.3 符号字面量"></a>2.3 符号字面量</h3><p>符号字面量写法为： <code>&#39;标识符</code> ，这里 标识符可以是任何字母或数字的组合。符号字面量会被映射成 <code>scala.Symbol</code> 的实例，如:符号字面量 <code>&#39;x</code> 会被编译器翻译为 <code>scala.Symbol(&quot;x&quot;)</code>。符号字面量可选方法很少，只能通过 <code>.name</code> 获取其名称。</p>
<p>注意：具有相同 <code>name</code> 的符号字面量一定指向同一个 Symbol 对象，不同 <code>name</code> 的符号字面量一定指向不同的 Symbol 对象。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> sym = <span class="symbol">'ID008</span></span><br><span class="line">sym: <span class="type">Symbol</span> = <span class="symbol">'ID008</span></span><br><span class="line"></span><br><span class="line">scala&gt; sym.name</span><br><span class="line">res12: <span class="type">String</span> = <span class="type">ID008</span></span><br></pre></td></tr></table></figure>
<h3 id="2-4-插值表达式"><a href="#2-4-插值表达式" class="headerlink" title="2.4 插值表达式"></a>2.4 插值表达式</h3><p>Scala 支持插值表达式。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> name=<span class="string">"xiaoming"</span></span><br><span class="line">name: <span class="type">String</span> = xiaoming</span><br><span class="line"></span><br><span class="line">scala&gt; println(<span class="string">s"My name is <span class="subst">$name</span>,I'm <span class="subst">$&#123;2*9&#125;</span>."</span>)</span><br><span class="line"><span class="type">My</span> name is xiaoming,<span class="type">I</span><span class="symbol">'m</span> <span class="number">18.</span></span><br></pre></td></tr></table></figure>
<h2 id="三、运算符"><a href="#三、运算符" class="headerlink" title="三、运算符"></a>三、运算符</h2><p>Scala 和其他语言一样，支持大多数的操作运算符：</p>
<ul>
<li>算术运算符（+，-，*，/，%）</li>
<li>关系运算符（==，!=，&gt;，&lt;，&gt;=，&lt;=）</li>
<li>逻辑运算符 (&amp;&amp;，||，!，&amp;，|)</li>
<li>位运算符 (~，&amp;，|，^，&lt;&lt;，&gt;&gt;，&gt;&gt;&gt;)</li>
<li>赋值运算符 (=，+=，-=，*=，/=，%=，&lt;&lt;=，&gt;&gt;=，&amp;=，^=，|=)</li>
</ul>
<p>以上操作符的基本使用与 Java 类似，下文主要介绍差异部分和注意事项。</p>
<h3 id="3-1-运算符即方法"><a href="#3-1-运算符即方法" class="headerlink" title="3.1 运算符即方法"></a>3.1 运算符即方法</h3><p>Scala 的面向对象比 Java 更加纯粹，在 Scala 中一切都是对象。所以对于 <code>1+2</code>,实际上是调用了 Int 类中名为 <code>+</code> 的方法，所以 1+2,也可以写成 <code>1.+(2)</code>。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="number">1</span>+<span class="number">2</span></span><br><span class="line">res14: <span class="type">Int</span> = <span class="number">3</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="number">1.</span>+(<span class="number">2</span>)</span><br><span class="line">res15: <span class="type">Int</span> = <span class="number">3</span></span><br></pre></td></tr></table></figure>
<p>Int 类中包含了多个重载的 <code>+</code> 方法，用于分别接收不同类型的参数。</p>
<div align="center"> <img src="../pictures/scala-int+.png"> </div>

<h3 id="3-2-逻辑运算符"><a href="#3-2-逻辑运算符" class="headerlink" title="3.2 逻辑运算符"></a>3.2 逻辑运算符</h3><p>和其他语言一样，在 Scala 中 <code>&amp;&amp;</code>，<code>||</code> 的执行是短路的，即如果左边的表达式能确定整个结果，右边的表达式就不会被执行，这满足大多数使用场景。但是如果你需要在无论什么情况下，都执行右边的表达式，则可以使用 <code>&amp;</code> 或 <code>|</code> 代替。</p>
<h3 id="3-3-赋值运算符"><a href="#3-3-赋值运算符" class="headerlink" title="3.3 赋值运算符"></a>3.3 赋值运算符</h3><p>在 Scala 中没有 Java 中的 <code>++</code> 和 <code>--</code> 运算符，如果你想要实现类似的操作，只能使用 <code>+=1</code>，或者 <code>-=1</code>。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="keyword">var</span> a=<span class="number">1</span></span><br><span class="line">a: <span class="type">Int</span> = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">scala&gt; a+=<span class="number">1</span></span><br><span class="line"></span><br><span class="line">scala&gt; a</span><br><span class="line">res8: <span class="type">Int</span> = <span class="number">2</span></span><br><span class="line"></span><br><span class="line">scala&gt; a-=<span class="number">1</span></span><br><span class="line"></span><br><span class="line">scala&gt; a</span><br><span class="line">res10: <span class="type">Int</span> = <span class="number">1</span></span><br></pre></td></tr></table></figure>
<h3 id="3-4-运算符优先级"><a href="#3-4-运算符优先级" class="headerlink" title="3.4 运算符优先级"></a>3.4 运算符优先级</h3><p>操作符的优先级如下：优先级由上至下，逐级递减。</p>
<div align="center"> <img src="../pictures/scala-操作符优先级.png"> </div>

<p>在表格中某个字符的优先级越高，那么以这个字符打头的方法就拥有更高的优先级。如 <code>+</code> 的优先级大于 <code>&lt;</code>，也就意味则 <code>+</code> 的优先级大于以 <code>&lt;</code> 开头的 <code>&lt;&lt;</code>，所以 <code>2&lt;&lt;2+2</code> , 实际上等价于 <code>2&lt;&lt;(2+2)</code> :</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="number">2</span>&lt;&lt;<span class="number">2</span>+<span class="number">2</span></span><br><span class="line">res0: <span class="type">Int</span> = <span class="number">32</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="number">2</span>&lt;&lt;(<span class="number">2</span>+<span class="number">2</span>)</span><br><span class="line">res1: <span class="type">Int</span> = <span class="number">32</span></span><br></pre></td></tr></table></figure>
<h3 id="3-5-对象相等性"><a href="#3-5-对象相等性" class="headerlink" title="3.5 对象相等性"></a>3.5 对象相等性</h3><p>如果想要判断两个对象是否相等，可以使用 <code>==</code> 和 <code>!=</code>,这两个操作符可以用于所有的对象，包括 null。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="number">1</span>==<span class="number">2</span></span><br><span class="line">res2: <span class="type">Boolean</span> = <span class="literal">false</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>)==<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">res3: <span class="type">Boolean</span> = <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="number">1</span>==<span class="number">1.0</span></span><br><span class="line">res4: <span class="type">Boolean</span> = <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>)==<span class="literal">null</span></span><br><span class="line">res5: <span class="type">Boolean</span> = <span class="literal">false</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="literal">null</span>==<span class="literal">null</span></span><br><span class="line">res6: <span class="type">Boolean</span> = <span class="literal">true</span></span><br></pre></td></tr></table></figure>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol>
<li>Martin Odersky . Scala 编程 (第 3 版)[M] . 电子工业出版社 . 2018-1-1 </li>
</ol>
]]></content>
      <categories>
        <category>notes</category>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>Scala</tag>
      </tags>
  </entry>
  <entry>
    <title>Scala List &amp; Set</title>
    <url>/2021/03/18/Scala%E5%88%97%E8%A1%A8%E5%92%8C%E9%9B%86/</url>
    <content><![CDATA[<h2 id="一、List字面量"><a href="#一、List字面量" class="headerlink" title="一、List字面量"></a>一、List字面量</h2><p>List 是 Scala 中非常重要的一个数据结构，其与 Array(数组) 非常类似，但是 List 是不可变的，和 Java 中的 List 一样，其底层实现是链表。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt;  <span class="keyword">val</span> list = <span class="type">List</span>(<span class="string">"hadoop"</span>, <span class="string">"spark"</span>, <span class="string">"storm"</span>)</span><br><span class="line">list: <span class="type">List</span>[<span class="type">String</span>] = <span class="type">List</span>(hadoop, spark, storm)</span><br><span class="line"></span><br><span class="line"><span class="comment">// List 是不可变</span></span><br><span class="line">scala&gt; list(<span class="number">1</span>) = <span class="string">"hive"</span></span><br><span class="line">&lt;console&gt;:<span class="number">9</span>: error: value update is not a member of <span class="type">List</span>[<span class="type">String</span>]</span><br></pre></td></tr></table></figure>
<h2 id="二、List类型"><a href="#二、List类型" class="headerlink" title="二、List类型"></a>二、List类型</h2><p>Scala 中 List 具有以下两个特性：</p>
<ul>
<li><strong>同构 (homogeneous)</strong>：同一个 List 中的所有元素都必须是相同的类型；</li>
<li><strong>协变 (covariant)</strong>：如果 S 是 T 的子类型，那么 <code>List[S]</code> 就是 <code>List[T]</code> 的子类型，例如 <code>List[String]</code> 是 <code>List[Object]</code> 的子类型。</li>
</ul>
<p>需要特别说明的是空列表的类型为 <code>List[Nothing]</code>：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="type">List</span>()</span><br><span class="line">res1: <span class="type">List</span>[<span class="type">Nothing</span>] = <span class="type">List</span>()</span><br></pre></td></tr></table></figure>
<h2 id="三、构建List"><a href="#三、构建List" class="headerlink" title="三、构建List"></a>三、构建List</h2><p>所有 List 都由两个基本单元构成：<code>Nil</code> 和 <code>::</code>(读作”cons”)。即列表要么是空列表 (Nil)，要么是由一个 head 加上一个 tail 组成，而 tail 又是一个 List。我们在上面使用的 <code>List(&quot;hadoop&quot;, &quot;spark&quot;, &quot;storm&quot;)</code> 最终也是被解释为 <code>&quot;hadoop&quot;::&quot;spark&quot;:: &quot;storm&quot;::Nil</code>。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt;  <span class="keyword">val</span> list01 = <span class="string">"hadoop"</span>::<span class="string">"spark"</span>:: <span class="string">"storm"</span>::<span class="type">Nil</span></span><br><span class="line">list01: <span class="type">List</span>[<span class="type">String</span>] = <span class="type">List</span>(hadoop, spark, storm)</span><br><span class="line"></span><br><span class="line"><span class="comment">// :: 操作符号是右结合的，所以上面的表达式和下面的等同</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> list02 = <span class="string">"hadoop"</span>::(<span class="string">"spark"</span>:: (<span class="string">"storm"</span>::<span class="type">Nil</span>))</span><br><span class="line">list02: <span class="type">List</span>[<span class="type">String</span>] = <span class="type">List</span>(hadoop, spark, storm)</span><br></pre></td></tr></table></figure>
<h2 id="四、模式匹配"><a href="#四、模式匹配" class="headerlink" title="四、模式匹配"></a>四、模式匹配</h2><p>Scala 支持展开列表以实现模式匹配。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt;  <span class="keyword">val</span> list = <span class="type">List</span>(<span class="string">"hadoop"</span>, <span class="string">"spark"</span>, <span class="string">"storm"</span>)</span><br><span class="line">list: <span class="type">List</span>[<span class="type">String</span>] = <span class="type">List</span>(hadoop, spark, storm)</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> <span class="type">List</span>(a,b,c)=list</span><br><span class="line">a: <span class="type">String</span> = hadoop</span><br><span class="line">b: <span class="type">String</span> = spark</span><br><span class="line">c: <span class="type">String</span> = storm</span><br></pre></td></tr></table></figure>
<p>如果只需要匹配部分内容，可以如下：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> a::rest=list</span><br><span class="line">a: <span class="type">String</span> = hadoop</span><br><span class="line">rest: <span class="type">List</span>[<span class="type">String</span>] = <span class="type">List</span>(spark, storm)</span><br></pre></td></tr></table></figure>
<h2 id="五、列表的基本操作"><a href="#五、列表的基本操作" class="headerlink" title="五、列表的基本操作"></a>五、列表的基本操作</h2><h3 id="5-1-常用方法"><a href="#5-1-常用方法" class="headerlink" title="5.1 常用方法"></a>5.1 常用方法</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaApp</span> <span class="keyword">extends</span> <span class="title">App</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> list = <span class="type">List</span>(<span class="string">"hadoop"</span>, <span class="string">"spark"</span>, <span class="string">"storm"</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 1.列表是否为空</span></span><br><span class="line">  list.isEmpty</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 2.返回列表中的第一个元素</span></span><br><span class="line">  list.head</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 3.返回列表中除第一个元素外的所有元素 这里输出 List(spark, storm)</span></span><br><span class="line">  list.tail</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 4.tail 和 head 可以结合使用</span></span><br><span class="line">  list.tail.head</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 5.返回列表中的最后一个元素 与 head 相反</span></span><br><span class="line">  list.init</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 6.返回列表中除了最后一个元素之外的其他元素 与 tail 相反 这里输出 List(hadoop, spark)</span></span><br><span class="line">  list.last</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 7.使用下标访问元素</span></span><br><span class="line">  list(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 8.获取列表长度</span></span><br><span class="line">  list.length</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 9. 反转列表</span></span><br><span class="line">  list.reverse</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="5-2-indices"><a href="#5-2-indices" class="headerlink" title="5.2 indices"></a>5.2 indices</h3><p>indices 方法返回所有下标。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; list.indices</span><br><span class="line">res2: scala.collection.immutable.<span class="type">Range</span> = <span class="type">Range</span>(<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<h3 id="5-3-take-amp-drop-amp-splitAt"><a href="#5-3-take-amp-drop-amp-splitAt" class="headerlink" title="5.3 take &amp; drop &amp; splitAt"></a>5.3 take &amp; drop &amp; splitAt</h3><ul>
<li><strong>take</strong>：获取前 n 个元素；</li>
<li><strong>drop</strong>：删除前 n 个元素；</li>
<li><strong>splitAt</strong>：从第几个位置开始拆分。</li>
</ul>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; list take <span class="number">2</span></span><br><span class="line">res3: <span class="type">List</span>[<span class="type">String</span>] = <span class="type">List</span>(hadoop, spark)</span><br><span class="line"></span><br><span class="line">scala&gt; list drop <span class="number">2</span></span><br><span class="line">res4: <span class="type">List</span>[<span class="type">String</span>] = <span class="type">List</span>(storm)</span><br><span class="line"></span><br><span class="line">scala&gt; list splitAt <span class="number">2</span></span><br><span class="line">res5: (<span class="type">List</span>[<span class="type">String</span>], <span class="type">List</span>[<span class="type">String</span>]) = (<span class="type">List</span>(hadoop, spark),<span class="type">List</span>(storm))</span><br></pre></td></tr></table></figure>
<h3 id="5-4-flatten"><a href="#5-4-flatten" class="headerlink" title="5.4 flatten"></a>5.4 flatten</h3><p>flatten 接收一个由列表组成的列表，并将其进行扁平化操作，返回单个列表。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt;  <span class="type">List</span>(<span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>), <span class="type">List</span>(<span class="number">3</span>), <span class="type">List</span>(), <span class="type">List</span>(<span class="number">4</span>, <span class="number">5</span>)).flatten</span><br><span class="line">res6: <span class="type">List</span>[<span class="type">Int</span>] = <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>)</span><br></pre></td></tr></table></figure>
<h3 id="5-5-zip-amp-unzip"><a href="#5-5-zip-amp-unzip" class="headerlink" title="5.5 zip &amp; unzip"></a>5.5 zip &amp; unzip</h3><p>对两个 List 执行 <code>zip</code> 操作结果如下，返回对应位置元素组成的元组的列表，<code>unzip</code> 则执行反向操作。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> list = <span class="type">List</span>(<span class="string">"hadoop"</span>, <span class="string">"spark"</span>, <span class="string">"storm"</span>)</span><br><span class="line">scala&gt; <span class="keyword">val</span> score = <span class="type">List</span>(<span class="number">10</span>,<span class="number">20</span>,<span class="number">30</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> zipped=list zip score</span><br><span class="line">zipped: <span class="type">List</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">List</span>((hadoop,<span class="number">10</span>), (spark,<span class="number">20</span>), (storm,<span class="number">30</span>))</span><br><span class="line"></span><br><span class="line">scala&gt; zipped.unzip</span><br><span class="line">res7: (<span class="type">List</span>[<span class="type">String</span>], <span class="type">List</span>[<span class="type">Int</span>]) = (<span class="type">List</span>(hadoop, spark, storm),<span class="type">List</span>(<span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span>))</span><br></pre></td></tr></table></figure>
<h3 id="5-6-toString-amp-mkString"><a href="#5-6-toString-amp-mkString" class="headerlink" title="5.6 toString &amp; mkString"></a>5.6 toString &amp; mkString</h3><p>toString 返回 List 的字符串表现形式。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; list.toString</span><br><span class="line">res8: <span class="type">String</span> = <span class="type">List</span>(hadoop, spark, storm)</span><br></pre></td></tr></table></figure>
<p>如果想改变 List 的字符串表现形式，可以使用 mkString。mkString 有三个重载方法，方法定义如下：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// start：前缀  sep：分隔符  end:后缀</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mkString</span></span>(start: <span class="type">String</span>, sep: <span class="type">String</span>, end: <span class="type">String</span>): <span class="type">String</span> =</span><br><span class="line">  addString(<span class="keyword">new</span> <span class="type">StringBuilder</span>(), start, sep, end).toString</span><br><span class="line"></span><br><span class="line"><span class="comment">// seq 分隔符</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mkString</span></span>(sep: <span class="type">String</span>): <span class="type">String</span> = mkString(<span class="string">""</span>, sep, <span class="string">""</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 如果不指定分隔符 默认使用""分隔</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mkString</span></span>: <span class="type">String</span> = mkString(<span class="string">""</span>)</span><br></pre></td></tr></table></figure>
<p>使用示例如下：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; list.mkString</span><br><span class="line">res9: <span class="type">String</span> = hadoopsparkstorm</span><br><span class="line"></span><br><span class="line">scala&gt;  list.mkString(<span class="string">","</span>)</span><br><span class="line">res10: <span class="type">String</span> = hadoop,spark,storm</span><br><span class="line"></span><br><span class="line">scala&gt; list.mkString(<span class="string">"&#123;"</span>,<span class="string">","</span>,<span class="string">"&#125;"</span>)</span><br><span class="line">res11: <span class="type">String</span> = &#123;hadoop,spark,storm&#125;</span><br></pre></td></tr></table></figure>
<h3 id="5-7-iterator-amp-toArray-amp-copyToArray"><a href="#5-7-iterator-amp-toArray-amp-copyToArray" class="headerlink" title="5.7 iterator &amp; toArray &amp; copyToArray"></a>5.7 iterator &amp; toArray &amp; copyToArray</h3><p>iterator 方法返回的是迭代器，这和其他语言的使用是一样的。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaApp</span> <span class="keyword">extends</span> <span class="title">App</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> list = <span class="type">List</span>(<span class="string">"hadoop"</span>, <span class="string">"spark"</span>, <span class="string">"storm"</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> iterator: <span class="type">Iterator</span>[<span class="type">String</span>] = list.iterator</span><br><span class="line"></span><br><span class="line">  <span class="keyword">while</span> (iterator.hasNext) &#123;</span><br><span class="line">    println(iterator.next)</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>toArray 和 toList 用于 List 和数组之间的互相转换。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> array = list.toArray</span><br><span class="line">array: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(hadoop, spark, storm)</span><br><span class="line"></span><br><span class="line">scala&gt; array.toList</span><br><span class="line">res13: <span class="type">List</span>[<span class="type">String</span>] = <span class="type">List</span>(hadoop, spark, storm)</span><br></pre></td></tr></table></figure>
<p>copyToArray 将 List 中的元素拷贝到数组中指定位置。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaApp</span> <span class="keyword">extends</span> <span class="title">App</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> list = <span class="type">List</span>(<span class="string">"hadoop"</span>, <span class="string">"spark"</span>, <span class="string">"storm"</span>)</span><br><span class="line">  <span class="keyword">val</span> array = <span class="type">Array</span>(<span class="string">"10"</span>, <span class="string">"20"</span>, <span class="string">"30"</span>)</span><br><span class="line"></span><br><span class="line">  list.copyToArray(array,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">  println(array.toBuffer)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出 ：ArrayBuffer(10, hadoop, spark)</span></span><br></pre></td></tr></table></figure>
<h2 id="六、列表的高级操作"><a href="#六、列表的高级操作" class="headerlink" title="六、列表的高级操作"></a>六、列表的高级操作</h2><h3 id="6-1-列表转换：map-amp-flatMap-amp-foreach"><a href="#6-1-列表转换：map-amp-flatMap-amp-foreach" class="headerlink" title="6.1 列表转换：map &amp; flatMap &amp; foreach"></a>6.1 列表转换：map &amp; flatMap &amp; foreach</h3><p>map 与 Java 8 函数式编程中的 map 类似，都是对 List 中每一个元素执行指定操作。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>).map(_+<span class="number">10</span>)</span><br><span class="line">res15: <span class="type">List</span>[<span class="type">Int</span>] = <span class="type">List</span>(<span class="number">11</span>, <span class="number">12</span>, <span class="number">13</span>)</span><br></pre></td></tr></table></figure>
<p>flatMap 与 map 类似，但如果 List 中的元素还是 List，则会对其进行 flatten 操作。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; list.map(_.toList)</span><br><span class="line">res16: <span class="type">List</span>[<span class="type">List</span>[<span class="type">Char</span>]] = <span class="type">List</span>(<span class="type">List</span>(h, a, d, o, o, p), <span class="type">List</span>(s, p, a, r, k), <span class="type">List</span>(s, t, o, r, m))</span><br><span class="line"></span><br><span class="line">scala&gt; list.flatMap(_.toList)</span><br><span class="line">res17: <span class="type">List</span>[<span class="type">Char</span>] = <span class="type">List</span>(h, a, d, o, o, p, s, p, a, r, k, s, t, o, r, m)</span><br></pre></td></tr></table></figure>
<p>foreach 要求右侧的操作是一个返回值为 Unit 的函数，你也可以简单理解为执行一段没有返回值代码。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="keyword">var</span> sum = <span class="number">0</span></span><br><span class="line">sum: <span class="type">Int</span> = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>) foreach (sum += _)</span><br><span class="line"></span><br><span class="line">scala&gt; sum</span><br><span class="line">res19: <span class="type">Int</span> = <span class="number">15</span></span><br></pre></td></tr></table></figure>
<h3 id="6-2-列表过滤：filter-amp-partition-amp-find-amp-takeWhile-amp-dropWhile-amp-span"><a href="#6-2-列表过滤：filter-amp-partition-amp-find-amp-takeWhile-amp-dropWhile-amp-span" class="headerlink" title="6.2 列表过滤：filter &amp; partition &amp; find &amp; takeWhile &amp; dropWhile &amp; span"></a>6.2 列表过滤：filter &amp; partition &amp; find &amp; takeWhile &amp; dropWhile &amp; span</h3><p>filter 用于筛选满足条件元素，返回新的 List。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>) filter (_ % <span class="number">2</span> == <span class="number">0</span>)</span><br><span class="line">res20: <span class="type">List</span>[<span class="type">Int</span>] = <span class="type">List</span>(<span class="number">2</span>, <span class="number">4</span>)</span><br></pre></td></tr></table></figure>
<p>partition 会按照筛选条件对元素进行分组，返回类型是 tuple(元组)。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>) partition (_ % <span class="number">2</span> == <span class="number">0</span>)</span><br><span class="line">res21: (<span class="type">List</span>[<span class="type">Int</span>], <span class="type">List</span>[<span class="type">Int</span>]) = (<span class="type">List</span>(<span class="number">2</span>, <span class="number">4</span>),<span class="type">List</span>(<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>))</span><br></pre></td></tr></table></figure>
<p>find 查找第一个满足条件的值，由于可能并不存在这样的值，所以返回类型是 <code>Option</code>，可以通过 <code>getOrElse</code> 在不存在满足条件值的情况下返回默认值。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>) find (_ % <span class="number">2</span> == <span class="number">0</span>)</span><br><span class="line">res22: <span class="type">Option</span>[<span class="type">Int</span>] = <span class="type">Some</span>(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> result: <span class="type">Option</span>[<span class="type">Int</span>] = <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>) find (_ % <span class="number">2</span> == <span class="number">0</span>)</span><br><span class="line">result.getOrElse(<span class="number">10</span>)</span><br></pre></td></tr></table></figure>
<p>takeWhile 遍历元素，直到遇到第一个不符合条件的值则结束遍历，返回所有遍历到的值。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">-4</span>, <span class="number">5</span>) takeWhile (_ &gt; <span class="number">0</span>)</span><br><span class="line">res23: <span class="type">List</span>[<span class="type">Int</span>] = <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<p>dropWhile 遍历元素，直到遇到第一个不符合条件的值则结束遍历，返回所有未遍历到的值。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 第一个值就不满足条件,所以返回列表中所有的值</span></span><br><span class="line">scala&gt; <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">-4</span>, <span class="number">5</span>) dropWhile  (_ &lt; <span class="number">0</span>)</span><br><span class="line">res24: <span class="type">List</span>[<span class="type">Int</span>] = <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">-4</span>, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">-4</span>, <span class="number">5</span>) dropWhile (_ &lt; <span class="number">3</span>)</span><br><span class="line">res26: <span class="type">List</span>[<span class="type">Int</span>] = <span class="type">List</span>(<span class="number">3</span>, <span class="number">-4</span>, <span class="number">5</span>)</span><br></pre></td></tr></table></figure>
<p>span 遍历元素，直到遇到第一个不符合条件的值则结束遍历，将遍历到的值和未遍历到的值分别放入两个 List 中返回，返回类型是 tuple(元组)。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">-4</span>, <span class="number">5</span>) span (_ &gt; <span class="number">0</span>)</span><br><span class="line">res27: (<span class="type">List</span>[<span class="type">Int</span>], <span class="type">List</span>[<span class="type">Int</span>]) = (<span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>),<span class="type">List</span>(<span class="number">-4</span>, <span class="number">5</span>))</span><br></pre></td></tr></table></figure>
<h3 id="6-3-列表检查：forall-amp-exists"><a href="#6-3-列表检查：forall-amp-exists" class="headerlink" title="6.3 列表检查：forall &amp; exists"></a>6.3 列表检查：forall &amp; exists</h3><p>forall 检查 List 中所有元素，如果所有元素都满足条件，则返回 true。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">-4</span>, <span class="number">5</span>) forall ( _ &gt; <span class="number">0</span> )</span><br><span class="line">res28: <span class="type">Boolean</span> = <span class="literal">false</span></span><br></pre></td></tr></table></figure>
<p>exists 检查 List 中的元素，如果某个元素已经满足条件，则返回 true。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt;  <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">-4</span>, <span class="number">5</span>) exists (_ &gt; <span class="number">0</span> )</span><br><span class="line">res29: <span class="type">Boolean</span> = <span class="literal">true</span></span><br></pre></td></tr></table></figure>
<h3 id="6-4-列表排序：sortWith"><a href="#6-4-列表排序：sortWith" class="headerlink" title="6.4 列表排序：sortWith"></a>6.4 列表排序：sortWith</h3><p>sortWith 对 List 中所有元素按照指定规则进行排序，由于 List 是不可变的，所以排序返回一个新的 List。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="type">List</span>(<span class="number">1</span>, <span class="number">-3</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">6</span>) sortWith (_ &lt; _)</span><br><span class="line">res30: <span class="type">List</span>[<span class="type">Int</span>] = <span class="type">List</span>(<span class="number">-3</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> list = <span class="type">List</span>( <span class="string">"hive"</span>,<span class="string">"spark"</span>,<span class="string">"azkaban"</span>,<span class="string">"hadoop"</span>)</span><br><span class="line">list: <span class="type">List</span>[<span class="type">String</span>] = <span class="type">List</span>(hive, spark, azkaban, hadoop)</span><br><span class="line"></span><br><span class="line">scala&gt; list.sortWith(_.length&gt;_.length)</span><br><span class="line">res33: <span class="type">List</span>[<span class="type">String</span>] = <span class="type">List</span>(azkaban, hadoop, spark, hive)</span><br></pre></td></tr></table></figure>
<h2 id="七、List对象的方法"><a href="#七、List对象的方法" class="headerlink" title="七、List对象的方法"></a>七、List对象的方法</h2><p>上面介绍的所有方法都是 List 类上的方法，下面介绍的是 List 伴生对象中的方法。</p>
<h3 id="7-1-List-range"><a href="#7-1-List-range" class="headerlink" title="7.1 List.range"></a>7.1 List.range</h3><p>List.range 可以产生指定的前闭后开区间内的值组成的 List，它有三个可选参数: start(开始值)，end(结束值，不包含)，step(步长)。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt;  <span class="type">List</span>.range(<span class="number">1</span>, <span class="number">5</span>)</span><br><span class="line">res34: <span class="type">List</span>[<span class="type">Int</span>] = <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="type">List</span>.range(<span class="number">1</span>, <span class="number">9</span>, <span class="number">2</span>)</span><br><span class="line">res35: <span class="type">List</span>[<span class="type">Int</span>] = <span class="type">List</span>(<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">7</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="type">List</span>.range(<span class="number">9</span>, <span class="number">1</span>, <span class="number">-3</span>)</span><br><span class="line">res36: <span class="type">List</span>[<span class="type">Int</span>] = <span class="type">List</span>(<span class="number">9</span>, <span class="number">6</span>, <span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<h3 id="7-2-List-fill"><a href="#7-2-List-fill" class="headerlink" title="7.2 List.fill"></a>7.2 List.fill</h3><p>List.fill 使用指定值填充 List。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="type">List</span>.fill(<span class="number">3</span>)(<span class="string">"hello"</span>)</span><br><span class="line">res37: <span class="type">List</span>[<span class="type">String</span>] = <span class="type">List</span>(hello, hello, hello)</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="type">List</span>.fill(<span class="number">2</span>,<span class="number">3</span>)(<span class="string">"world"</span>)</span><br><span class="line">res38: <span class="type">List</span>[<span class="type">List</span>[<span class="type">String</span>]] = <span class="type">List</span>(<span class="type">List</span>(world, world, world), <span class="type">List</span>(world, world, world))</span><br></pre></td></tr></table></figure>
<h3 id="7-3-List-concat"><a href="#7-3-List-concat" class="headerlink" title="7.3 List.concat"></a>7.3 List.concat</h3><p>List.concat 用于拼接多个 List。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="type">List</span>.concat(<span class="type">List</span>('a', 'b'), <span class="type">List</span>('c'))</span><br><span class="line">res39: <span class="type">List</span>[<span class="type">Char</span>] = <span class="type">List</span>(a, b, c)</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="type">List</span>.concat(<span class="type">List</span>(), <span class="type">List</span>('b'), <span class="type">List</span>('c'))</span><br><span class="line">res40: <span class="type">List</span>[<span class="type">Char</span>] = <span class="type">List</span>(b, c)</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="type">List</span>.concat()</span><br><span class="line">res41: <span class="type">List</span>[<span class="type">Nothing</span>] = <span class="type">List</span>()</span><br></pre></td></tr></table></figure>
<h2 id="八、处理多个List"><a href="#八、处理多个List" class="headerlink" title="八、处理多个List"></a>八、处理多个List</h2><p>当多个 List 被放入同一个 tuple 中时候，可以通过 zipped 对多个 List 进行关联处理。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 两个 List 对应位置的元素相乘</span></span><br><span class="line">scala&gt; (<span class="type">List</span>(<span class="number">10</span>, <span class="number">20</span>), <span class="type">List</span>(<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>)).zipped.map(_ * _)</span><br><span class="line">res42: <span class="type">List</span>[<span class="type">Int</span>] = <span class="type">List</span>(<span class="number">30</span>, <span class="number">80</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 三个 List 的操作也是一样的</span></span><br><span class="line">scala&gt; (<span class="type">List</span>(<span class="number">10</span>, <span class="number">20</span>), <span class="type">List</span>(<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>), <span class="type">List</span>(<span class="number">100</span>, <span class="number">200</span>)).zipped.map(_ * _ + _)</span><br><span class="line">res43: <span class="type">List</span>[<span class="type">Int</span>] = <span class="type">List</span>(<span class="number">130</span>, <span class="number">280</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 判断第一个 List 中元素的长度与第二个 List 中元素的值是否相等</span></span><br><span class="line">scala&gt;  (<span class="type">List</span>(<span class="string">"abc"</span>, <span class="string">"de"</span>), <span class="type">List</span>(<span class="number">3</span>, <span class="number">2</span>)).zipped.forall(_.length == _)</span><br><span class="line">res44: <span class="type">Boolean</span> = <span class="literal">true</span></span><br></pre></td></tr></table></figure>
<h2 id="九、缓冲列表ListBuffer"><a href="#九、缓冲列表ListBuffer" class="headerlink" title="九、缓冲列表ListBuffer"></a>九、缓冲列表ListBuffer</h2><p>上面介绍的 List，由于其底层实现是链表，这意味着能快速访问 List 头部元素，但对尾部元素的访问则比较低效，这时候可以采用 <code>ListBuffer</code>，ListBuffer 提供了在常量时间内往头部和尾部追加元素。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> scala.collection.mutable.<span class="type">ListBuffer</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaApp</span> <span class="keyword">extends</span> <span class="title">App</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> buffer = <span class="keyword">new</span> <span class="type">ListBuffer</span>[<span class="type">Int</span>]</span><br><span class="line">  <span class="comment">// 1.在尾部追加元素</span></span><br><span class="line">  buffer += <span class="number">1</span></span><br><span class="line">  buffer += <span class="number">2</span></span><br><span class="line">  <span class="comment">// 2.在头部追加元素</span></span><br><span class="line">  <span class="number">3</span> +=: buffer</span><br><span class="line">  <span class="comment">// 3. ListBuffer 转 List</span></span><br><span class="line">  <span class="keyword">val</span> list: <span class="type">List</span>[<span class="type">Int</span>] = buffer.toList</span><br><span class="line">  println(list)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//输出：List(3, 1, 2)</span></span><br></pre></td></tr></table></figure>
<h2 id="十、集-Set"><a href="#十、集-Set" class="headerlink" title="十、集(Set)"></a>十、集(Set)</h2><p>Set 是不重复元素的集合。分为可变 Set 和不可变 Set。</p>
<h3 id="10-1-可变Set"><a href="#10-1-可变Set" class="headerlink" title="10.1 可变Set"></a>10.1 可变Set</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaApp</span> <span class="keyword">extends</span> <span class="title">App</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 可变 Set</span></span><br><span class="line">  <span class="keyword">val</span> mutableSet = <span class="keyword">new</span> collection.mutable.<span class="type">HashSet</span>[<span class="type">Int</span>]</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 1.添加元素</span></span><br><span class="line">  mutableSet.add(<span class="number">1</span>)</span><br><span class="line">  mutableSet.add(<span class="number">2</span>)</span><br><span class="line">  mutableSet.add(<span class="number">3</span>)</span><br><span class="line">  mutableSet.add(<span class="number">3</span>)</span><br><span class="line">  mutableSet.add(<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 2.移除元素</span></span><br><span class="line">  mutableSet.remove(<span class="number">2</span>)</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// 3.调用 mkString 方法 输出 1,3,4</span></span><br><span class="line">  println(mutableSet.mkString(<span class="string">","</span>))</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 4. 获取 Set 中最小元素</span></span><br><span class="line">  println(mutableSet.min)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 5. 获取 Set 中最大元素</span></span><br><span class="line">  println(mutableSet.max)</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="10-2-不可变Set"><a href="#10-2-不可变Set" class="headerlink" title="10.2 不可变Set"></a>10.2 不可变Set</h3><p>不可变 Set 没有 add 方法，可以使用 <code>+</code> 添加元素，但是此时会返回一个新的不可变 Set，原来的 Set 不变。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaApp</span> <span class="keyword">extends</span> <span class="title">App</span> </span>&#123;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// 不可变 Set</span></span><br><span class="line">  <span class="keyword">val</span> immutableSet = <span class="keyword">new</span> collection.immutable.<span class="type">HashSet</span>[<span class="type">Int</span>]</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> ints: <span class="type">HashSet</span>[<span class="type">Int</span>] = immutableSet+<span class="number">1</span></span><br><span class="line"></span><br><span class="line">  println(ints)</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出 Set(1)</span></span><br></pre></td></tr></table></figure>
<h3 id="10-3-Set间操作"><a href="#10-3-Set间操作" class="headerlink" title="10.3 Set间操作"></a>10.3 Set间操作</h3><p>多个 Set 之间可以进行求交集或者合集等操作。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaApp</span> <span class="keyword">extends</span> <span class="title">App</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 声明有序 Set</span></span><br><span class="line">  <span class="keyword">val</span> mutableSet = collection.mutable.<span class="type">SortedSet</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>)</span><br><span class="line">  <span class="keyword">val</span> immutableSet = collection.immutable.<span class="type">SortedSet</span>(<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>)</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// 两个 Set 的合集  输出：TreeSet(1, 2, 3, 4, 5, 6, 7)</span></span><br><span class="line">  println(mutableSet ++ immutableSet)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 两个 Set 的交集  输出：TreeSet(3, 4, 5)</span></span><br><span class="line">  println(mutableSet intersect immutableSet)</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol>
<li>Martin Odersky . Scala 编程 (第 3 版)[M] . 电子工业出版社 . 2018-1-1  </li>
<li>凯.S.霍斯特曼  . 快学 Scala(第 2 版)[M] . 电子工业出版社 . 2017-7</li>
</ol>
]]></content>
      <categories>
        <category>notes</category>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>Scala</tag>
      </tags>
  </entry>
  <entry>
    <title>Scala 数组相关操作</title>
    <url>/2021/03/18/Scala%E6%95%B0%E7%BB%84/</url>
    <content><![CDATA[<h2 id="一、定长数组"><a href="#一、定长数组" class="headerlink" title="一、定长数组"></a>一、定长数组</h2><p>在 Scala 中，如果你需要一个长度不变的数组，可以使用 Array。但需要注意以下两点：</p>
<ul>
<li>在 Scala 中使用 <code>(index)</code> 而不是 <code>[index]</code> 来访问数组中的元素，因为访问元素，对于 Scala 来说是方法调用，<code>(index)</code> 相当于执行了 <code>.apply(index)</code> 方法。</li>
<li>Scala 中的数组与 Java 中的是等价的，<code>Array[Int]()</code> 在虚拟机层面就等价于 Java 的 <code>int[]</code>。</li>
</ul>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 10 个整数的数组，所有元素初始化为 0</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> nums=<span class="keyword">new</span> <span class="type">Array</span>[<span class="type">Int</span>](<span class="number">10</span>)</span><br><span class="line">nums: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 10 个元素的字符串数组，所有元素初始化为 null</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> strings=<span class="keyword">new</span> <span class="type">Array</span>[<span class="type">String</span>](<span class="number">10</span>)</span><br><span class="line">strings: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(<span class="literal">null</span>, <span class="literal">null</span>, <span class="literal">null</span>, <span class="literal">null</span>, <span class="literal">null</span>, <span class="literal">null</span>, <span class="literal">null</span>, <span class="literal">null</span>, <span class="literal">null</span>, <span class="literal">null</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 使用指定值初始化，此时不需要 new 关键字</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> a=<span class="type">Array</span>(<span class="string">"hello"</span>,<span class="string">"scala"</span>)</span><br><span class="line">a: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(hello, scala)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 使用 () 来访问元素</span></span><br><span class="line">scala&gt; a(<span class="number">0</span>)</span><br><span class="line">res3: <span class="type">String</span> = hello</span><br></pre></td></tr></table></figure>
<h2 id="二、变长数组"><a href="#二、变长数组" class="headerlink" title="二、变长数组"></a>二、变长数组</h2><p>在 scala 中通过 ArrayBuffer 实现变长数组 (又称缓冲数组)。在构建 ArrayBuffer 时必须给出类型参数，但不必指定长度，因为 ArrayBuffer 会在需要的时候自动扩容和缩容。变长数组的构建方式及常用操作如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> scala.collection.mutable.ArrayBuffer</span><br><span class="line"></span><br><span class="line">object ScalaApp &#123;</span><br><span class="line">    </span><br><span class="line">  <span class="comment">// 相当于 Java 中的 main 方法</span></span><br><span class="line">  <span class="function">def <span class="title">main</span><span class="params">(args: Array[String])</span>: Unit </span>= &#123;</span><br><span class="line">    <span class="comment">// 1.声明变长数组 (缓冲数组)</span></span><br><span class="line">    val ab = <span class="keyword">new</span> ArrayBuffer[Int]()</span><br><span class="line">    <span class="comment">// 2.在末端增加元素</span></span><br><span class="line">    ab += <span class="number">1</span></span><br><span class="line">    <span class="comment">// 3.在末端添加多个元素</span></span><br><span class="line">    ab += (<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">    <span class="comment">// 4.可以使用 ++=追加任何集合</span></span><br><span class="line">    ab ++= Array(<span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>)</span><br><span class="line">    <span class="comment">// 5.缓冲数组可以直接打印查看</span></span><br><span class="line">    println(ab)</span><br><span class="line">    <span class="comment">// 6.移除最后三个元素</span></span><br><span class="line">    ab.trimEnd(<span class="number">3</span>)</span><br><span class="line">    <span class="comment">// 7.在第 1 个元素之后插入多个新元素</span></span><br><span class="line">    ab.insert(<span class="number">1</span>, <span class="number">8</span>, <span class="number">9</span>)</span><br><span class="line">    <span class="comment">// 8.从第 2 个元素开始,移除 3 个元素,不指定第二个参数的话,默认值为 1</span></span><br><span class="line">    ab.remove(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">    <span class="comment">// 9.缓冲数组转定长数组</span></span><br><span class="line">    val abToA = ab.toArray</span><br><span class="line">    <span class="comment">// 10. 定长数组打印为其 hashcode 值</span></span><br><span class="line">    println(abToA)</span><br><span class="line">    <span class="comment">// 11. 定长数组转缓冲数组</span></span><br><span class="line">    val aToAb = abToA.toBuffer</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>需要注意的是：使用 <code>+=</code> 在末尾插入元素是一个高效的操作，其时间复杂度是 O(1)。而使用 <code>insert</code> 随机插入元素的时间复杂度是 O(n)，因为在其插入位置之后的所有元素都要进行对应的后移，所以在 <code>ArrayBuffer</code> 中随机插入元素是一个低效的操作。</p>
<h2 id="三、数组遍历"><a href="#三、数组遍历" class="headerlink" title="三、数组遍历"></a>三、数组遍历</h2><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaApp</span> <span class="keyword">extends</span> <span class="title">App</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> a = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 1.方式一 相当于 Java 中的增强 for 循环</span></span><br><span class="line">  <span class="keyword">for</span> (elem &lt;- a) &#123;</span><br><span class="line">    print(elem)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 2.方式二</span></span><br><span class="line">  <span class="keyword">for</span> (index &lt;- <span class="number">0</span> until a.length) &#123;</span><br><span class="line">    print(a(index))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 3.方式三, 是第二种方式的简写</span></span><br><span class="line">  <span class="keyword">for</span> (index &lt;- a.indices) &#123;</span><br><span class="line">    print(a(index))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 4.反向遍历</span></span><br><span class="line">  <span class="keyword">for</span> (index &lt;- a.indices.reverse) &#123;</span><br><span class="line">    print(a(index))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里我们没有将代码写在 main 方法中，而是继承自 App.scala，这是 Scala 提供的一种简写方式，此时将代码写在类中，等价于写在 main 方法中，直接运行该类即可。</p>
<h2 id="四、数组转换"><a href="#四、数组转换" class="headerlink" title="四、数组转换"></a>四、数组转换</h2><p>数组转换是指由现有数组产生新的数组。假设当前拥有 a 数组，想把 a 中的偶数元素乘以 10 后产生一个新的数组，可以采用下面两种方式来实现：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaApp</span> <span class="keyword">extends</span> <span class="title">App</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> a = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 1.方式一 yield 关键字</span></span><br><span class="line">  <span class="keyword">val</span> ints1 = <span class="keyword">for</span> (elem &lt;- a <span class="keyword">if</span> elem % <span class="number">2</span> == <span class="number">0</span>) <span class="keyword">yield</span> <span class="number">10</span> * elem</span><br><span class="line">  <span class="keyword">for</span> (elem &lt;- ints1) &#123;</span><br><span class="line">    println(elem)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 2.方式二 采用函数式编程的方式,这和 Java 8 中的函数式编程是类似的，这里采用下划线标表示其中的每个元素</span></span><br><span class="line">  <span class="keyword">val</span> ints2 = a.filter(_ % <span class="number">2</span> == <span class="number">0</span>).map(_ * <span class="number">10</span>)</span><br><span class="line">  <span class="keyword">for</span> (elem &lt;- ints1) &#123;</span><br><span class="line">    println(elem)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="五、多维数组"><a href="#五、多维数组" class="headerlink" title="五、多维数组"></a>五、多维数组</h2><p>和 Java 中一样，多维数组由单维数组组成。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaApp</span> <span class="keyword">extends</span> <span class="title">App</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> matrix = <span class="type">Array</span>(<span class="type">Array</span>(<span class="number">11</span>, <span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>, <span class="number">16</span>, <span class="number">17</span>, <span class="number">18</span>, <span class="number">19</span>, <span class="number">20</span>),</span><br><span class="line">    <span class="type">Array</span>(<span class="number">21</span>, <span class="number">22</span>, <span class="number">23</span>, <span class="number">24</span>, <span class="number">25</span>, <span class="number">26</span>, <span class="number">27</span>, <span class="number">28</span>, <span class="number">29</span>, <span class="number">30</span>),</span><br><span class="line">    <span class="type">Array</span>(<span class="number">31</span>, <span class="number">32</span>, <span class="number">33</span>, <span class="number">34</span>, <span class="number">35</span>, <span class="number">36</span>, <span class="number">37</span>, <span class="number">38</span>, <span class="number">39</span>, <span class="number">40</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> (elem &lt;- matrix) &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (elem &lt;- elem) &#123;</span><br><span class="line">      print(elem + <span class="string">"-"</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    println()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">打印输出如下：</span><br><span class="line"><span class="number">11</span><span class="number">-12</span><span class="number">-13</span><span class="number">-14</span><span class="number">-15</span><span class="number">-16</span><span class="number">-17</span><span class="number">-18</span><span class="number">-19</span><span class="number">-20</span>-</span><br><span class="line"><span class="number">21</span><span class="number">-22</span><span class="number">-23</span><span class="number">-24</span><span class="number">-25</span><span class="number">-26</span><span class="number">-27</span><span class="number">-28</span><span class="number">-29</span><span class="number">-30</span>-</span><br><span class="line"><span class="number">31</span><span class="number">-32</span><span class="number">-33</span><span class="number">-34</span><span class="number">-35</span><span class="number">-36</span><span class="number">-37</span><span class="number">-38</span><span class="number">-39</span><span class="number">-40</span>-</span><br></pre></td></tr></table></figure>
<h2 id="六、与Java互操作"><a href="#六、与Java互操作" class="headerlink" title="六、与Java互操作"></a>六、与Java互操作</h2><p>由于 Scala 的数组是使用 Java 的数组来实现的，所以两者之间可以相互转换。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scala.collection.mutable.<span class="type">ArrayBuffer</span></span><br><span class="line"><span class="keyword">import</span> scala.collection.&#123;<span class="type">JavaConverters</span>, mutable&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaApp</span> <span class="keyword">extends</span> <span class="title">App</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> element = <span class="type">ArrayBuffer</span>(<span class="string">"hadoop"</span>, <span class="string">"spark"</span>, <span class="string">"storm"</span>)</span><br><span class="line">  <span class="comment">// Scala 转 Java</span></span><br><span class="line">  <span class="keyword">val</span> javaList: util.<span class="type">List</span>[<span class="type">String</span>] = <span class="type">JavaConverters</span>.bufferAsJavaList(element)</span><br><span class="line">  <span class="comment">// Java 转 Scala</span></span><br><span class="line">  <span class="keyword">val</span> scalaBuffer: mutable.<span class="type">Buffer</span>[<span class="type">String</span>] = <span class="type">JavaConverters</span>.asScalaBuffer(javaList)</span><br><span class="line">  <span class="keyword">for</span> (elem &lt;- scalaBuffer) &#123;</span><br><span class="line">    println(elem)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol>
<li>Martin Odersky . Scala 编程 (第 3 版)[M] . 电子工业出版社 . 2018-1-1  </li>
<li>凯.S.霍斯特曼  . 快学 Scala(第 2 版)[M] . 电子工业出版社 . 2017-7</li>
</ol>
]]></content>
      <categories>
        <category>notes</category>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>Scala</tag>
      </tags>
  </entry>
  <entry>
    <title>Scala模式匹配</title>
    <url>/2021/03/18/Scala%E6%A8%A1%E5%BC%8F%E5%8C%B9%E9%85%8D/</url>
    <content><![CDATA[<h2 id="一、模式匹配"><a href="#一、模式匹配" class="headerlink" title="一、模式匹配"></a>一、模式匹配</h2><p>Scala 支持模式匹配机制，可以代替 swith 语句、执行类型检查、以及支持析构表达式等。</p>
<h3 id="1-1-更好的swith"><a href="#1-1-更好的swith" class="headerlink" title="1.1 更好的swith"></a>1.1 更好的swith</h3><p>Scala 不支持 swith，可以使用模式匹配 <code>match...case</code> 语法代替。但是 match 语句与 Java 中的 switch 有以下三点不同：</p>
<ul>
<li>Scala 中的 case 语句支持任何类型；而 Java 中 case 语句仅支持整型、枚举和字符串常量；</li>
<li>Scala 中每个分支语句后面不需要写 break，因为在 case 语句中 break 是隐含的，默认就有；</li>
<li>在 Scala 中 match 语句是有返回值的，而 Java 中 switch 语句是没有返回值的。如下：</li>
</ul>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaApp</span> <span class="keyword">extends</span> <span class="title">App</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">matchTest</span></span>(x: <span class="type">Int</span>) = x <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="number">1</span> =&gt; <span class="string">"one"</span></span><br><span class="line">    <span class="keyword">case</span> <span class="number">2</span> =&gt; <span class="string">"two"</span></span><br><span class="line">    <span class="keyword">case</span> _ <span class="keyword">if</span> x &gt; <span class="number">9</span> &amp;&amp; x &lt; <span class="number">100</span> =&gt; <span class="string">"两位数"</span>   <span class="comment">//支持条件表达式 这被称为模式守卫</span></span><br><span class="line">    <span class="keyword">case</span> _ =&gt; <span class="string">"other"</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  println(matchTest(<span class="number">1</span>))   <span class="comment">//输出 one</span></span><br><span class="line">  println(matchTest(<span class="number">10</span>))  <span class="comment">//输出 两位数</span></span><br><span class="line">  println(matchTest(<span class="number">200</span>)) <span class="comment">//输出 other</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="1-2-用作类型检查"><a href="#1-2-用作类型检查" class="headerlink" title="1.2 用作类型检查"></a>1.2 用作类型检查</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaApp</span> <span class="keyword">extends</span> <span class="title">App</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">matchTest</span></span>[<span class="type">T</span>](x: <span class="type">T</span>) = x <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> x: <span class="type">Int</span> =&gt; <span class="string">"数值型"</span></span><br><span class="line">    <span class="keyword">case</span> x: <span class="type">String</span> =&gt; <span class="string">"字符型"</span></span><br><span class="line">    <span class="keyword">case</span> x: <span class="type">Float</span> =&gt; <span class="string">"浮点型"</span></span><br><span class="line">    <span class="keyword">case</span> _ =&gt; <span class="string">"other"</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  println(matchTest(<span class="number">1</span>))     <span class="comment">//输出 数值型</span></span><br><span class="line">  println(matchTest(<span class="number">10.3</span>f)) <span class="comment">//输出 浮点型</span></span><br><span class="line">  println(matchTest(<span class="string">"str"</span>)) <span class="comment">//输出 字符型</span></span><br><span class="line">  println(matchTest(<span class="number">2.1</span>))   <span class="comment">//输出 other</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="1-3-匹配数据结构"><a href="#1-3-匹配数据结构" class="headerlink" title="1.3 匹配数据结构"></a>1.3 匹配数据结构</h3><p>匹配元组示例：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaApp</span> <span class="keyword">extends</span> <span class="title">App</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">matchTest</span></span>(x: <span class="type">Any</span>) = x <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> (<span class="number">0</span>, _, _) =&gt; <span class="string">"匹配第一个元素为 0 的元组"</span></span><br><span class="line">    <span class="keyword">case</span> (a, b, c) =&gt; println(a + <span class="string">"~"</span> + b + <span class="string">"~"</span> + c)</span><br><span class="line">    <span class="keyword">case</span> _ =&gt; <span class="string">"other"</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  println(matchTest((<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>)))             <span class="comment">// 输出: 匹配第一个元素为 0 的元组</span></span><br><span class="line">  matchTest((<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>))                      <span class="comment">// 输出: 1~2~3</span></span><br><span class="line">  println(matchTest(<span class="type">Array</span>(<span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>, <span class="number">14</span>))) <span class="comment">// 输出: other</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>匹配数组示例：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaApp</span> <span class="keyword">extends</span> <span class="title">App</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">matchTest</span></span>[<span class="type">T</span>](x: <span class="type">Array</span>[<span class="type">T</span>]) = x <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">Array</span>(<span class="number">0</span>) =&gt; <span class="string">"匹配只有一个元素 0 的数组"</span></span><br><span class="line">    <span class="keyword">case</span> <span class="type">Array</span>(a, b) =&gt; println(a + <span class="string">"~"</span> + b)</span><br><span class="line">    <span class="keyword">case</span> <span class="type">Array</span>(<span class="number">10</span>, _*) =&gt; <span class="string">"第一个元素为 10 的数组"</span></span><br><span class="line">    <span class="keyword">case</span> _ =&gt; <span class="string">"other"</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  println(matchTest(<span class="type">Array</span>(<span class="number">0</span>)))          <span class="comment">// 输出: 匹配只有一个元素 0 的数组</span></span><br><span class="line">  matchTest(<span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>))                <span class="comment">// 输出: 1~2</span></span><br><span class="line">  println(matchTest(<span class="type">Array</span>(<span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>))) <span class="comment">// 输出: 第一个元素为 10 的数组</span></span><br><span class="line">  println(matchTest(<span class="type">Array</span>(<span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>)))    <span class="comment">// 输出: other</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="1-4-提取器"><a href="#1-4-提取器" class="headerlink" title="1.4 提取器"></a>1.4 提取器</h3><p>数组、列表和元组能使用模式匹配，都是依靠提取器 (extractor) 机制，它们伴生对象中定义了 <code>unapply</code> 或 <code>unapplySeq</code> 方法：</p>
<ul>
<li><strong>unapply</strong>：用于提取固定数量的对象；</li>
<li><strong>unapplySeq</strong>：用于提取一个序列；</li>
</ul>
<p>这里以数组为例，<code>Array.scala</code> 定义了 <code>unapplySeq</code> 方法：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">unapplySeq</span></span>[<span class="type">T</span>](x : scala.<span class="type">Array</span>[<span class="type">T</span>]) : scala.<span class="type">Option</span>[scala.<span class="type">IndexedSeq</span>[<span class="type">T</span>]] = &#123; <span class="comment">/* compiled code */</span> &#125;</span><br></pre></td></tr></table></figure>
<p><code>unapplySeq</code> 返回一个序列，包含数组中的所有值，这样在模式匹配时，才能知道对应位置上的值。</p>
<h2 id="二、样例类"><a href="#二、样例类" class="headerlink" title="二、样例类"></a>二、样例类</h2><h3 id="2-1-样例类"><a href="#2-1-样例类" class="headerlink" title="2.1 样例类"></a>2.1 样例类</h3><p>样例类是一种的特殊的类，它们被经过优化以用于模式匹配，样例类的声明比较简单，只需要在 <code>class</code> 前面加上关键字 <code>case</code>。下面给出一个样例类及其用于模式匹配的示例：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//声明一个抽象类</span></span><br><span class="line"><span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span></span>&#123;&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 样例类 Employee</span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Employee</span>(<span class="params">name: <span class="type">String</span>, age: <span class="type">Int</span>, salary: <span class="type">Double</span></span>) <span class="keyword">extends</span> <span class="title">Person</span> </span>&#123;&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 样例类 Student</span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Student</span>(<span class="params">name: <span class="type">String</span>, age: <span class="type">Int</span></span>) <span class="keyword">extends</span> <span class="title">Person</span> </span>&#123;&#125;</span><br></pre></td></tr></table></figure>
<p>当你声明样例类后，编译器自动进行以下配置：</p>
<ul>
<li>构造器中每个参数都默认为 <code>val</code>；</li>
<li>自动地生成 <code>equals, hashCode, toString, copy</code> 等方法；</li>
<li>伴生对象中自动生成 <code>apply</code> 方法，使得可以不用 new 关键字就能构造出相应的对象；</li>
<li>伴生对象中自动生成 <code>unapply</code> 方法，以支持模式匹配。</li>
</ul>
<p>除了上面的特征外，样例类和其他类相同，可以任意添加方法和字段，扩展它们。</p>
<h3 id="2-3-用于模式匹配"><a href="#2-3-用于模式匹配" class="headerlink" title="2.3 用于模式匹配"></a>2.3 用于模式匹配</h3><p>样例的伴生对象中自动生成 <code>unapply</code> 方法，所以样例类可以支持模式匹配，使用如下：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaApp</span> <span class="keyword">extends</span> <span class="title">App</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">matchTest</span></span>(person: <span class="type">Person</span>) = person <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">Student</span>(name, _) =&gt; <span class="string">"student:"</span> + name</span><br><span class="line">    <span class="keyword">case</span> <span class="type">Employee</span>(_, _, salary) =&gt; <span class="string">"employee salary:"</span> + salary</span><br><span class="line">    <span class="keyword">case</span> _ =&gt; <span class="string">"other"</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  println(matchTest(<span class="type">Student</span>(<span class="string">"heibai"</span>, <span class="number">12</span>)))        <span class="comment">//输出: student:heibai</span></span><br><span class="line">  println(matchTest(<span class="type">Employee</span>(<span class="string">"ying"</span>, <span class="number">22</span>, <span class="number">999999</span>))) <span class="comment">//输出: employee salary:999999.0</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol>
<li>Martin Odersky . Scala 编程 (第 3 版)[M] . 电子工业出版社 . 2018-1-1  </li>
<li>凯.S.霍斯特曼  . 快学 Scala(第 2 版)[M] . 电子工业出版社 . 2017-7</li>
</ol>
]]></content>
      <categories>
        <category>notes</category>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>Scala</tag>
      </tags>
  </entry>
  <entry>
    <title>Scala Map &amp; Tuple</title>
    <url>/2021/03/18/Scala%E6%98%A0%E5%B0%84%E5%92%8C%E5%85%83%E7%BB%84/</url>
    <content><![CDATA[<h2 id="一、映射-Map"><a href="#一、映射-Map" class="headerlink" title="一、映射(Map)"></a>一、映射(Map)</h2><h3 id="1-1-构造Map"><a href="#1-1-构造Map" class="headerlink" title="1.1 构造Map"></a>1.1 构造Map</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 初始化一个空 map</span></span><br><span class="line"><span class="keyword">val</span> scores01 = <span class="keyword">new</span> <span class="type">HashMap</span>[<span class="type">String</span>, <span class="type">Int</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">// 从指定的值初始化 Map（方式一）</span></span><br><span class="line"><span class="keyword">val</span> scores02 = <span class="type">Map</span>(<span class="string">"hadoop"</span> -&gt; <span class="number">10</span>, <span class="string">"spark"</span> -&gt; <span class="number">20</span>, <span class="string">"storm"</span> -&gt; <span class="number">30</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 从指定的值初始化 Map（方式二）</span></span><br><span class="line"><span class="keyword">val</span> scores03 = <span class="type">Map</span>((<span class="string">"hadoop"</span>, <span class="number">10</span>), (<span class="string">"spark"</span>, <span class="number">20</span>), (<span class="string">"storm"</span>, <span class="number">30</span>))</span><br></pre></td></tr></table></figure>
<p>采用上面方式得到的都是不可变 Map(immutable map)，想要得到可变 Map(mutable map)，则需要使用：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> scores04 = scala.collection.mutable.<span class="type">Map</span>(<span class="string">"hadoop"</span> -&gt; <span class="number">10</span>, <span class="string">"spark"</span> -&gt; <span class="number">20</span>, <span class="string">"storm"</span> -&gt; <span class="number">30</span>)</span><br></pre></td></tr></table></figure>
<h3 id="1-2-获取值"><a href="#1-2-获取值" class="headerlink" title="1.2 获取值"></a>1.2 获取值</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaApp</span> <span class="keyword">extends</span> <span class="title">App</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> scores = <span class="type">Map</span>(<span class="string">"hadoop"</span> -&gt; <span class="number">10</span>, <span class="string">"spark"</span> -&gt; <span class="number">20</span>, <span class="string">"storm"</span> -&gt; <span class="number">30</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 1.获取指定 key 对应的值</span></span><br><span class="line">  println(scores(<span class="string">"hadoop"</span>))</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 2. 如果对应的值不存在则使用默认值</span></span><br><span class="line">  println(scores.getOrElse(<span class="string">"hadoop01"</span>, <span class="number">100</span>))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="1-3-新增-修改-删除值"><a href="#1-3-新增-修改-删除值" class="headerlink" title="1.3 新增/修改/删除值"></a>1.3 新增/修改/删除值</h3><p>可变 Map 允许进行新增、修改、删除等操作。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaApp</span> <span class="keyword">extends</span> <span class="title">App</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> scores = scala.collection.mutable.<span class="type">Map</span>(<span class="string">"hadoop"</span> -&gt; <span class="number">10</span>, <span class="string">"spark"</span> -&gt; <span class="number">20</span>, <span class="string">"storm"</span> -&gt; <span class="number">30</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 1.如果 key 存在则更新</span></span><br><span class="line">  scores(<span class="string">"hadoop"</span>) = <span class="number">100</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// 2.如果 key 不存在则新增</span></span><br><span class="line">  scores(<span class="string">"flink"</span>) = <span class="number">40</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// 3.可以通过 += 来进行多个更新或新增操作</span></span><br><span class="line">  scores += (<span class="string">"spark"</span> -&gt; <span class="number">200</span>, <span class="string">"hive"</span> -&gt; <span class="number">50</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 4.可以通过 -= 来移除某个键和值</span></span><br><span class="line">  scores -= <span class="string">"storm"</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> (elem &lt;- scores) &#123;println(elem)&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出内容如下</span></span><br><span class="line">(spark,<span class="number">200</span>)</span><br><span class="line">(hadoop,<span class="number">100</span>)</span><br><span class="line">(flink,<span class="number">40</span>)</span><br><span class="line">(hive,<span class="number">50</span>)</span><br></pre></td></tr></table></figure>
<p>不可变 Map 不允许进行新增、修改、删除等操作，但是允许由不可变 Map 产生新的 Map。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaApp</span> <span class="keyword">extends</span> <span class="title">App</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> scores = <span class="type">Map</span>(<span class="string">"hadoop"</span> -&gt; <span class="number">10</span>, <span class="string">"spark"</span> -&gt; <span class="number">20</span>, <span class="string">"storm"</span> -&gt; <span class="number">30</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> newScores = scores + (<span class="string">"spark"</span> -&gt; <span class="number">200</span>, <span class="string">"hive"</span> -&gt; <span class="number">50</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> (elem &lt;- scores) &#123;println(elem)&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出内容如下</span></span><br><span class="line">(hadoop,<span class="number">10</span>)</span><br><span class="line">(spark,<span class="number">200</span>)</span><br><span class="line">(storm,<span class="number">30</span>)</span><br><span class="line">(hive,<span class="number">50</span>)</span><br></pre></td></tr></table></figure>
<h3 id="1-4-遍历Map"><a href="#1-4-遍历Map" class="headerlink" title="1.4 遍历Map"></a>1.4 遍历Map</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">object ScalaApp extends App &#123;</span><br><span class="line"></span><br><span class="line">  val scores = Map(<span class="string">"hadoop"</span> -&gt; <span class="number">10</span>, <span class="string">"spark"</span> -&gt; <span class="number">20</span>, <span class="string">"storm"</span> -&gt; <span class="number">30</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 1. 遍历键</span></span><br><span class="line">  <span class="keyword">for</span> (key &lt;- scores.keys) &#123; println(key) &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 2. 遍历值</span></span><br><span class="line">  <span class="keyword">for</span> (value &lt;- scores.values) &#123; println(value) &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 3. 遍历键值对</span></span><br><span class="line">  <span class="keyword">for</span> ((key, value) &lt;- scores) &#123; println(key + <span class="string">":"</span> + value) &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="1-5-yield关键字"><a href="#1-5-yield关键字" class="headerlink" title="1.5 yield关键字"></a>1.5 yield关键字</h3><p>可以使用 <code>yield</code> 关键字从现有 Map 产生新的 Map。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaApp</span> <span class="keyword">extends</span> <span class="title">App</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> scores = <span class="type">Map</span>(<span class="string">"hadoop"</span> -&gt; <span class="number">10</span>, <span class="string">"spark"</span> -&gt; <span class="number">20</span>, <span class="string">"storm"</span> -&gt; <span class="number">30</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 1.将 scores 中所有的值扩大 10 倍</span></span><br><span class="line">  <span class="keyword">val</span> newScore = <span class="keyword">for</span> ((key, value) &lt;- scores) <span class="keyword">yield</span> (key, value * <span class="number">10</span>)</span><br><span class="line">  <span class="keyword">for</span> (elem &lt;- newScore) &#123; println(elem) &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="comment">// 2.将键和值互相调换</span></span><br><span class="line">  <span class="keyword">val</span> reversalScore: <span class="type">Map</span>[<span class="type">Int</span>, <span class="type">String</span>] = <span class="keyword">for</span> ((key, value) &lt;- scores) <span class="keyword">yield</span> (value, key)</span><br><span class="line">  <span class="keyword">for</span> (elem &lt;- reversalScore) &#123; println(elem) &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出</span></span><br><span class="line">(hadoop,<span class="number">100</span>)</span><br><span class="line">(spark,<span class="number">200</span>)</span><br><span class="line">(storm,<span class="number">300</span>)</span><br><span class="line"></span><br><span class="line">(<span class="number">10</span>,hadoop)</span><br><span class="line">(<span class="number">20</span>,spark)</span><br><span class="line">(<span class="number">30</span>,storm)</span><br></pre></td></tr></table></figure>
<h3 id="1-6-其他Map结构"><a href="#1-6-其他Map结构" class="headerlink" title="1.6 其他Map结构"></a>1.6 其他Map结构</h3><p>在使用 Map 时候，如果不指定，默认使用的是 HashMap，如果想要使用 <code>TreeMap</code> 或者 <code>LinkedHashMap</code>，则需要显式的指定。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaApp</span> <span class="keyword">extends</span> <span class="title">App</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 1.使用 TreeMap,按照键的字典序进行排序</span></span><br><span class="line">  <span class="keyword">val</span> scores01 = scala.collection.mutable.<span class="type">TreeMap</span>(<span class="string">"B"</span> -&gt; <span class="number">20</span>, <span class="string">"A"</span> -&gt; <span class="number">10</span>, <span class="string">"C"</span> -&gt; <span class="number">30</span>)</span><br><span class="line">  <span class="keyword">for</span> (elem &lt;- scores01) &#123;println(elem)&#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 2.使用 LinkedHashMap,按照键值对的插入顺序进行排序</span></span><br><span class="line">  <span class="keyword">val</span> scores02 = scala.collection.mutable.<span class="type">LinkedHashMap</span>(<span class="string">"B"</span> -&gt; <span class="number">20</span>, <span class="string">"A"</span> -&gt; <span class="number">10</span>, <span class="string">"C"</span> -&gt; <span class="number">30</span>)</span><br><span class="line">  <span class="keyword">for</span> (elem &lt;- scores02) &#123;println(elem)&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出</span></span><br><span class="line">(<span class="type">A</span>,<span class="number">10</span>)</span><br><span class="line">(<span class="type">B</span>,<span class="number">20</span>)</span><br><span class="line">(<span class="type">C</span>,<span class="number">30</span>)</span><br><span class="line"></span><br><span class="line">(<span class="type">B</span>,<span class="number">20</span>)</span><br><span class="line">(<span class="type">A</span>,<span class="number">10</span>)</span><br><span class="line">(<span class="type">C</span>,<span class="number">30</span>)</span><br></pre></td></tr></table></figure>
<h3 id="1-7-可选方法"><a href="#1-7-可选方法" class="headerlink" title="1.7 可选方法"></a>1.7 可选方法</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaApp</span> <span class="keyword">extends</span> <span class="title">App</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> scores = scala.collection.mutable.<span class="type">TreeMap</span>(<span class="string">"B"</span> -&gt; <span class="number">20</span>, <span class="string">"A"</span> -&gt; <span class="number">10</span>, <span class="string">"C"</span> -&gt; <span class="number">30</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 1. 获取长度</span></span><br><span class="line">  println(scores.size)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 2. 判断是否为空</span></span><br><span class="line">  println(scores.isEmpty)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 3. 判断是否包含特定的 key</span></span><br><span class="line">  println(scores.contains(<span class="string">"A"</span>))</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="1-8-与Java互操作"><a href="#1-8-与Java互操作" class="headerlink" title="1.8 与Java互操作"></a>1.8 与Java互操作</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util</span><br><span class="line"><span class="keyword">import</span> scala.collection.&#123;<span class="type">JavaConverters</span>, mutable&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaApp</span> <span class="keyword">extends</span> <span class="title">App</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> scores = <span class="type">Map</span>(<span class="string">"hadoop"</span> -&gt; <span class="number">10</span>, <span class="string">"spark"</span> -&gt; <span class="number">20</span>, <span class="string">"storm"</span> -&gt; <span class="number">30</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// scala map 转 java map</span></span><br><span class="line">  <span class="keyword">val</span> javaMap: util.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">Int</span>] = <span class="type">JavaConverters</span>.mapAsJavaMap(scores)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// java map 转 scala map</span></span><br><span class="line">  <span class="keyword">val</span> scalaMap: mutable.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">Int</span>] = <span class="type">JavaConverters</span>.mapAsScalaMap(javaMap)</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">for</span> (elem &lt;- scalaMap) &#123;println(elem)&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="二、元组-Tuple"><a href="#二、元组-Tuple" class="headerlink" title="二、元组(Tuple)"></a>二、元组(Tuple)</h2><p>元组与数组类似，但是数组中所有的元素必须是同一种类型，而元组则可以包含不同类型的元素。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> tuple=(<span class="number">1</span>,<span class="number">3.24</span>f,<span class="string">"scala"</span>)</span><br><span class="line">tuple: (<span class="type">Int</span>, <span class="type">Float</span>, <span class="type">String</span>) = (<span class="number">1</span>,<span class="number">3.24</span>,scala)</span><br></pre></td></tr></table></figure>
<h3 id="2-1-模式匹配"><a href="#2-1-模式匹配" class="headerlink" title="2.1  模式匹配"></a>2.1  模式匹配</h3><p>可以通过模式匹配来获取元组中的值并赋予对应的变量：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> (a,b,c)=tuple</span><br><span class="line">a: <span class="type">Int</span> = <span class="number">1</span></span><br><span class="line">b: <span class="type">Float</span> = <span class="number">3.24</span></span><br><span class="line">c: <span class="type">String</span> = scala</span><br></pre></td></tr></table></figure>
<p>如果某些位置不需要赋值，则可以使用下划线代替：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> (a,_,_)=tuple</span><br><span class="line">a: <span class="type">Int</span> = <span class="number">1</span></span><br></pre></td></tr></table></figure>
<h3 id="2-2-zip方法"><a href="#2-2-zip方法" class="headerlink" title="2.2 zip方法"></a>2.2 zip方法</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaApp</span> <span class="keyword">extends</span> <span class="title">App</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">   <span class="keyword">val</span> array01 = <span class="type">Array</span>(<span class="string">"hadoop"</span>, <span class="string">"spark"</span>, <span class="string">"storm"</span>)</span><br><span class="line">  <span class="keyword">val</span> array02 = <span class="type">Array</span>(<span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span>)</span><br><span class="line">    </span><br><span class="line">  <span class="comment">// 1.zip 方法得到的是多个 tuple 组成的数组</span></span><br><span class="line">  <span class="keyword">val</span> tuples: <span class="type">Array</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = array01.zip(array02)</span><br><span class="line">  <span class="comment">// 2.也可以在 zip 后调用 toMap 方法转换为 Map</span></span><br><span class="line">  <span class="keyword">val</span> map: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">Int</span>] = array01.zip(array02).toMap</span><br><span class="line">    </span><br><span class="line">  <span class="keyword">for</span> (elem &lt;- tuples) &#123; println(elem) &#125;</span><br><span class="line">  <span class="keyword">for</span> (elem &lt;- map) &#123;println(elem)&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出</span></span><br><span class="line">(hadoop,<span class="number">10</span>)</span><br><span class="line">(spark,<span class="number">20</span>)</span><br><span class="line">(storm,<span class="number">30</span>)</span><br><span class="line"></span><br><span class="line">(hadoop,<span class="number">10</span>)</span><br><span class="line">(spark,<span class="number">20</span>)</span><br><span class="line">(storm,<span class="number">30</span>)</span><br></pre></td></tr></table></figure>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol>
<li>Martin Odersky . Scala 编程 (第 3 版)[M] . 电子工业出版社 . 2018-1-1  </li>
<li>凯.S.霍斯特曼  . 快学 Scala(第 2 版)[M] . 电子工业出版社 . 2017-7</li>
</ol>
]]></content>
      <categories>
        <category>notes</category>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>Scala</tag>
      </tags>
  </entry>
  <entry>
    <title>Scala简介及开发环境配置</title>
    <url>/2021/03/18/Scala%E7%AE%80%E4%BB%8B%E5%8F%8A%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/</url>
    <content><![CDATA[<h2 id="一、Scala简介"><a href="#一、Scala简介" class="headerlink" title="一、Scala简介"></a>一、Scala简介</h2><h3 id="1-1-概念"><a href="#1-1-概念" class="headerlink" title="1.1 概念"></a>1.1 概念</h3><p>Scala 全称为 Scalable Language，即“可伸缩的语言”，之所以这样命名，是因为它的设计目标是希望伴随着用户的需求一起成长。Scala 是一门综合了<strong>面向对象</strong>和<strong>函数式编程概念</strong>的<strong>静态类型</strong>的编程语言，它运行在标准的 Java 平台上，可以与所有的 Java 类库无缝协作。</p>
<h3 id="1-2-特点"><a href="#1-2-特点" class="headerlink" title="1.2 特点"></a>1.2 特点</h3><h4 id="1-Scala是面向对象的"><a href="#1-Scala是面向对象的" class="headerlink" title="1. Scala是面向对象的"></a>1. Scala是面向对象的</h4><p>Scala 是一种面向对象的语言，每个值都是对象，每个方法都是调用。举例来说，如果你执行 <code>1+2</code>，则对于 Scala 而言，实际是在调用 Int 类里定义的名为 <code>+</code> 的方法。</p>
<h4 id="2-Scala是函数式的"><a href="#2-Scala是函数式的" class="headerlink" title="2. Scala是函数式的"></a>2. Scala是函数式的</h4><p>Scala 不只是一门纯的面对对象的语言，它也是功能完整的函数式编程语言。函数式编程以两大核心理念为指导：</p>
<ul>
<li>函数是一等公民；</li>
<li>程序中的操作应该将输入值映射成输出值，而不是当场修改数据。即方法不应该有副作用。</li>
</ul>
<h3 id="1-3-Scala的优点"><a href="#1-3-Scala的优点" class="headerlink" title="1.3 Scala的优点"></a>1.3 Scala的优点</h3><h4 id="1-与Java的兼容"><a href="#1-与Java的兼容" class="headerlink" title="1. 与Java的兼容"></a>1. 与Java的兼容</h4><p>Scala 可以与 Java 无缝对接，其在执行时会被编译成 JVM 字节码，这使得其性能与 Java 相当。Scala 可以直接调用 Java 中的方法、访问 Java 中的字段、继承 Java 类、实现 Java 接口。Scala 重度复用并包装了原生的 Java 类型，并支持隐式转换。</p>
<h4 id="2-精简的语法"><a href="#2-精简的语法" class="headerlink" title="2. 精简的语法"></a>2. 精简的语法</h4><p>Scala 的程序通常比较简洁，相比 Java 而言，代码行数会大大减少，这使得程序员对代码的阅读和理解更快，缺陷也更少。</p>
<h4 id="3-高级语言的特性"><a href="#3-高级语言的特性" class="headerlink" title="3. 高级语言的特性"></a>3. 高级语言的特性</h4><p>Scala 具有高级语言的特定，对代码进行了高级别的抽象，能够让你更好地控制程序的复杂度，保证开发的效率。</p>
<h4 id="4-静态类型"><a href="#4-静态类型" class="headerlink" title="4. 静态类型"></a>4. 静态类型</h4><p>Scala 拥有非常先进的静态类型系统，Scala 不仅拥有与 Java 类似的允许嵌套类的类型系统，还支持使用泛型对类型进行参数化，用交集（intersection）来组合类型，以及使用抽象类型来进行隐藏类型的细节。通过这些特性，可以更快地设计出安全易用的程序和接口。</p>
<h2 id="二、配置IDEA开发环境"><a href="#二、配置IDEA开发环境" class="headerlink" title="二、配置IDEA开发环境"></a>二、配置IDEA开发环境</h2><h3 id="2-1-前置条件"><a href="#2-1-前置条件" class="headerlink" title="2.1 前置条件"></a>2.1 前置条件</h3><p>Scala 的运行依赖于 JDK，Scala 2.12.x 需要 JDK 1.8+。</p>
<h3 id="2-2-安装Scala插件"><a href="#2-2-安装Scala插件" class="headerlink" title="2.2 安装Scala插件"></a>2.2 安装Scala插件</h3><p>IDEA 默认不支持 Scala 语言的开发，需要通过插件进行扩展。打开 IDEA，依次点击 <strong>File</strong> =&gt; <strong>settings</strong>=&gt; <strong>plugins</strong> 选项卡，搜索 Scala 插件 (如下图)。找到插件后进行安装，并重启 IDEA 使得安装生效。</p>
<div align="center"> <img width="700px" src="../pictures/idea-scala-plugin.png"> </div>



<h3 id="2-3-创建Scala项目"><a href="#2-3-创建Scala项目" class="headerlink" title="2.3 创建Scala项目"></a>2.3 创建Scala项目</h3><p>在 IDEA 中依次点击 <strong>File</strong> =&gt; <strong>New</strong> =&gt; <strong>Project</strong> 选项卡，然后选择创建 <code>Scala—IDEA</code> 工程：</p>
<div align="center"> <img width="700px" src="../pictures/idea-newproject-scala.png"> </div>



<h3 id="2-4-下载Scala-SDK"><a href="#2-4-下载Scala-SDK" class="headerlink" title="2.4 下载Scala SDK"></a>2.4 下载Scala SDK</h3><h4 id="1-方式一"><a href="#1-方式一" class="headerlink" title="1. 方式一"></a>1. 方式一</h4><p>此时看到 <code>Scala SDK</code> 为空，依次点击 <code>Create</code> =&gt; <code>Download</code> ，选择所需的版本后，点击 <code>OK</code> 按钮进行下载，下载完成点击 <code>Finish</code> 进入工程。</p>
<div align="center"> <img width="700px" src="../pictures/idea-scala-select.png"> </div>



<h4 id="2-方式二"><a href="#2-方式二" class="headerlink" title="2. 方式二"></a>2. 方式二</h4><p>方式一是 Scala 官方安装指南里使用的方式，但下载速度通常比较慢，且这种安装下并没有直接提供 Scala 命令行工具。所以个人推荐到官网下载安装包进行安装，下载地址：<a href="https://www.scala-lang.org/download/" target="_blank" rel="noopener">https://www.scala-lang.org/download/</a></p>
<p>这里我的系统是 Windows，下载 msi 版本的安装包后，一直点击下一步进行安装，安装完成后会自动配置好环境变量。</p>
<div align="center"> <img width="700px" src="../pictures/scala-other-resources.png"> </div>



<p>由于安装时已经自动配置好环境变量，所以 IDEA 会自动选择对应版本的 SDK。</p>
<div align="center"> <img width="700px" src="../pictures/idea-scala-2.1.8.png"> </div>



<h3 id="2-5-创建Hello-World"><a href="#2-5-创建Hello-World" class="headerlink" title="2.5 创建Hello World"></a>2.5 创建Hello World</h3><p>在工程 <code>src</code> 目录上右击 <strong>New</strong> =&gt; <strong>Scala class</strong> 创建 <code>Hello.scala</code>。输入代码如下，完成后点击运行按钮，成功运行则代表搭建成功。</p>
<div align="center"> <img width="700px" src="../pictures/scala-hello-world.png"> </div>





<h3 id="2-6-切换Scala版本"><a href="#2-6-切换Scala版本" class="headerlink" title="2.6 切换Scala版本"></a>2.6 切换Scala版本</h3><p>在日常的开发中，由于对应软件（如 Spark）的版本切换，可能导致需要切换 Scala 的版本，则可以在 <code>Project Structures</code> 中的 <code>Global Libraries</code> 选项卡中进行切换。</p>
<div align="center"> <img width="700px" src="../pictures/idea-scala-change.png"> </div>





<h3 id="2-7-使用scala命令行"><a href="#2-7-使用scala命令行" class="headerlink" title="2.7 使用scala命令行"></a>2.7 使用scala命令行</h3><p>采用 <code>msi</code> 方式安装，程序会自动配置好环境变量。此时可以直接使用命令行工具：</p>
<div align="center"> <img width="700px" src="../pictures/scala-shell.png"> </div>



<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol>
<li>Martin Odersky(著)，高宇翔 (译) . Scala 编程 (第 3 版)[M] . 电子工业出版社 . 2018-1-1 </li>
<li><a href="https://www.scala-lang.org/download/" target="_blank" rel="noopener">https://www.scala-lang.org/download/</a></li>
</ol>
]]></content>
      <categories>
        <category>notes</category>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>Scala</tag>
      </tags>
  </entry>
  <entry>
    <title>类型参数</title>
    <url>/2021/03/18/Scala%E7%B1%BB%E5%9E%8B%E5%8F%82%E6%95%B0/</url>
    <content><![CDATA[<h2 id="一、泛型"><a href="#一、泛型" class="headerlink" title="一、泛型"></a>一、泛型</h2><p>Scala 支持类型参数化，使得我们能够编写泛型程序。</p>
<h3 id="1-1-泛型类"><a href="#1-1-泛型类" class="headerlink" title="1.1 泛型类"></a>1.1 泛型类</h3><p>Java 中使用 <code>&lt;&gt;</code> 符号来包含定义的类型参数，Scala 则使用 <code>[]</code>。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Pair</span>[<span class="type">T</span>, <span class="type">S</span>](<span class="params">val first: <span class="type">T</span>, val second: <span class="type">S</span></span>) </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">toString</span></span>: <span class="type">String</span> = first + <span class="string">":"</span> + second</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaApp</span> <span class="keyword">extends</span> <span class="title">App</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 使用时候你直接指定参数类型，也可以不指定，由程序自动推断</span></span><br><span class="line">  <span class="keyword">val</span> pair01 = <span class="keyword">new</span> <span class="type">Pair</span>(<span class="string">"heibai01"</span>, <span class="number">22</span>)</span><br><span class="line">  <span class="keyword">val</span> pair02 = <span class="keyword">new</span> <span class="type">Pair</span>[<span class="type">String</span>,<span class="type">Int</span>](<span class="string">"heibai02"</span>, <span class="number">33</span>)</span><br><span class="line"></span><br><span class="line">  println(pair01)</span><br><span class="line">  println(pair02)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="1-2-泛型方法"><a href="#1-2-泛型方法" class="headerlink" title="1.2 泛型方法"></a>1.2 泛型方法</h3><p>函数和方法也支持类型参数。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Utils</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getHalf</span></span>[<span class="type">T</span>](a: <span class="type">Array</span>[<span class="type">T</span>]): <span class="type">Int</span> = a.length / <span class="number">2</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="二、类型限定"><a href="#二、类型限定" class="headerlink" title="二、类型限定"></a>二、类型限定</h2><h3 id="2-1-类型上界限定"><a href="#2-1-类型上界限定" class="headerlink" title="2.1 类型上界限定"></a>2.1 类型上界限定</h3><p>Scala 和 Java 一样，对于对象之间进行大小比较，要求被比较的对象实现 <code>java.lang.Comparable</code> 接口。所以如果想对泛型进行比较，需要限定类型上界为 <code>java.lang.Comparable</code>，语法为 <code>S &lt;: T</code>，代表类型 S 是类型 T 的子类或其本身。示例如下：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 使用 &lt;: 符号，限定 T 必须是 Comparable[T]的子类型</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Pair</span>[<span class="type">T</span> &lt;: <span class="type">Comparable</span>[<span class="type">T</span>]](<span class="params">val first: <span class="type">T</span>, val second: <span class="type">T</span></span>) </span>&#123;</span><br><span class="line">  <span class="comment">// 返回较小的值</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">smaller</span></span>: <span class="type">T</span> = <span class="keyword">if</span> (first.compareTo(second) &lt; <span class="number">0</span>) first <span class="keyword">else</span> second</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 测试代码</span></span><br><span class="line"><span class="keyword">val</span> pair = <span class="keyword">new</span> <span class="type">Pair</span>(<span class="string">"abc"</span>, <span class="string">"abcd"</span>)</span><br><span class="line">println(pair.smaller) <span class="comment">// 输出 abc</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>扩展：如果你想要在 Java 中实现类型变量限定，需要使用关键字 extends 来实现，等价的 Java 代码如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">&gt;<span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Pair</span>&lt;<span class="title">T</span> <span class="keyword">extends</span> <span class="title">Comparable</span>&lt;<span class="title">T</span>&gt;&gt; </span>&#123;</span><br><span class="line">&gt;    <span class="keyword">private</span> T first;</span><br><span class="line">&gt;    <span class="keyword">private</span> T second;</span><br><span class="line">&gt;    Pair(T first, T second) &#123;</span><br><span class="line">&gt;        <span class="keyword">this</span>.first = first;</span><br><span class="line">&gt;        <span class="keyword">this</span>.second = second;</span><br><span class="line">&gt;    &#125;</span><br><span class="line">&gt;    <span class="function"><span class="keyword">public</span> T <span class="title">smaller</span><span class="params">()</span> </span>&#123;</span><br><span class="line">&gt;        <span class="keyword">return</span> first.compareTo(second) &lt; <span class="number">0</span> ? first : second;</span><br><span class="line">&gt;     &#125;</span><br><span class="line">&gt;&#125;</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<h3 id="2-2-视图界定"><a href="#2-2-视图界定" class="headerlink" title="2.2 视图界定"></a>2.2 视图界定</h3><p>在上面的例子中，如果你使用 Int 类型或者 Double 等类型进行测试，点击运行后，你会发现程序根本无法通过编译：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> pair1 = <span class="keyword">new</span> <span class="type">Pair</span>(<span class="number">10</span>, <span class="number">12</span>)</span><br><span class="line"><span class="keyword">val</span> pair2 = <span class="keyword">new</span> <span class="type">Pair</span>(<span class="number">10.0</span>, <span class="number">12.0</span>)</span><br></pre></td></tr></table></figure>
<p>之所以出现这样的问题，是因为 Scala 中的 Int 类并没有实现 Comparable 接口。在 Scala 中直接继承 Comparable 接口的是特质 Ordered，它在继承 compareTo 方法的基础上，额外定义了关系符方法，源码如下:</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 除了 compareTo 方法外，还提供了额外的关系符方法</span></span><br><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">Ordered</span>[<span class="type">A</span>] <span class="keyword">extends</span> <span class="title">Any</span> <span class="keyword">with</span> <span class="title">java</span>.<span class="title">lang</span>.<span class="title">Comparable</span>[<span class="type">A</span>] </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">compare</span></span>(that: <span class="type">A</span>): <span class="type">Int</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">&lt;</span>  </span>(that: <span class="type">A</span>): <span class="type">Boolean</span> = (<span class="keyword">this</span> compare that) &lt;  <span class="number">0</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">&gt;</span>  </span>(that: <span class="type">A</span>): <span class="type">Boolean</span> = (<span class="keyword">this</span> compare that) &gt;  <span class="number">0</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">&lt;=</span> </span>(that: <span class="type">A</span>): <span class="type">Boolean</span> = (<span class="keyword">this</span> compare that) &lt;= <span class="number">0</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">&gt;=</span> </span>(that: <span class="type">A</span>): <span class="type">Boolean</span> = (<span class="keyword">this</span> compare that) &gt;= <span class="number">0</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">compareTo</span></span>(that: <span class="type">A</span>): <span class="type">Int</span> = compare(that)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>之所以在日常的编程中之所以你能够执行 <code>3&gt;2</code> 这样的判断操作，是因为程序执行了定义在 <code>Predef</code> 中的隐式转换方法 <code>intWrapper(x: Int)</code>，将 Int 类型转换为 RichInt 类型，而 RichInt 间接混入了 Ordered 特质，所以能够进行比较。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Predef.scala</span></span><br><span class="line"><span class="meta">@inline</span> <span class="keyword">implicit</span> <span class="function"><span class="keyword">def</span> <span class="title">intWrapper</span></span>(x: <span class="type">Int</span>)   = <span class="keyword">new</span> runtime.<span class="type">RichInt</span>(x)</span><br></pre></td></tr></table></figure>
<div align="center"> <img src="../pictures/scala-richInt.png"> </div>

<p>要想解决传入数值无法进行比较的问题，可以使用视图界定。语法为 <code>T &lt;% U</code>，代表 T 能够通过隐式转换转为 U，即允许 Int 型参数在无法进行比较的时候转换为 RichInt 类型。示例如下：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 视图界定符号 &lt;%</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Pair</span>[<span class="type">T</span> &lt;% <span class="type">Comparable</span>[<span class="type">T</span>]](<span class="params">val first: <span class="type">T</span>, val second: <span class="type">T</span></span>) </span>&#123;</span><br><span class="line">  <span class="comment">// 返回较小的值</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">smaller</span></span>: <span class="type">T</span> = <span class="keyword">if</span> (first.compareTo(second) &lt; <span class="number">0</span>) first <span class="keyword">else</span> second</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注：由于直接继承 Java 中 Comparable 接口的是特质 Ordered，所以如下的视图界定和上面是等效的：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">&gt; <span class="comment">// 隐式转换为 Ordered[T]</span></span><br><span class="line">&gt;    <span class="class"><span class="keyword">class</span> <span class="title">Pair</span>[<span class="type">T</span> &lt;% <span class="type">Ordered</span>[<span class="type">T</span>]](<span class="params">val first: <span class="type">T</span>, val second: <span class="type">T</span></span>) </span>&#123;</span><br><span class="line">&gt; <span class="function"><span class="keyword">def</span> <span class="title">smaller</span></span>: <span class="type">T</span> = <span class="keyword">if</span> (first.compareTo(second) &lt; <span class="number">0</span>) first <span class="keyword">else</span> second</span><br><span class="line">&gt;    &#125;</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<h3 id="2-3-类型约束"><a href="#2-3-类型约束" class="headerlink" title="2.3 类型约束"></a>2.3 类型约束</h3><p>如果你用的 Scala 是 2.11+，会发现视图界定已被标识为废弃。官方推荐使用类型约束 (type constraint) 来实现同样的功能，其本质是使用隐式参数进行隐式转换，示例如下：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"> <span class="comment">// 1.使用隐式参数隐式转换为 Comparable[T]</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Pair</span>[<span class="type">T</span>](<span class="params">val first: <span class="type">T</span>, val second: <span class="type">T</span></span>)(<span class="params">implicit ev: <span class="type">T</span> =&gt; <span class="type">Comparable</span>[<span class="type">T</span>]</span>) </span></span><br><span class="line"><span class="class">  <span class="title">def</span> <span class="title">smaller</span></span>: <span class="type">T</span> = <span class="keyword">if</span> (first.compareTo(second) &lt; <span class="number">0</span>) first <span class="keyword">else</span> second</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 2.由于直接继承 Java 中 Comparable 接口的是特质 Ordered，所以也可以隐式转换为 Ordered[T]</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Pair</span>[<span class="type">T</span>](<span class="params">val first: <span class="type">T</span>, val second: <span class="type">T</span></span>)(<span class="params">implicit ev: <span class="type">T</span> =&gt; <span class="type">Ordered</span>[<span class="type">T</span>]</span>) </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">smaller</span></span>: <span class="type">T</span> = <span class="keyword">if</span> (first.compareTo(second) &lt; <span class="number">0</span>) first <span class="keyword">else</span> second</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>当然，隐式参数转换也可以运用在具体的方法上：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">PairUtils</span></span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">smaller</span></span>[<span class="type">T</span>](a: <span class="type">T</span>, b: <span class="type">T</span>)(<span class="keyword">implicit</span> order: <span class="type">T</span> =&gt; <span class="type">Ordered</span>[<span class="type">T</span>]) = <span class="keyword">if</span> (a &lt; b) a <span class="keyword">else</span> b</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="2-4-上下文界定"><a href="#2-4-上下文界定" class="headerlink" title="2.4 上下文界定"></a>2.4 上下文界定</h3><p>上下文界定的形式为 <code>T:M</code>，其中 M 是一个泛型，它要求必须存在一个类型为 M[T]的隐式值，当你声明一个带隐式参数的方法时，需要定义一个隐式默认值。所以上面的程序也可以使用上下文界定进行改写：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Pair</span>[<span class="type">T</span>](<span class="params">val first: <span class="type">T</span>, val second: <span class="type">T</span></span>) </span>&#123;</span><br><span class="line">  <span class="comment">// 请注意 这个地方用的是 Ordering[T]，而上面视图界定和类型约束，用的是 Ordered[T]，两者的区别会在后文给出解释</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">smaller</span></span>(<span class="keyword">implicit</span> ord: <span class="type">Ordering</span>[<span class="type">T</span>]): <span class="type">T</span> = <span class="keyword">if</span> (ord.compare(first, second) &lt; <span class="number">0</span>) first <span class="keyword">else</span> second </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 测试</span></span><br><span class="line"><span class="keyword">val</span> pair= <span class="keyword">new</span> <span class="type">Pair</span>(<span class="number">88</span>, <span class="number">66</span>)</span><br><span class="line">println(pair.smaller)  <span class="comment">//输出：66</span></span><br></pre></td></tr></table></figure>
<p>在上面的示例中，我们无需手动添加隐式默认值就可以完成转换，这是因为 Scala 自动引入了 Ordering[Int]这个隐式值。为了更好的说明上下文界定，下面给出一个自定义类型的比较示例：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 1.定义一个人员类</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">val name: <span class="type">String</span>, val age: <span class="type">Int</span></span>) </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">toString</span></span>: <span class="type">String</span> = name + <span class="string">":"</span> + age</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 2.继承 Ordering[T],实现自定义比较器,按照自己的规则重写比较方法</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PersonOrdering</span> <span class="keyword">extends</span> <span class="title">Ordering</span>[<span class="type">Person</span>] </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">compare</span></span>(x: <span class="type">Person</span>, y: <span class="type">Person</span>): <span class="type">Int</span> = <span class="keyword">if</span> (x.age &gt; y.age) <span class="number">1</span> <span class="keyword">else</span> <span class="number">-1</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Pair</span>[<span class="type">T</span>](<span class="params">val first: <span class="type">T</span>, val second: <span class="type">T</span></span>) </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">smaller</span></span>(<span class="keyword">implicit</span> ord: <span class="type">Ordering</span>[<span class="type">T</span>]): <span class="type">T</span> = <span class="keyword">if</span> (ord.compare(first, second) &lt; <span class="number">0</span>) first <span class="keyword">else</span> second</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaApp</span> <span class="keyword">extends</span> <span class="title">App</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> pair = <span class="keyword">new</span> <span class="type">Pair</span>(<span class="keyword">new</span> <span class="type">Person</span>(<span class="string">"hei"</span>, <span class="number">88</span>), <span class="keyword">new</span> <span class="type">Person</span>(<span class="string">"bai"</span>, <span class="number">66</span>))</span><br><span class="line">  <span class="comment">// 3.定义隐式默认值,如果不定义,则下一行代码无法通过编译</span></span><br><span class="line">  <span class="keyword">implicit</span> <span class="keyword">val</span> <span class="type">ImpPersonOrdering</span> = <span class="keyword">new</span> <span class="type">PersonOrdering</span></span><br><span class="line">  println(pair.smaller) <span class="comment">//输出： bai:66</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="2-5-ClassTag上下文界定"><a href="#2-5-ClassTag上下文界定" class="headerlink" title="2.5 ClassTag上下文界定"></a>2.5 ClassTag上下文界定</h3><p>这里先看一个例子：下面这段代码，没有任何语法错误，但是在运行时会抛出异常：<code>Error: cannot find class tag for element type T</code>, 这是由于 Scala 和 Java 一样，都存在类型擦除，即<strong>泛型信息只存在于代码编译阶段，在进入 JVM 之前，与泛型相关的信息会被擦除掉</strong>。对于下面的代码，在运行阶段创建 Array 时，你必须明确指明其类型，但是此时泛型信息已经被擦除，导致出现找不到类型的异常。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaApp</span> <span class="keyword">extends</span> <span class="title">App</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">makePair</span></span>[<span class="type">T</span>](first: <span class="type">T</span>, second: <span class="type">T</span>) = &#123;</span><br><span class="line">    <span class="comment">// 创建以一个数组 并赋值</span></span><br><span class="line">    <span class="keyword">val</span> r = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">T</span>](<span class="number">2</span>); r(<span class="number">0</span>) = first; r(<span class="number">1</span>) = second; r</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Scala 针对这个问题，提供了 ClassTag 上下文界定，即把泛型的信息存储在 ClassTag 中，这样在运行阶段需要时，只需要从 ClassTag 中进行获取即可。其语法为 <code>T : ClassTag</code>，示例如下：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> scala.reflect._</span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaApp</span> <span class="keyword">extends</span> <span class="title">App</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">makePair</span></span>[<span class="type">T</span> : <span class="type">ClassTag</span>](first: <span class="type">T</span>, second: <span class="type">T</span>) = &#123;</span><br><span class="line">    <span class="keyword">val</span> r = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">T</span>](<span class="number">2</span>); r(<span class="number">0</span>) = first; r(<span class="number">1</span>) = second; r</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="2-6-类型下界限定"><a href="#2-6-类型下界限定" class="headerlink" title="2.6 类型下界限定"></a>2.6 类型下界限定</h3><p>2.1 小节介绍了类型上界的限定，Scala 同时也支持下界的限定，语法为：<code>U &gt;: T</code>，即 U 必须是类型 T 的超类或本身。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 首席执行官</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CEO</span></span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">//</span> <span class="title">部门经理</span></span></span><br><span class="line"><span class="class"><span class="title">class</span> <span class="title">Manager</span> <span class="keyword">extends</span> <span class="title">CEO</span></span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">//</span> <span class="title">本公司普通员工</span></span></span><br><span class="line"><span class="class"><span class="title">class</span> <span class="title">Employee</span> <span class="keyword">extends</span> <span class="title">Manager</span></span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">//</span> <span class="title">其他公司人员</span></span></span><br><span class="line"><span class="class"><span class="title">class</span> <span class="title">OtherCompany</span></span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">object</span> <span class="title">ScalaApp</span> <span class="keyword">extends</span> <span class="title">App</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 限定：只有本公司部门经理以上人员才能获取权限</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">Check</span></span>[<span class="type">T</span> &gt;: <span class="type">Manager</span>](t: <span class="type">T</span>): <span class="type">T</span> = &#123;</span><br><span class="line">    println(<span class="string">"获得审核权限"</span>)</span><br><span class="line">    t</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 错误写法: 省略泛型参数后,以下所有人都能获得权限,显然这是不正确的</span></span><br><span class="line">  <span class="type">Check</span>(<span class="keyword">new</span> <span class="type">CEO</span>)</span><br><span class="line">  <span class="type">Check</span>(<span class="keyword">new</span> <span class="type">Manager</span>)</span><br><span class="line">  <span class="type">Check</span>(<span class="keyword">new</span> <span class="type">Employee</span>)</span><br><span class="line">  <span class="type">Check</span>(<span class="keyword">new</span> <span class="type">OtherCompany</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="comment">// 正确写法,传入泛型参数</span></span><br><span class="line">  <span class="type">Check</span>[<span class="type">CEO</span>](<span class="keyword">new</span> <span class="type">CEO</span>)</span><br><span class="line">  <span class="type">Check</span>[<span class="type">Manager</span>](<span class="keyword">new</span> <span class="type">Manager</span>)</span><br><span class="line">  <span class="comment">/*</span></span><br><span class="line"><span class="comment">   * 以下两条语句无法通过编译,异常信息为: </span></span><br><span class="line"><span class="comment">   * do not conform to method Check's type parameter bounds(不符合方法 Check 的类型参数边界)</span></span><br><span class="line"><span class="comment">   * 这种情况就完成了下界限制，即只有本公司经理及以上的人员才能获得审核权限</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="type">Check</span>[<span class="type">Employee</span>](<span class="keyword">new</span> <span class="type">Employee</span>)</span><br><span class="line">  <span class="type">Check</span>[<span class="type">OtherCompany</span>](<span class="keyword">new</span> <span class="type">OtherCompany</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="2-7-多重界定"><a href="#2-7-多重界定" class="headerlink" title="2.7 多重界定"></a>2.7 多重界定</h3><ul>
<li><p>类型变量可以同时有上界和下界。 写法为 ：<code>T &gt; : Lower &lt;: Upper</code>；</p>
</li>
<li><p>不能同时有多个上界或多个下界 。但可以要求一个类型实现多个特质，写法为 :</p>
<p><code>T &lt; : Comparable[T] with Serializable with Cloneable</code>；</p>
</li>
<li><p>你可以有多个上下文界定，写法为 <code>T : Ordering : ClassTag</code> 。</p>
</li>
</ul>
<h2 id="三、Ordering-amp-Ordered"><a href="#三、Ordering-amp-Ordered" class="headerlink" title="三、Ordering &amp; Ordered"></a>三、Ordering &amp; Ordered</h2><p>上文中使用到 Ordering 和 Ordered 特质，它们最主要的区别在于分别继承自不同的 Java 接口：Comparable 和 Comparator：</p>
<ul>
<li><strong>Comparable</strong>：可以理解为内置的比较器，实现此接口的对象可以与自身进行比较；</li>
<li><strong>Comparator</strong>：可以理解为外置的比较器；当对象自身并没有定义比较规则的时候，可以传入外部比较器进行比较。</li>
</ul>
<p>为什么 Java 中要同时给出这两个比较接口，这是因为你要比较的对象不一定实现了 Comparable 接口，而你又想对其进行比较，这时候当然你可以修改代码实现 Comparable，但是如果这个类你无法修改 (如源码中的类)，这时候就可以使用外置的比较器。同样的问题在 Scala 中当然也会出现，所以 Scala 分别使用了 Ordering 和 Ordered 来继承它们。</p>
<div align="center"> <img src="../pictures/scala-ordered-ordering.png"> </div>



<p>下面分别给出 Java 中 Comparable 和 Comparator 接口的使用示例：</p>
<h3 id="3-1-Comparable"><a href="#3-1-Comparable" class="headerlink" title="3.1 Comparable"></a>3.1 Comparable</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="comment">// 实现 Comparable 接口</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span> <span class="keyword">implements</span> <span class="title">Comparable</span>&lt;<span class="title">Person</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> String name;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> age;</span><br><span class="line"></span><br><span class="line">    Person(String name,<span class="keyword">int</span> age) &#123;<span class="keyword">this</span>.name=name;<span class="keyword">this</span>.age=age;&#125;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123; <span class="keyword">return</span> name+<span class="string">":"</span>+age; &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 核心的方法是重写比较规则，按照年龄进行排序</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compareTo</span><span class="params">(Person person)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">this</span>.age - person.age;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Person[] peoples= &#123;<span class="keyword">new</span> Person(<span class="string">"hei"</span>, <span class="number">66</span>), <span class="keyword">new</span> Person(<span class="string">"bai"</span>, <span class="number">55</span>), <span class="keyword">new</span> Person(<span class="string">"ying"</span>, <span class="number">77</span>)&#125;;</span><br><span class="line">        Arrays.sort(peoples);</span><br><span class="line">        Arrays.stream(peoples).forEach(System.out::println);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">bai:<span class="number">55</span></span><br><span class="line">hei:<span class="number">66</span></span><br><span class="line">ying:<span class="number">77</span></span><br></pre></td></tr></table></figure>
<h3 id="3-2-Comparator"><a href="#3-2-Comparator" class="headerlink" title="3.2 Comparator"></a>3.2 Comparator</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="keyword">import</span> java.util.Comparator;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> String name;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> age;</span><br><span class="line"></span><br><span class="line">    Person(String name,<span class="keyword">int</span> age) &#123;<span class="keyword">this</span>.name=name;<span class="keyword">this</span>.age=age;&#125;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123; <span class="keyword">return</span> name+<span class="string">":"</span>+age; &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Person[] peoples= &#123;<span class="keyword">new</span> Person(<span class="string">"hei"</span>, <span class="number">66</span>), <span class="keyword">new</span> Person(<span class="string">"bai"</span>, <span class="number">55</span>), <span class="keyword">new</span> Person(<span class="string">"ying"</span>, <span class="number">77</span>)&#125;;</span><br><span class="line">        <span class="comment">// 这里为了直观直接使用匿名内部类,实现 Comparator 接口</span></span><br><span class="line">        <span class="comment">//如果是 Java8 你也可以写成 Arrays.sort(peoples, Comparator.comparingInt(o -&gt; o.age));</span></span><br><span class="line">        Arrays.sort(peoples, <span class="keyword">new</span> Comparator&lt;Person&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compare</span><span class="params">(Person o1, Person o2)</span> </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> o1.age-o2.age;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        Arrays.stream(peoples).forEach(System.out::println);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>使用外置比较器还有一个好处，就是你可以随时定义其排序规则：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 按照年龄大小排序</span></span><br><span class="line"><span class="type">Arrays</span>.sort(peoples, <span class="type">Comparator</span>.comparingInt(o -&gt; o.age));</span><br><span class="line"><span class="type">Arrays</span>.stream(peoples).forEach(<span class="type">System</span>.out::println);</span><br><span class="line"><span class="comment">// 按照名字长度倒序排列</span></span><br><span class="line"><span class="type">Arrays</span>.sort(peoples, <span class="type">Comparator</span>.comparingInt(o -&gt; -o.name.length()));</span><br><span class="line"><span class="type">Arrays</span>.stream(peoples).forEach(<span class="type">System</span>.out::println);</span><br></pre></td></tr></table></figure>
<h3 id="3-3-上下文界定的优点"><a href="#3-3-上下文界定的优点" class="headerlink" title="3.3 上下文界定的优点"></a>3.3 上下文界定的优点</h3><p>这里再次给出上下文界定中的示例代码作为回顾：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 1.定义一个人员类</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">val name: <span class="type">String</span>, val age: <span class="type">Int</span></span>) </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">toString</span></span>: <span class="type">String</span> = name + <span class="string">":"</span> + age</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 2.继承 Ordering[T],实现自定义比较器,这个比较器就是一个外置比较器</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PersonOrdering</span> <span class="keyword">extends</span> <span class="title">Ordering</span>[<span class="type">Person</span>] </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">compare</span></span>(x: <span class="type">Person</span>, y: <span class="type">Person</span>): <span class="type">Int</span> = <span class="keyword">if</span> (x.age &gt; y.age) <span class="number">1</span> <span class="keyword">else</span> <span class="number">-1</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Pair</span>[<span class="type">T</span>](<span class="params">val first: <span class="type">T</span>, val second: <span class="type">T</span></span>) </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">smaller</span></span>(<span class="keyword">implicit</span> ord: <span class="type">Ordering</span>[<span class="type">T</span>]): <span class="type">T</span> = <span class="keyword">if</span> (ord.compare(first, second) &lt; <span class="number">0</span>) first <span class="keyword">else</span> second</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaApp</span> <span class="keyword">extends</span> <span class="title">App</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> pair = <span class="keyword">new</span> <span class="type">Pair</span>(<span class="keyword">new</span> <span class="type">Person</span>(<span class="string">"hei"</span>, <span class="number">88</span>), <span class="keyword">new</span> <span class="type">Person</span>(<span class="string">"bai"</span>, <span class="number">66</span>))</span><br><span class="line">  <span class="comment">// 3.在当前上下文定义隐式默认值,这就相当于传入了外置比较器</span></span><br><span class="line">  <span class="keyword">implicit</span> <span class="keyword">val</span> <span class="type">ImpPersonOrdering</span> = <span class="keyword">new</span> <span class="type">PersonOrdering</span></span><br><span class="line">  println(pair.smaller) <span class="comment">//输出： bai:66</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>使用上下文界定和 Ordering 带来的好处是：传入 <code>Pair</code> 中的参数不一定需要可比较，只要在比较时传入外置比较器即可。</p>
<p>需要注意的是由于隐式默认值二义性的限制，你不能像上面 Java 代码一样，在同一个上下文作用域中传入两个外置比较器，即下面的代码是无法通过编译的。但是你可以在不同的上下文作用域中引入不同的隐式默认值，即使用不同的外置比较器。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">implicit</span> <span class="keyword">val</span> <span class="type">ImpPersonOrdering</span> = <span class="keyword">new</span> <span class="type">PersonOrdering</span></span><br><span class="line">println(pair.smaller) </span><br><span class="line"><span class="keyword">implicit</span> <span class="keyword">val</span> <span class="type">ImpPersonOrdering2</span> = <span class="keyword">new</span> <span class="type">PersonOrdering</span></span><br><span class="line">println(pair.smaller)</span><br></pre></td></tr></table></figure>
<h2 id="四、通配符"><a href="#四、通配符" class="headerlink" title="四、通配符"></a>四、通配符</h2><p>在实际编码中，通常需要把泛型限定在某个范围内，比如限定为某个类及其子类。因此 Scala 和 Java 一样引入了通配符这个概念，用于限定泛型的范围。不同的是 Java 使用 <code>?</code> 表示通配符，Scala 使用 <code>_</code> 表示通配符。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Ceo</span>(<span class="params">val name: <span class="type">String</span></span>) </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">toString</span></span>: <span class="type">String</span> = name</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Manager</span>(<span class="params">name: <span class="type">String</span></span>) <span class="keyword">extends</span> <span class="title">Ceo</span>(<span class="params">name</span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">class</span> <span class="title">Employee</span>(<span class="params">name: <span class="type">String</span></span>) <span class="keyword">extends</span> <span class="title">Manager</span>(<span class="params">name</span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">class</span> <span class="title">Pair</span>[<span class="type">T</span>](<span class="params">val first: <span class="type">T</span>, val second: <span class="type">T</span></span>) </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">toString</span></span>: <span class="type">String</span> = <span class="string">"first:"</span> + first + <span class="string">", second: "</span> + second</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaApp</span> <span class="keyword">extends</span> <span class="title">App</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 限定部门经理及以下的人才可以组队</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">makePair</span></span>(p: <span class="type">Pair</span>[_ &lt;: <span class="type">Manager</span>]): <span class="type">Unit</span> = &#123;println(p)&#125;</span><br><span class="line">  makePair(<span class="keyword">new</span> <span class="type">Pair</span>(<span class="keyword">new</span> <span class="type">Employee</span>(<span class="string">"heibai"</span>), <span class="keyword">new</span> <span class="type">Manager</span>(<span class="string">"ying"</span>)))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>目前 Scala 中的通配符在某些复杂情况下还不完善，如下面的语句在 Scala 2.12 中并不能通过编译：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">min</span></span>[<span class="type">T</span> &lt;: <span class="type">Comparable</span>[_ &gt;: <span class="type">T</span>]](p: <span class="type">Pair</span>[<span class="type">T</span>]) =&#123;&#125;</span><br></pre></td></tr></table></figure>
<p>可以使用以下语法代替：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">type</span> <span class="title">SuperComparable</span>[<span class="type">T</span>] </span>= <span class="type">Comparable</span>[_ &gt;: <span class="type">T</span>]</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">min</span></span>[<span class="type">T</span> &lt;: <span class="type">SuperComparable</span>[<span class="type">T</span>]](p: <span class="type">Pair</span>[<span class="type">T</span>]) = &#123;&#125;</span><br></pre></td></tr></table></figure>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol>
<li>Martin Odersky . Scala 编程 (第 3 版)[M] . 电子工业出版社 . 2018-1-1  </li>
<li>凯.S.霍斯特曼  . 快学 Scala(第 2 版)[M] . 电子工业出版社 . 2017-7</li>
</ol>
]]></content>
      <categories>
        <category>notes</category>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>Scala</tag>
      </tags>
  </entry>
  <entry>
    <title>流程控制语句</title>
    <url>/2021/03/18/Scala%E6%B5%81%E7%A8%8B%E6%8E%A7%E5%88%B6%E8%AF%AD%E5%8F%A5/</url>
    <content><![CDATA[<h2 id="一、条件表达式if"><a href="#一、条件表达式if" class="headerlink" title="一、条件表达式if"></a>一、条件表达式if</h2><p>Scala 中的 if/else 语法结构与 Java 中的一样，唯一不同的是，Scala 中的 if 表达式是有返回值的。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaApp</span> <span class="keyword">extends</span> <span class="title">App</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> x = <span class="string">"scala"</span></span><br><span class="line">  <span class="keyword">val</span> result = <span class="keyword">if</span> (x.length == <span class="number">5</span>) <span class="string">"true"</span> <span class="keyword">else</span> <span class="string">"false"</span></span><br><span class="line">  print(result)</span><br><span class="line">  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在 Java 中，每行语句都需要使用 <code>;</code> 表示结束，但是在 Scala 中并不需要。除非你在单行语句中写了多行代码。</p>
<h2 id="二、块表达式"><a href="#二、块表达式" class="headerlink" title="二、块表达式"></a>二、块表达式</h2><p>在 Scala 中，可以使用 <code>{}</code> 块包含一系列表达式，块中最后一个表达式的值就是块的值。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaApp</span> <span class="keyword">extends</span> <span class="title">App</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> result = &#123;</span><br><span class="line">    <span class="keyword">val</span> a = <span class="number">1</span> + <span class="number">1</span>; <span class="keyword">val</span> b = <span class="number">2</span> + <span class="number">2</span>; a + b</span><br><span class="line">  &#125;</span><br><span class="line">  print(result)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出： 6</span></span><br></pre></td></tr></table></figure>
<p>如果块中的最后一个表达式没有返回值，则块的返回值是 Unit 类型。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> result =&#123; <span class="keyword">val</span> a = <span class="number">1</span> + <span class="number">1</span>; <span class="keyword">val</span> b = <span class="number">2</span> + <span class="number">2</span> &#125;</span><br><span class="line">result: <span class="type">Unit</span> = ()</span><br></pre></td></tr></table></figure>
<h2 id="三、循环表达式while"><a href="#三、循环表达式while" class="headerlink" title="三、循环表达式while"></a>三、循环表达式while</h2><p>Scala 和大多数语言一样，支持 <code>while</code> 和 <code>do ... while</code> 表达式。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaApp</span> <span class="keyword">extends</span> <span class="title">App</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">var</span> n = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">while</span> (n &lt; <span class="number">10</span>) &#123;</span><br><span class="line">    n += <span class="number">1</span></span><br><span class="line">    println(n)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 循环至少要执行一次</span></span><br><span class="line">  do &#123;</span><br><span class="line">    println(n)</span><br><span class="line">  &#125; <span class="keyword">while</span> (n &gt; <span class="number">10</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="四、循环表达式for"><a href="#四、循环表达式for" class="headerlink" title="四、循环表达式for"></a>四、循环表达式for</h2><p>for 循环的基本使用如下：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaApp</span> <span class="keyword">extends</span> <span class="title">App</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 1.基本使用  输出[1,9)</span></span><br><span class="line">  <span class="keyword">for</span> (n &lt;- <span class="number">1</span> until <span class="number">10</span>) &#123;print(n)&#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 2.使用多个表达式生成器  输出: 11 12 13 21 22 23 31 32 33</span></span><br><span class="line">  <span class="keyword">for</span> (i &lt;- <span class="number">1</span> to <span class="number">3</span>; j &lt;- <span class="number">1</span> to <span class="number">3</span>) print(<span class="string">f"<span class="subst">$&#123;10 * i + j&#125;</span>%3d"</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 3.使用带条件的表达式生成器  输出: 12 13 21 23 31 32</span></span><br><span class="line">  <span class="keyword">for</span> (i &lt;- <span class="number">1</span> to <span class="number">3</span>; j &lt;- <span class="number">1</span> to <span class="number">3</span> <span class="keyword">if</span> i != j) print(<span class="string">f"<span class="subst">$&#123;10 * i + j&#125;</span>%3d"</span>)</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>除了基本使用外，还可以使用 <code>yield</code> 关键字从 for 循环中产生 Vector，这称为 for 推导式。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="keyword">for</span> (i &lt;- <span class="number">1</span> to <span class="number">10</span>) <span class="keyword">yield</span> i * <span class="number">6</span></span><br><span class="line">res1: scala.collection.immutable.<span class="type">IndexedSeq</span>[<span class="type">Int</span>] = <span class="type">Vector</span>(<span class="number">6</span>, <span class="number">12</span>, <span class="number">18</span>, <span class="number">24</span>, <span class="number">30</span>, <span class="number">36</span>, <span class="number">42</span>, <span class="number">48</span>, <span class="number">54</span>, <span class="number">60</span>)</span><br></pre></td></tr></table></figure>
<h2 id="五、异常处理try"><a href="#五、异常处理try" class="headerlink" title="五、异常处理try"></a>五、异常处理try</h2><p>和 Java 中一样，支持 <code>try...catch...finally</code> 语句。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.&#123;<span class="type">FileNotFoundException</span>, <span class="type">FileReader</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaApp</span> <span class="keyword">extends</span> <span class="title">App</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="keyword">val</span> reader = <span class="keyword">new</span> <span class="type">FileReader</span>(<span class="string">"wordCount.txt"</span>)</span><br><span class="line">  &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> ex: <span class="type">FileNotFoundException</span> =&gt;</span><br><span class="line">      ex.printStackTrace()</span><br><span class="line">      println(<span class="string">"没有找到对应的文件!"</span>)</span><br><span class="line">  &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">    println(<span class="string">"finally 语句一定会被执行！"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里需要注意的是因为 finally 语句一定会被执行，所以不要在该语句中返回值，否则返回值会被作为整个 try 语句的返回值，如下：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="function"><span class="keyword">def</span> <span class="title">g</span></span>():<span class="type">Int</span> = <span class="keyword">try</span> <span class="keyword">return</span> <span class="number">1</span> <span class="keyword">finally</span>  <span class="keyword">return</span>  <span class="number">2</span></span><br><span class="line">g: ()<span class="type">Int</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 方法 g() 总会返回 2</span></span><br><span class="line">scala&gt; g()</span><br><span class="line">res3: <span class="type">Int</span> = <span class="number">2</span></span><br></pre></td></tr></table></figure>
<h2 id="六、条件选择表达式match"><a href="#六、条件选择表达式match" class="headerlink" title="六、条件选择表达式match"></a>六、条件选择表达式match</h2><p>match 类似于 java 中的 switch 语句。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaApp</span> <span class="keyword">extends</span> <span class="title">App</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> elements = <span class="type">Array</span>(<span class="string">"A"</span>, <span class="string">"B"</span>, <span class="string">"C"</span>, <span class="string">"D"</span>, <span class="string">"E"</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> (elem &lt;- elements) &#123;</span><br><span class="line">    elem <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="string">"A"</span> =&gt; println(<span class="number">10</span>)</span><br><span class="line">      <span class="keyword">case</span> <span class="string">"B"</span> =&gt; println(<span class="number">20</span>)</span><br><span class="line">      <span class="keyword">case</span> <span class="string">"C"</span> =&gt; println(<span class="number">30</span>)</span><br><span class="line">      <span class="keyword">case</span> _ =&gt; println(<span class="number">50</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>但是与 Java 中的 switch 有以下三点不同：</p>
<ul>
<li>Scala 中的 case 语句支持任何类型；而 Java 中 case 语句仅支持整型、枚举和字符串常量；</li>
<li>Scala 中每个分支语句后面不需要写 break，因为在 case 语句中 break 是隐含的，默认就有；</li>
<li>在 Scala 中 match 语句是有返回值的，而 Java 中 switch 语句是没有返回值的。如下：</li>
</ul>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaApp</span> <span class="keyword">extends</span> <span class="title">App</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> elements = <span class="type">Array</span>(<span class="string">"A"</span>, <span class="string">"B"</span>, <span class="string">"C"</span>, <span class="string">"D"</span>, <span class="string">"E"</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> (elem &lt;- elements) &#123;</span><br><span class="line">    <span class="keyword">val</span> score = elem <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="string">"A"</span> =&gt; <span class="number">10</span></span><br><span class="line">      <span class="keyword">case</span> <span class="string">"B"</span> =&gt; <span class="number">20</span></span><br><span class="line">      <span class="keyword">case</span> <span class="string">"C"</span> =&gt; <span class="number">30</span></span><br><span class="line">      <span class="keyword">case</span> _ =&gt; <span class="number">50</span></span><br><span class="line">    &#125;</span><br><span class="line">    print(elem + <span class="string">":"</span> + score + <span class="string">";"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 输出： A:10;B:20;C:30;D:50;E:50;</span></span><br></pre></td></tr></table></figure>
<h2 id="七、没有break和continue"><a href="#七、没有break和continue" class="headerlink" title="七、没有break和continue"></a>七、没有break和continue</h2><p>额外注意一下：Scala 中并不支持 Java 中的 break 和 continue 关键字。</p>
<h2 id="八、输入与输出"><a href="#八、输入与输出" class="headerlink" title="八、输入与输出"></a>八、输入与输出</h2><p>在 Scala 中可以使用 print、println、printf 打印输出，这与 Java 中是一样的。如果需要从控制台中获取输入，则可以使用 <code>StdIn</code> 中定义的各种方法。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> name = <span class="type">StdIn</span>.readLine(<span class="string">"Your name: "</span>)</span><br><span class="line">print(<span class="string">"Your age: "</span>)</span><br><span class="line"><span class="keyword">val</span> age = <span class="type">StdIn</span>.readInt()</span><br><span class="line">println(<span class="string">s"Hello, <span class="subst">$&#123;name&#125;</span>! Next year, you will be <span class="subst">$&#123;age + 1&#125;</span>."</span>)</span><br></pre></td></tr></table></figure>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol>
<li>Martin Odersky . Scala 编程 (第 3 版)[M] . 电子工业出版社 . 2018-1-1  </li>
<li>凯.S.霍斯特曼  . 快学 Scala(第 2 版)[M] . 电子工业出版社 . 2017-7</li>
</ol>
]]></content>
      <categories>
        <category>notes</category>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>Scala</tag>
      </tags>
  </entry>
  <entry>
    <title>类和对象</title>
    <url>/2021/03/18/Scala%E7%B1%BB%E5%92%8C%E5%AF%B9%E8%B1%A1/</url>
    <content><![CDATA[<h2 id="一、初识类和对象"><a href="#一、初识类和对象" class="headerlink" title="一、初识类和对象"></a>一、初识类和对象</h2><p>Scala 的类与 Java 的类具有非常多的相似性，示例如下：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 1. 在 scala 中，类不需要用 public 声明,所有的类都具有公共的可见性</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 2. 声明私有变量,用 var 修饰的变量默认拥有 getter/setter 属性</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> age = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// 3.如果声明的变量不需要进行初始赋值，此时 Scala 就无法进行类型推断，所以需要显式指明类型</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> name: <span class="type">String</span> = _</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="comment">// 4. 定义方法,应指明传参类型。返回值类型不是必须的，Scala 可以自动推断出来，但是为了方便调用者，建议指明</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">growUp</span></span>(step: <span class="type">Int</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    age += step</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 5.对于改值器方法 (即改变对象状态的方法),即使不需要传入参数,也建议在声明中包含 ()</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">growUpFix</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    age += <span class="number">10</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 6.对于取值器方法 (即不会改变对象状态的方法),不必在声明中包含 ()</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">currentAge</span></span>: <span class="type">Int</span> = &#123;</span><br><span class="line">    age</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 7.不建议使用 return 关键字,默认方法中最后一行代码的计算结果为返回值</span></span><br><span class="line"><span class="comment">   *   如果方法很简短，甚至可以写在同一行中</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getName</span></span>: <span class="type">String</span> = name</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 伴生对象</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// 8.创建类的实例</span></span><br><span class="line">    <span class="keyword">val</span> counter = <span class="keyword">new</span> <span class="type">Person</span>()</span><br><span class="line">    <span class="comment">// 9.用 var 修饰的变量默认拥有 getter/setter 属性，可以直接对其进行赋值</span></span><br><span class="line">    counter.age = <span class="number">12</span></span><br><span class="line">    counter.growUp(<span class="number">8</span>)</span><br><span class="line">    counter.growUpFix()</span><br><span class="line">    <span class="comment">// 10.用 var 修饰的变量默认拥有 getter/setter 属性，可以直接对其进行取值，输出: 30</span></span><br><span class="line">    println(counter.age)</span><br><span class="line">    <span class="comment">// 输出: 30</span></span><br><span class="line">    println(counter.currentAge)</span><br><span class="line">    <span class="comment">// 输出: null</span></span><br><span class="line">    println(counter.getName)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><br></p>
<h2 id="二、类"><a href="#二、类" class="headerlink" title="二、类"></a>二、类</h2><h3 id="2-1-成员变量可见性"><a href="#2-1-成员变量可见性" class="headerlink" title="2.1 成员变量可见性"></a>2.1 成员变量可见性</h3><p>Scala 中成员变量的可见性默认都是 public，如果想要保证其不被外部干扰，可以声明为 private，并通过 getter 和 setter 方法进行访问。</p>
<h3 id="2-2-getter和setter属性"><a href="#2-2-getter和setter属性" class="headerlink" title="2.2 getter和setter属性"></a>2.2 getter和setter属性</h3><p>getter 和 setter 属性与声明变量时使用的关键字有关：</p>
<ul>
<li>使用 var 关键字：变量同时拥有 getter 和 setter 属性；</li>
<li>使用 val 关键字：变量只拥有 getter 属性；</li>
<li>使用 private[this]：变量既没有 getter 属性、也没有 setter 属性，只能通过内部的方法访问；</li>
</ul>
<p>需要特别说明的是：假设变量名为 age,则其对应的 get 和 set 的方法名分别叫做 <code>age</code> 和 <code>age_=</code>。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> name = <span class="string">"heibaiying"</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> age = <span class="number">12</span></span><br><span class="line">  <span class="keyword">private</span>[<span class="keyword">this</span>] <span class="keyword">var</span> birthday = <span class="string">"2019-08-08"</span></span><br><span class="line">  <span class="comment">// birthday 只能被内部方法所访问</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getBirthday</span></span>: <span class="type">String</span> = birthday</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> person = <span class="keyword">new</span> <span class="type">Person</span></span><br><span class="line">    person.age = <span class="number">30</span></span><br><span class="line">    println(person.name)</span><br><span class="line">    println(person.age)</span><br><span class="line">    println(person.getBirthday)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>解释说明：</p>
<p>示例代码中 <code>person.age=30</code> 在执行时内部实际是调用了方法 <code>person.age_=(30)</code>，而 <code>person.age</code> 内部执行时实际是调用了 <code>person.age()</code> 方法。想要证明这一点，可以对代码进行反编译。同时为了说明成员变量可见性的问题，我们对下面这段代码进行反编译：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">&gt; <span class="class"><span class="keyword">class</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line">&gt; <span class="keyword">var</span> name = <span class="string">""</span></span><br><span class="line">&gt; <span class="keyword">private</span> <span class="keyword">var</span> age = <span class="string">""</span></span><br><span class="line">&gt; &#125;</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<blockquote>
<p>依次执行下面编译命令：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash"> &gt; scalac Person.scala</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> &gt; javap -private Person</span></span><br><span class="line"><span class="meta">&gt;</span></span><br></pre></td></tr></table></figure>
</blockquote>
<blockquote>
<p>编译结果如下，从编译结果可以看到实际的 get 和 set 的方法名 (因为 JVM 不允许在方法名中出现＝，所以它被翻译成$eq)，同时也验证了成员变量默认的可见性为 public。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">&gt; Compiled from <span class="string">"Person.scala"</span></span><br><span class="line">&gt; <span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line">&gt; <span class="keyword">private</span> java.lang.String name;</span><br><span class="line">&gt; <span class="keyword">private</span> java.lang.String age;</span><br><span class="line">&gt;  </span><br><span class="line">&gt; <span class="keyword">public</span> java.lang.<span class="function">String <span class="title">name</span><span class="params">()</span></span>;</span><br><span class="line">&gt; <span class="keyword">public</span> <span class="keyword">void</span> name_$eq(java.lang.String);</span><br><span class="line">&gt;  </span><br><span class="line">&gt; <span class="keyword">private</span> java.lang.<span class="function">String <span class="title">age</span><span class="params">()</span></span>;</span><br><span class="line">&gt; <span class="keyword">private</span> <span class="keyword">void</span> age_$eq(java.lang.String);</span><br><span class="line">&gt;  </span><br><span class="line">&gt; <span class="function"><span class="keyword">public</span> <span class="title">Person</span><span class="params">()</span></span>;</span><br><span class="line">&gt; &#125;</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<h3 id="2-3-BeanProperty"><a href="#2-3-BeanProperty" class="headerlink" title="2.3 @BeanProperty"></a>2.3 @BeanProperty</h3><p>在上面的例子中可以看到我们是使用 <code>.</code> 来对成员变量进行访问的，如果想要额外生成和 Java 中一样的 getXXX 和 setXXX 方法，则需要使用@BeanProperty 进行注解。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line">  <span class="meta">@BeanProperty</span> <span class="keyword">var</span> name = <span class="string">""</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> person = <span class="keyword">new</span> <span class="type">Person</span></span><br><span class="line">    person.setName(<span class="string">"heibaiying"</span>)</span><br><span class="line">    println(person.getName)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="2-4-主构造器"><a href="#2-4-主构造器" class="headerlink" title="2.4 主构造器"></a>2.4 主构造器</h3><p>和 Java 不同的是，Scala 类的主构造器直接写在类名后面，但注意以下两点：</p>
<ul>
<li>主构造器传入的参数默认就是 val 类型的，即不可变，你没有办法在内部改变传参；</li>
<li>写在主构造器中的代码块会在类初始化的时候被执行，功能类似于 Java 的静态代码块 <code>static{}</code></li>
</ul>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">val name: <span class="type">String</span>, val age: <span class="type">Int</span></span>) </span>&#123;</span><br><span class="line"></span><br><span class="line">  println(<span class="string">"功能类似于 Java 的静态代码块 static&#123;&#125;"</span>)</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getDetail</span></span>: <span class="type">String</span> = &#123;</span><br><span class="line">    <span class="comment">//name="heibai" 无法通过编译</span></span><br><span class="line">    name + <span class="string">":"</span> + age</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> person = <span class="keyword">new</span> <span class="type">Person</span>(<span class="string">"heibaiying"</span>, <span class="number">20</span>)</span><br><span class="line">    println(person.getDetail)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">功能类似于 <span class="type">Java</span> 的静态代码块 static&#123;&#125;</span><br><span class="line">heibaiying:<span class="number">20</span></span><br></pre></td></tr></table></figure>
<h3 id="2-5-辅助构造器"><a href="#2-5-辅助构造器" class="headerlink" title="2.5 辅助构造器"></a>2.5 辅助构造器</h3><p>辅助构造器有两点硬性要求：</p>
<ul>
<li>辅助构造器的名称必须为 this；</li>
<li>每个辅助构造器必须以主构造器或其他的辅助构造器的调用开始。</li>
</ul>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">val name: <span class="type">String</span>, val age: <span class="type">Int</span></span>) </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> birthday = <span class="string">""</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// 1.辅助构造器的名称必须为 this</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">this</span></span>(name: <span class="type">String</span>, age: <span class="type">Int</span>, birthday: <span class="type">String</span>) &#123;</span><br><span class="line">    <span class="comment">// 2.每个辅助构造器必须以主构造器或其他的辅助构造器的调用开始</span></span><br><span class="line">    <span class="keyword">this</span>(name, age)</span><br><span class="line">    <span class="keyword">this</span>.birthday = birthday</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 3.重写 toString 方法</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">toString</span></span>: <span class="type">String</span> = name + <span class="string">":"</span> + age + <span class="string">":"</span> + birthday</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    println(<span class="keyword">new</span> <span class="type">Person</span>(<span class="string">"heibaiying"</span>, <span class="number">20</span>, <span class="string">"2019-02-21"</span>))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="2-6-方法传参不可变"><a href="#2-6-方法传参不可变" class="headerlink" title="2.6 方法传参不可变"></a>2.6 方法传参不可变</h3><p>在 Scala 中，方法传参默认是 val 类型，即不可变，这意味着你在方法体内部不能改变传入的参数。这和 Scala 的设计理念有关，Scala 遵循函数式编程理念，强调方法不应该有副作用。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params"></span>) </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">low</span></span>(word: <span class="type">String</span>): <span class="type">String</span> = &#123;</span><br><span class="line">    word=<span class="string">"word"</span> <span class="comment">// 编译无法通过</span></span><br><span class="line">    word.toLowerCase</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><br></p>
<h2 id="三、对象"><a href="#三、对象" class="headerlink" title="三、对象"></a>三、对象</h2><p>Scala 中的 object(对象) 主要有以下几个作用：</p>
<ul>
<li>因为 object 中的变量和方法都是静态的，所以可以用于存放工具类；</li>
<li>可以作为单例对象的容器；</li>
<li>可以作为类的伴生对象；</li>
<li>可以拓展类或特质；</li>
<li>可以拓展 Enumeration 来实现枚举。</li>
</ul>
<h3 id="3-1-工具类-amp-单例-amp-全局静态常量-amp-拓展特质"><a href="#3-1-工具类-amp-单例-amp-全局静态常量-amp-拓展特质" class="headerlink" title="3.1 工具类&amp;单例&amp;全局静态常量&amp;拓展特质"></a>3.1 工具类&amp;单例&amp;全局静态常量&amp;拓展特质</h3><p>这里我们创建一个对象 <code>Utils</code>,代码如下：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Utils</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/*</span></span><br><span class="line"><span class="comment">   *1. 相当于 Java 中的静态代码块 static,会在对象初始化时候被执行</span></span><br><span class="line"><span class="comment">   *   这种方式实现的单例模式是饿汉式单例,即无论你的单例对象是否被用到，</span></span><br><span class="line"><span class="comment">   *   都在一开始被初始化完成</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">val</span> person = <span class="keyword">new</span> <span class="type">Person</span></span><br><span class="line">  </span><br><span class="line">  <span class="comment">// 2. 全局固定常量 等价于 Java 的 public static final </span></span><br><span class="line">  <span class="keyword">val</span> <span class="type">CONSTANT</span> = <span class="string">"固定常量"</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// 3. 全局静态方法</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">low</span></span>(word: <span class="type">String</span>): <span class="type">String</span> = &#123;</span><br><span class="line">    word.toLowerCase</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>其中 Person 类代码如下：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params"></span>) </span>&#123;</span><br><span class="line">  println(<span class="string">"Person 默认构造器被调用"</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>新建测试类：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 1.ScalaApp 对象扩展自 trait App</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaApp</span> <span class="keyword">extends</span> <span class="title">App</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 2.验证单例</span></span><br><span class="line">  println(<span class="type">Utils</span>.person == <span class="type">Utils</span>.person)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 3.获取全局常量</span></span><br><span class="line">  println(<span class="type">Utils</span>.<span class="type">CONSTANT</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 4.调用工具类</span></span><br><span class="line">  println(<span class="type">Utils</span>.low(<span class="string">"ABCDEFG"</span>))</span><br><span class="line">  </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出如下：</span></span><br><span class="line"><span class="type">Person</span> 默认构造器被调用</span><br><span class="line"><span class="literal">true</span></span><br><span class="line">固定常量</span><br><span class="line">abcdefg</span><br></pre></td></tr></table></figure>
<h3 id="3-2-伴生对象"><a href="#3-2-伴生对象" class="headerlink" title="3.2 伴生对象"></a>3.2 伴生对象</h3><p>在 Java 中，你通常会用到既有实例方法又有静态方法的类，在 Scala 中，可以通过类和与类同名的伴生对象来实现。类和伴生对象必须存在与同一个文件中。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params"></span>) </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> name = <span class="string">"HEIBAIYING"</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getName</span></span>: <span class="type">String</span> = &#123;</span><br><span class="line">    <span class="comment">// 调用伴生对象的方法和属性</span></span><br><span class="line">    <span class="type">Person</span>.toLow(<span class="type">Person</span>.<span class="type">PREFIX</span> + name)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 伴生对象</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> <span class="type">PREFIX</span> = <span class="string">"prefix-"</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">toLow</span></span>(word: <span class="type">String</span>): <span class="type">String</span> = &#123;</span><br><span class="line">    word.toLowerCase</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> person = <span class="keyword">new</span> <span class="type">Person</span></span><br><span class="line">    <span class="comment">// 输出 prefix-heibaiying  </span></span><br><span class="line">    println(person.getName)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="3-3-实现枚举类"><a href="#3-3-实现枚举类" class="headerlink" title="3.3 实现枚举类"></a>3.3 实现枚举类</h3><p>Scala 中没有直接提供枚举类，需要通过扩展 <code>Enumeration</code>，并调用其中的 Value 方法对所有枚举值进行初始化来实现。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Color</span> <span class="keyword">extends</span> <span class="title">Enumeration</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 1.类型别名,建议声明,在 import 时有用</span></span><br><span class="line">  <span class="class"><span class="keyword">type</span> <span class="title">Color</span> </span>= <span class="type">Value</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// 2.调用 Value 方法</span></span><br><span class="line">  <span class="keyword">val</span> <span class="type">GREEN</span> = <span class="type">Value</span></span><br><span class="line">  <span class="comment">// 3.只传入 id</span></span><br><span class="line">  <span class="keyword">val</span> <span class="type">RED</span> = <span class="type">Value</span>(<span class="number">3</span>)</span><br><span class="line">  <span class="comment">// 4.只传入值</span></span><br><span class="line">  <span class="keyword">val</span> <span class="type">BULE</span> = <span class="type">Value</span>(<span class="string">"blue"</span>)</span><br><span class="line">  <span class="comment">// 5.传入 id 和值</span></span><br><span class="line">  <span class="keyword">val</span> <span class="type">YELLOW</span> = <span class="type">Value</span>(<span class="number">5</span>, <span class="string">"yellow"</span>)</span><br><span class="line">  <span class="comment">// 6. 不传入 id 时,id 为上一个声明变量的 id+1,值默认和变量名相同</span></span><br><span class="line">  <span class="keyword">val</span> <span class="type">PINK</span> = <span class="type">Value</span></span><br><span class="line"> </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>使用枚举类：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 1.使用类型别名导入枚举类</span></span><br><span class="line"><span class="keyword">import</span> com.heibaiying.<span class="type">Color</span>.<span class="type">Color</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaApp</span> <span class="keyword">extends</span> <span class="title">App</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 2.使用枚举类型,这种情况下需要导入枚举类</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">printColor</span></span>(color: <span class="type">Color</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    println(color.toString)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 3.判断传入值和枚举值是否相等</span></span><br><span class="line">  println(<span class="type">Color</span>.<span class="type">YELLOW</span>.toString == <span class="string">"yellow"</span>)</span><br><span class="line">  <span class="comment">// 4.遍历枚举类和值</span></span><br><span class="line">  <span class="keyword">for</span> (c &lt;- <span class="type">Color</span>.values) println(c.id + <span class="string">":"</span> + c.toString)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//输出</span></span><br><span class="line"><span class="literal">true</span></span><br><span class="line"><span class="number">0</span>:<span class="type">GREEN</span></span><br><span class="line"><span class="number">3</span>:<span class="type">RED</span></span><br><span class="line"><span class="number">4</span>:blue</span><br><span class="line"><span class="number">5</span>:yellow</span><br><span class="line"><span class="number">6</span>:<span class="type">PINK</span></span><br></pre></td></tr></table></figure>
<p><br></p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol>
<li>Martin Odersky . Scala 编程 (第 3 版)[M] . 电子工业出版社 . 2018-1-1  </li>
<li>凯.S.霍斯特曼  . 快学 Scala(第 2 版)[M] . 电子工业出版社 . 2017-7</li>
</ol>
]]></content>
      <categories>
        <category>notes</category>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>Scala</tag>
      </tags>
  </entry>
  <entry>
    <title>继承和特质</title>
    <url>/2021/03/18/Scala%E7%BB%A7%E6%89%BF%E5%92%8C%E7%89%B9%E8%B4%A8/</url>
    <content><![CDATA[<h2 id="一、继承"><a href="#一、继承" class="headerlink" title="一、继承"></a>一、继承</h2><h3 id="1-1-Scala中的继承结构"><a href="#1-1-Scala中的继承结构" class="headerlink" title="1.1 Scala中的继承结构"></a>1.1 Scala中的继承结构</h3><p>Scala 中继承关系如下图：</p>
<ul>
<li>Any 是整个继承关系的根节点；</li>
<li>AnyRef 包含 Scala Classes 和 Java Classes，等价于 Java 中的 java.lang.Object；</li>
<li>AnyVal 是所有值类型的一个标记；</li>
<li>Null 是所有引用类型的子类型，唯一实例是 null，可以将 null 赋值给除了值类型外的所有类型的变量;</li>
<li>Nothing 是所有类型的子类型。</li>
</ul>
<div align="center"> <img src="../pictures/scala继承层次.png"> </div>

<h3 id="1-2-extends-amp-override"><a href="#1-2-extends-amp-override" class="headerlink" title="1.2 extends &amp; override"></a>1.2 extends &amp; override</h3><p>Scala 的集成机制和 Java 有很多相似之处，比如都使用 <code>extends</code> 关键字表示继承，都使用 <code>override</code> 关键字表示重写父类的方法或成员变量。示例如下：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//父类</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">var</span> name = <span class="string">""</span></span><br><span class="line">  <span class="comment">// 1.不加任何修饰词,默认为 public,能被子类和外部访问</span></span><br><span class="line">  <span class="keyword">var</span> age = <span class="number">0</span></span><br><span class="line">  <span class="comment">// 2.使用 protected 修饰的变量能子类访问，但是不能被外部访问</span></span><br><span class="line">  <span class="keyword">protected</span> <span class="keyword">var</span> birthday = <span class="string">""</span></span><br><span class="line">  <span class="comment">// 3.使用 private 修饰的变量不能被子类和外部访问</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> sex = <span class="string">""</span></span><br><span class="line">    </span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">setSex</span></span>(sex: <span class="type">String</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">this</span>.sex = sex</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 4.重写父类的方法建议使用 override 关键字修饰</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">toString</span></span>: <span class="type">String</span> = name + <span class="string">":"</span> + age + <span class="string">":"</span> + birthday + <span class="string">":"</span> + sex</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>使用 <code>extends</code> 关键字实现继承：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 1.使用 extends 关键字实现继承</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Employee</span> <span class="keyword">extends</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">toString</span></span>: <span class="type">String</span> = <span class="string">"Employee~"</span> + <span class="keyword">super</span>.toString</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 2.使用 public 或 protected 关键字修饰的变量能被子类访问</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">setBirthday</span></span>(date: <span class="type">String</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    birthday = date</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>测试继承：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaApp</span> <span class="keyword">extends</span> <span class="title">App</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> employee = <span class="keyword">new</span> <span class="type">Employee</span></span><br><span class="line"></span><br><span class="line">  employee.name = <span class="string">"heibaiying"</span></span><br><span class="line">  employee.age = <span class="number">20</span></span><br><span class="line">  employee.setBirthday(<span class="string">"2019-03-05"</span>)</span><br><span class="line">  employee.setSex(<span class="string">"男"</span>)</span><br><span class="line"></span><br><span class="line">  println(employee)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出： Employee~heibaiying:20:2019-03-05:男</span></span><br></pre></td></tr></table></figure>
<h3 id="1-3-调用超类构造器"><a href="#1-3-调用超类构造器" class="headerlink" title="1.3 调用超类构造器"></a>1.3 调用超类构造器</h3><p>在 Scala 的类中，每个辅助构造器都必须首先调用其他构造器或主构造器，这样就导致了子类的辅助构造器永远无法直接调用超类的构造器，只有主构造器才能调用超类的构造器。所以想要调用超类的构造器，代码示例如下：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Employee</span>(<span class="params">name:<span class="type">String</span>,age:<span class="type">Int</span>,salary:<span class="type">Double</span></span>) <span class="keyword">extends</span> <span class="title">Person</span>(<span class="params">name:<span class="type">String</span>,age:<span class="type">Int</span></span>) </span>&#123;</span><br><span class="line">    .....</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="1-4-类型检查和转换"><a href="#1-4-类型检查和转换" class="headerlink" title="1.4 类型检查和转换"></a>1.4 类型检查和转换</h3><p>想要实现类检查可以使用 <code>isInstanceOf</code>，判断一个实例是否来源于某个类或者其子类，如果是，则可以使用 <code>asInstanceOf</code> 进行强制类型转换。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaApp</span> <span class="keyword">extends</span> <span class="title">App</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> employee = <span class="keyword">new</span> <span class="type">Employee</span></span><br><span class="line">  <span class="keyword">val</span> person = <span class="keyword">new</span> <span class="type">Person</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// 1. 判断一个实例是否来源于某个类或者其子类 输出 true </span></span><br><span class="line">  println(employee.isInstanceOf[<span class="type">Person</span>])</span><br><span class="line">  println(person.isInstanceOf[<span class="type">Person</span>])</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 2. 强制类型转换</span></span><br><span class="line">  <span class="keyword">var</span> p: <span class="type">Person</span> = employee.asInstanceOf[<span class="type">Person</span>]</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 3. 判断一个实例是否来源于某个类 (而不是其子类)</span></span><br><span class="line">  println(employee.getClass == classOf[<span class="type">Employee</span>])</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="1-5-构造顺序和提前定义"><a href="#1-5-构造顺序和提前定义" class="headerlink" title="1.5 构造顺序和提前定义"></a>1.5 构造顺序和提前定义</h3><h4 id="1-构造顺序"><a href="#1-构造顺序" class="headerlink" title="1. 构造顺序"></a><strong>1. 构造顺序</strong></h4><p>在 Scala 中还有一个需要注意的问题，如果你在子类中重写父类的 val 变量，并且超类的构造器中使用了该变量，那么可能会产生不可预期的错误。下面给出一个示例：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 父类</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line">  println(<span class="string">"父类的默认构造器"</span>)</span><br><span class="line">  <span class="keyword">val</span> range: <span class="type">Int</span> = <span class="number">10</span></span><br><span class="line">  <span class="keyword">val</span> array: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">Int</span>](range)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//子类</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Employee</span> <span class="keyword">extends</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line">  println(<span class="string">"子类的默认构造器"</span>)</span><br><span class="line">  <span class="keyword">override</span> <span class="keyword">val</span> range = <span class="number">2</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//测试</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaApp</span> <span class="keyword">extends</span> <span class="title">App</span> </span>&#123;</span><br><span class="line">  <span class="keyword">val</span> employee = <span class="keyword">new</span> <span class="type">Employee</span></span><br><span class="line">  println(employee.array.mkString(<span class="string">"("</span>, <span class="string">","</span>, <span class="string">")"</span>))</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里初始化 array 用到了变量 range，这里你会发现实际上 array 既不会被初始化 Array(10)，也不会被初始化为 Array(2)，实际的输出应该如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">父类的默认构造器</span><br><span class="line">子类的默认构造器</span><br><span class="line">()</span><br></pre></td></tr></table></figure>
<p>可以看到 array 被初始化为 Array(0)，主要原因在于父类构造器的执行顺序先于子类构造器，这里给出实际的执行步骤：</p>
<ol>
<li>父类的构造器被调用，执行 <code>new Array[Int](range)</code> 语句;</li>
<li>这里想要得到 range 的值，会去调用子类 range() 方法，因为 <code>override val</code> 重写变量的同时也重写了其 get 方法；</li>
<li>调用子类的 range() 方法，自然也是返回子类的 range 值，但是由于子类的构造器还没有执行，这也就意味着对 range 赋值的 <code>range = 2</code> 语句还没有被执行，所以自然返回 range 的默认值，也就是 0。</li>
</ol>
<p>这里可能比较疑惑的是为什么 <code>val range = 2</code> 没有被执行，却能使用 range 变量，这里因为在虚拟机层面，是先对成员变量先分配存储空间并赋给默认值，之后才赋予给定的值。想要证明这一点其实也比较简单，代码如下:</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line">  <span class="comment">// val range: Int = 10 正常代码 array 为 Array(10)</span></span><br><span class="line">  <span class="keyword">val</span> array: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">Int</span>](range)</span><br><span class="line">  <span class="keyword">val</span> range: <span class="type">Int</span> = <span class="number">10</span>  <span class="comment">//如果把变量的声明放在使用之后，此时数据 array 为 array(0)</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> person = <span class="keyword">new</span> <span class="type">Person</span></span><br><span class="line">    println(person.array.mkString(<span class="string">"("</span>, <span class="string">","</span>, <span class="string">")"</span>))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="2-提前定义"><a href="#2-提前定义" class="headerlink" title="2. 提前定义"></a><strong>2. 提前定义</strong></h4><p>想要解决上面的问题，有以下几种方法：</p>
<p>(1) . 将变量用 final 修饰，代表不允许被子类重写，即 <code>final val range: Int = 10</code>；</p>
<p>(2) . 将变量使用 lazy 修饰，代表懒加载，即只有当你实际使用到 array 时候，才去进行初始化；</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">lazy</span> <span class="keyword">val</span> array: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">Int</span>](range)</span><br></pre></td></tr></table></figure>
<p>(3) . 采用提前定义，代码如下，代表 range 的定义优先于超类构造器。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Employee</span> <span class="keyword">extends</span> </span>&#123;</span><br><span class="line">  <span class="comment">//这里不能定义其他方法</span></span><br><span class="line">  <span class="keyword">override</span> <span class="keyword">val</span> range = <span class="number">2</span></span><br><span class="line">&#125; <span class="keyword">with</span> <span class="type">Person</span> &#123;</span><br><span class="line">  <span class="comment">// 定义其他变量或者方法</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">pr</span></span>(): <span class="type">Unit</span> = &#123;println(<span class="string">"Employee"</span>)&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>但是这种语法也有其限制：你只能在上面代码块中重写已有的变量，而不能定义新的变量和方法，定义新的变量和方法只能写在下面代码块中。</p>
<blockquote>
<p><strong>注意事项</strong>：类的继承和下文特质 (trait) 的继承都存在这个问题，也同样可以通过提前定义来解决。虽然如此，但还是建议合理设计以规避该类问题。</p>
</blockquote>
<p><br></p>
<h2 id="二、抽象类"><a href="#二、抽象类" class="headerlink" title="二、抽象类"></a>二、抽象类</h2><p>Scala 中允许使用 <code>abstract</code> 定义抽象类，并且通过 <code>extends</code> 关键字继承它。</p>
<p>定义抽象类：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 1.定义字段</span></span><br><span class="line">  <span class="keyword">var</span> name: <span class="type">String</span></span><br><span class="line">  <span class="keyword">val</span> age: <span class="type">Int</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// 2.定义抽象方法</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">geDetail</span></span>: <span class="type">String</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// 3. scala 的抽象类允许定义具体方法</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">print</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    println(<span class="string">"抽象类中的默认方法"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>继承抽象类：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Employee</span> <span class="keyword">extends</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 覆盖抽象类中变量</span></span><br><span class="line">  <span class="keyword">override</span> <span class="keyword">var</span> name: <span class="type">String</span> = <span class="string">"employee"</span></span><br><span class="line">  <span class="keyword">override</span> <span class="keyword">val</span> age: <span class="type">Int</span> = <span class="number">12</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// 覆盖抽象方法</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">geDetail</span></span>: <span class="type">String</span> = name + <span class="string">":"</span> + age</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><br></p>
<h2 id="三、特质"><a href="#三、特质" class="headerlink" title="三、特质"></a>三、特质</h2><h3 id="3-1-trait-amp-with"><a href="#3-1-trait-amp-with" class="headerlink" title="3.1 trait &amp; with"></a>3.1 trait &amp; with</h3><p>Scala 中没有 interface 这个关键字，想要实现类似的功能，可以使用特质 (trait)。trait 等价于 Java 8 中的接口，因为 trait 中既能定义抽象方法，也能定义具体方法，这和 Java 8 中的接口是类似的。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 1.特质使用 trait 关键字修饰</span></span><br><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">Logger</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 2.定义抽象方法</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">log</span></span>(msg: <span class="type">String</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 3.定义具体方法</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">logInfo</span></span>(msg: <span class="type">String</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    println(<span class="string">"INFO:"</span> + msg)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>想要使用特质，需要使用 <code>extends</code> 关键字，而不是 <code>implements</code> 关键字，如果想要添加多个特质，可以使用 <code>with</code> 关键字。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 1.使用 extends 关键字,而不是 implements,如果想要添加多个特质，可以使用 with 关键字</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ConsoleLogger</span> <span class="keyword">extends</span> <span class="title">Logger</span> <span class="keyword">with</span> <span class="title">Serializable</span> <span class="keyword">with</span> <span class="title">Cloneable</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 2. 实现特质中的抽象方法</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">log</span></span>(msg: <span class="type">String</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    println(<span class="string">"CONSOLE:"</span> + msg)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="3-2-特质中的字段"><a href="#3-2-特质中的字段" class="headerlink" title="3.2 特质中的字段"></a>3.2 特质中的字段</h3><p>和方法一样，特质中的字段可以是抽象的，也可以是具体的：</p>
<ul>
<li>如果是抽象字段，则混入特质的类需要重写覆盖该字段；</li>
<li>如果是具体字段，则混入特质的类获得该字段，但是并非是通过继承关系得到，而是在编译时候，简单将该字段加入到子类。</li>
</ul>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">Logger</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 抽象字段</span></span><br><span class="line">  <span class="keyword">var</span> <span class="type">LogLevel</span>:<span class="type">String</span></span><br><span class="line">  <span class="comment">// 具体字段</span></span><br><span class="line">  <span class="keyword">var</span> <span class="type">LogType</span> = <span class="string">"FILE"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>覆盖抽象字段：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">InfoLogger</span> <span class="keyword">extends</span> <span class="title">Logger</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 覆盖抽象字段</span></span><br><span class="line">  <span class="keyword">override</span> <span class="keyword">var</span> <span class="type">LogLevel</span>: <span class="type">String</span> = <span class="string">"INFO"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="3-3-带有特质的对象"><a href="#3-3-带有特质的对象" class="headerlink" title="3.3 带有特质的对象"></a>3.3 带有特质的对象</h3><p>Scala 支持在类定义的时混入 <code>父类 trait</code>，而在类实例化为具体对象的时候指明其实际使用的 <code>子类 trait</code>。示例如下：</p>
<div align="center"> <img src="../pictures/scala带有特质的对象.png"> </div>

<p>trait Logger：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 父类</span></span><br><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">Logger</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 定义空方法 日志打印</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">log</span></span>(msg: <span class="type">String</span>) &#123;&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>trait ErrorLogger：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 错误日志打印，继承自 Logger</span></span><br><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">ErrorLogger</span> <span class="keyword">extends</span> <span class="title">Logger</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 覆盖空方法</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">log</span></span>(msg: <span class="type">String</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    println(<span class="string">"Error:"</span> + msg)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>trait InfoLogger：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 通知日志打印，继承自 Logger</span></span><br><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">InfoLogger</span> <span class="keyword">extends</span> <span class="title">Logger</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 覆盖空方法</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">log</span></span>(msg: <span class="type">String</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    println(<span class="string">"INFO:"</span> + msg)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>具体的使用类：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 混入 trait Logger</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span> <span class="keyword">extends</span> <span class="title">Logger</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 调用定义的抽象方法</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">printDetail</span></span>(detail: <span class="type">String</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    log(detail)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里通过 main 方法来测试：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaApp</span> <span class="keyword">extends</span> <span class="title">App</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 使用 with 指明需要具体使用的 trait  </span></span><br><span class="line">  <span class="keyword">val</span> person01 = <span class="keyword">new</span> <span class="type">Person</span> <span class="keyword">with</span> <span class="type">InfoLogger</span></span><br><span class="line">  <span class="keyword">val</span> person02 = <span class="keyword">new</span> <span class="type">Person</span> <span class="keyword">with</span> <span class="type">ErrorLogger</span></span><br><span class="line">  <span class="keyword">val</span> person03 = <span class="keyword">new</span>  <span class="type">Person</span> <span class="keyword">with</span> <span class="type">InfoLogger</span> <span class="keyword">with</span> <span class="type">ErrorLogger</span></span><br><span class="line">  person01.log(<span class="string">"scala"</span>)  <span class="comment">//输出 INFO:scala</span></span><br><span class="line">  person02.log(<span class="string">"scala"</span>)  <span class="comment">//输出 Error:scala</span></span><br><span class="line">  person03.log(<span class="string">"scala"</span>)  <span class="comment">//输出 Error:scala</span></span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里前面两个输出比较明显，因为只指明了一个具体的 <code>trait</code>，这里需要说明的是第三个输出，<strong>因为 trait 的调用是由右到左开始生效的</strong>，所以这里打印出 <code>Error:scala</code>。</p>
<h3 id="3-4-特质构造顺序"><a href="#3-4-特质构造顺序" class="headerlink" title="3.4 特质构造顺序"></a>3.4 特质构造顺序</h3><p><code>trait</code> 有默认的无参构造器，但是不支持有参构造器。一个类混入多个特质后初始化顺序应该如下：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 示例</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Employee</span> <span class="keyword">extends</span> <span class="title">Person</span> <span class="keyword">with</span> <span class="title">InfoLogger</span> <span class="keyword">with</span> <span class="title">ErrorLogger</span> </span>&#123;...&#125;</span><br></pre></td></tr></table></figure>
<ol>
<li>超类首先被构造，即 Person 的构造器首先被执行；</li>
<li>特质的构造器在超类构造器之前，在类构造器之后；特质由左到右被构造；每个特质中，父特质首先被构造；<ul>
<li>Logger 构造器执行（Logger 是 InfoLogger 的父类）；</li>
<li>InfoLogger 构造器执行；</li>
<li>ErrorLogger 构造器执行;</li>
</ul>
</li>
<li>所有超类和特质构造完毕，子类才会被构造。</li>
</ol>
<p><br></p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol>
<li>Martin Odersky . Scala 编程 (第 3 版)[M] . 电子工业出版社 . 2018-1-1  </li>
<li>凯.S.霍斯特曼  . 快学 Scala(第 2 版)[M] . 电子工业出版社 . 2017-7</li>
</ol>
]]></content>
      <categories>
        <category>notes</category>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>Scala</tag>
      </tags>
  </entry>
  <entry>
    <title>隐式转换和隐式参数</title>
    <url>/2021/03/18/Scala%E9%9A%90%E5%BC%8F%E8%BD%AC%E6%8D%A2%E5%92%8C%E9%9A%90%E5%BC%8F%E5%8F%82%E6%95%B0/</url>
    <content><![CDATA[<h2 id="一、隐式转换"><a href="#一、隐式转换" class="headerlink" title="一、隐式转换"></a>一、隐式转换</h2><h3 id="1-1-使用隐式转换"><a href="#1-1-使用隐式转换" class="headerlink" title="1.1 使用隐式转换"></a>1.1 使用隐式转换</h3><p>隐式转换指的是以 <code>implicit</code> 关键字声明带有单个参数的转换函数，它将值从一种类型转换为另一种类型，以便使用之前类型所没有的功能。示例如下：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 普通人</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">val name: <span class="type">String</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">//</span> <span class="title">雷神</span></span></span><br><span class="line"><span class="class"><span class="title">class</span> <span class="title">Thor</span>(<span class="params">val name: <span class="type">String</span></span>) </span>&#123;</span><br><span class="line">  <span class="comment">// 正常情况下只有雷神才能举起雷神之锤</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">hammer</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    println(name + <span class="string">"举起雷神之锤"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Thor</span> <span class="keyword">extends</span> <span class="title">App</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 定义隐式转换方法 将普通人转换为雷神 通常建议方法名使用 source2Target,即：被转换对象 To 转换对象</span></span><br><span class="line">  <span class="keyword">implicit</span> <span class="function"><span class="keyword">def</span> <span class="title">person2Thor</span></span>(p: <span class="type">Person</span>): <span class="type">Thor</span> = <span class="keyword">new</span> <span class="type">Thor</span>(p.name)</span><br><span class="line">  <span class="comment">// 这样普通人也能举起雷神之锤</span></span><br><span class="line">  <span class="keyword">new</span> <span class="type">Person</span>(<span class="string">"普通人"</span>).hammer()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">输出： 普通人举起雷神之锤</span><br></pre></td></tr></table></figure>
<h3 id="1-2-隐式转换规则"><a href="#1-2-隐式转换规则" class="headerlink" title="1.2 隐式转换规则"></a>1.2 隐式转换规则</h3><p>并不是你使用 <code>implicit</code> 转换后，隐式转换就一定会发生，比如上面如果不调用 <code>hammer()</code> 方法的时候，普通人就还是普通人。通常程序会在以下情况下尝试执行隐式转换：</p>
<ul>
<li>当对象访问一个不存在的成员时，即调用的方法不存在或者访问的成员变量不存在；</li>
<li>当对象调用某个方法，该方法存在，但是方法的声明参数与传入参数不匹配时。</li>
</ul>
<p>而在以下三种情况下编译器不会尝试执行隐式转换：</p>
<ul>
<li>如果代码能够在不使用隐式转换的前提下通过编译，则不会使用隐式转换；</li>
<li>编译器不会尝试同时执行多个转换，比如 <code>convert1(convert2(a))*b</code>；</li>
<li>转换存在二义性，也不会发生转换。</li>
</ul>
<p>这里首先解释一下二义性，上面的代码进行如下修改，由于两个隐式转换都是生效的，所以就存在了二义性：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//两个隐式转换都是有效的</span></span><br><span class="line"><span class="keyword">implicit</span> <span class="function"><span class="keyword">def</span> <span class="title">person2Thor</span></span>(p: <span class="type">Person</span>): <span class="type">Thor</span> = <span class="keyword">new</span> <span class="type">Thor</span>(p.name)</span><br><span class="line"><span class="keyword">implicit</span> <span class="function"><span class="keyword">def</span> <span class="title">person2Thor2</span></span>(p: <span class="type">Person</span>): <span class="type">Thor</span> = <span class="keyword">new</span> <span class="type">Thor</span>(p.name)</span><br><span class="line"><span class="comment">// 此时下面这段语句无法通过编译</span></span><br><span class="line"><span class="keyword">new</span> <span class="type">Person</span>(<span class="string">"普通人"</span>).hammer()</span><br></pre></td></tr></table></figure>
<p>其次再解释一下多个转换的问题：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ClassA</span> </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">toString</span> </span>= <span class="string">"This is Class A"</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ClassB</span> </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">toString</span> </span>= <span class="string">"This is Class B"</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">printB</span></span>(b: <span class="type">ClassB</span>): <span class="type">Unit</span> = println(b)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ClassC</span></span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">class</span> <span class="title">ClassD</span></span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">object</span> <span class="title">ImplicitTest</span> <span class="keyword">extends</span> <span class="title">App</span> </span>&#123;</span><br><span class="line">  <span class="keyword">implicit</span> <span class="function"><span class="keyword">def</span> <span class="title">A2B</span></span>(a: <span class="type">ClassA</span>): <span class="type">ClassB</span> = &#123;</span><br><span class="line">    println(<span class="string">"A2B"</span>)</span><br><span class="line">    <span class="keyword">new</span> <span class="type">ClassB</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">implicit</span> <span class="function"><span class="keyword">def</span> <span class="title">C2B</span></span>(c: <span class="type">ClassC</span>): <span class="type">ClassB</span> = &#123;</span><br><span class="line">    println(<span class="string">"C2B"</span>)</span><br><span class="line">    <span class="keyword">new</span> <span class="type">ClassB</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">implicit</span> <span class="function"><span class="keyword">def</span> <span class="title">D2C</span></span>(d: <span class="type">ClassD</span>): <span class="type">ClassC</span> = &#123;</span><br><span class="line">    println(<span class="string">"D2C"</span>)</span><br><span class="line">    <span class="keyword">new</span> <span class="type">ClassC</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 这行代码无法通过编译，因为要调用到 printB 方法，需要执行两次转换 C2B(D2C(ClassD))</span></span><br><span class="line">  <span class="keyword">new</span> <span class="type">ClassD</span>().printB(<span class="keyword">new</span> <span class="type">ClassA</span>)</span><br><span class="line">    </span><br><span class="line">  <span class="comment">/*</span></span><br><span class="line"><span class="comment">   *  下面的这一行代码虽然也进行了两次隐式转换，但是两次的转换对象并不是一个对象,所以它是生效的:</span></span><br><span class="line"><span class="comment">   *  转换流程如下:</span></span><br><span class="line"><span class="comment">   *  1. ClassC 中并没有 printB 方法,因此隐式转换为 ClassB,然后调用 printB 方法;</span></span><br><span class="line"><span class="comment">   *  2. 但是 printB 参数类型为 ClassB,然而传入的参数类型是 ClassA,所以需要将参数 ClassA 转换为 ClassB,这是第二次;</span></span><br><span class="line"><span class="comment">   *  即: C2B(ClassC) -&gt; ClassB.printB(ClassA) -&gt; ClassB.printB(A2B(ClassA)) -&gt; ClassB.printB(ClassB)</span></span><br><span class="line"><span class="comment">   *  转换过程 1 的对象是 ClassC,而转换过程 2 的转换对象是 ClassA,所以虽然是一行代码两次转换，但是仍然是有效转换</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">new</span> <span class="type">ClassC</span>().printB(<span class="keyword">new</span> <span class="type">ClassA</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出：</span></span><br><span class="line"><span class="type">C2B</span></span><br><span class="line"><span class="type">A2B</span></span><br><span class="line"><span class="type">This</span> is <span class="type">Class</span> <span class="type">B</span></span><br></pre></td></tr></table></figure>
<h3 id="1-3-引入隐式转换"><a href="#1-3-引入隐式转换" class="headerlink" title="1.3 引入隐式转换"></a>1.3 引入隐式转换</h3><p>隐式转换的可以定义在以下三个地方：</p>
<ul>
<li>定义在原类型的伴生对象中；</li>
<li>直接定义在执行代码的上下文作用域中；</li>
<li>统一定义在一个文件中，在使用时候导入。</li>
</ul>
<p>上面我们使用的方法相当于直接定义在执行代码的作用域中，下面分别给出其他两种定义的代码示例：</p>
<p><strong>定义在原类型的伴生对象中</strong>：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">val name: <span class="type">String</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">//</span> <span class="title">在伴生对象中定义隐式转换函数</span></span></span><br><span class="line"><span class="class"><span class="title">object</span> <span class="title">Person</span></span>&#123;</span><br><span class="line">  <span class="keyword">implicit</span> <span class="function"><span class="keyword">def</span> <span class="title">person2Thor</span></span>(p: <span class="type">Person</span>): <span class="type">Thor</span> = <span class="keyword">new</span> <span class="type">Thor</span>(p.name)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Thor</span>(<span class="params">val name: <span class="type">String</span></span>) </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">hammer</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    println(name + <span class="string">"举起雷神之锤"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 使用示例</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaApp</span> <span class="keyword">extends</span> <span class="title">App</span> </span>&#123;</span><br><span class="line">  <span class="keyword">new</span> <span class="type">Person</span>(<span class="string">"普通人"</span>).hammer()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>定义在一个公共的对象中</strong>：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Convert</span> </span>&#123;</span><br><span class="line">  <span class="keyword">implicit</span> <span class="function"><span class="keyword">def</span> <span class="title">person2Thor</span></span>(p: <span class="type">Person</span>): <span class="type">Thor</span> = <span class="keyword">new</span> <span class="type">Thor</span>(p.name)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 导入 Convert 下所有的隐式转换函数</span></span><br><span class="line"><span class="keyword">import</span> com.heibaiying.<span class="type">Convert</span>._</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaApp</span> <span class="keyword">extends</span> <span class="title">App</span> </span>&#123;</span><br><span class="line">  <span class="keyword">new</span> <span class="type">Person</span>(<span class="string">"普通人"</span>).hammer()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注：Scala 自身的隐式转换函数大部分定义在 <code>Predef.scala</code> 中，你可以打开源文件查看，也可以在 Scala 交互式命令行中采用 <code>:implicit -v</code> 查看全部隐式转换函数。</p>
</blockquote>
<p><br></p>
<h2 id="二、隐式参数"><a href="#二、隐式参数" class="headerlink" title="二、隐式参数"></a>二、隐式参数</h2><h3 id="2-1-使用隐式参数"><a href="#2-1-使用隐式参数" class="headerlink" title="2.1 使用隐式参数"></a>2.1 使用隐式参数</h3><p>在定义函数或方法时可以使用标记为 <code>implicit</code> 的参数，这种情况下，编译器将会查找默认值，提供给函数调用。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 定义分隔符类</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Delimiters</span>(<span class="params">val left: <span class="type">String</span>, val right: <span class="type">String</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">object</span> <span class="title">ScalaApp</span> <span class="keyword">extends</span> <span class="title">App</span> </span>&#123;</span><br><span class="line">  </span><br><span class="line">    <span class="comment">// 进行格式化输出</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">formatted</span></span>(context: <span class="type">String</span>)(<span class="keyword">implicit</span> deli: <span class="type">Delimiters</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    println(deli.left + context + deli.right)</span><br><span class="line">  &#125;</span><br><span class="line">    </span><br><span class="line">  <span class="comment">// 定义一个隐式默认值 使用左右中括号作为分隔符</span></span><br><span class="line">  <span class="keyword">implicit</span> <span class="keyword">val</span> bracket = <span class="keyword">new</span> <span class="type">Delimiters</span>(<span class="string">"("</span>, <span class="string">")"</span>)</span><br><span class="line">  formatted(<span class="string">"this is context"</span>) <span class="comment">// 输出: (this is context)</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>关于隐式参数，有两点需要注意：</p>
<p>1.我们上面定义 <code>formatted</code> 函数的时候使用了柯里化，如果你不使用柯里化表达式，按照通常习惯只有下面两种写法：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 这种写法没有语法错误，但是无法通过编译</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">formatted</span></span>(<span class="keyword">implicit</span> context: <span class="type">String</span>, deli: <span class="type">Delimiters</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">  println(deli.left + context + deli.right)</span><br><span class="line">&#125; </span><br><span class="line"><span class="comment">// 不存在这种写法，IDEA 直接会直接提示语法错误</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">formatted</span></span>( context: <span class="type">String</span>,  <span class="keyword">implicit</span> deli: <span class="type">Delimiters</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">  println(deli.left + context + deli.right)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上面第一种写法编译的时候会出现下面所示 <code>error</code> 信息,从中也可以看出 <code>implicit</code> 是作用于参数列表中每个参数的，这显然不是我们想要到达的效果，所以上面的写法采用了柯里化。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">not enough arguments for method formatted: </span><br><span class="line">(implicit context: String, implicit deli: com.heibaiying.Delimiters)</span><br></pre></td></tr></table></figure>
<p>2.第二个问题和隐式函数一样，隐式默认值不能存在二义性，否则无法通过编译，示例如下：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">implicit</span> <span class="keyword">val</span> bracket = <span class="keyword">new</span> <span class="type">Delimiters</span>(<span class="string">"("</span>, <span class="string">")"</span>)</span><br><span class="line"><span class="keyword">implicit</span> <span class="keyword">val</span> brace = <span class="keyword">new</span> <span class="type">Delimiters</span>(<span class="string">"&#123;"</span>, <span class="string">"&#125;"</span>)</span><br><span class="line">formatted(<span class="string">"this is context"</span>)</span><br></pre></td></tr></table></figure>
<p>上面代码无法通过编译，出现错误提示 <code>ambiguous implicit values</code>，即隐式值存在冲突。</p>
<h3 id="2-2-引入隐式参数"><a href="#2-2-引入隐式参数" class="headerlink" title="2.2 引入隐式参数"></a>2.2 引入隐式参数</h3><p>引入隐式参数和引入隐式转换函数方法是一样的，有以下三种方式：</p>
<ul>
<li>定义在隐式参数对应类的伴生对象中；</li>
<li>直接定义在执行代码的上下文作用域中；</li>
<li>统一定义在一个文件中，在使用时候导入。</li>
</ul>
<p>我们上面示例程序相当于直接定义执行代码的上下文作用域中，下面给出其他两种方式的示例：</p>
<p><strong>定义在隐式参数对应类的伴生对象中</strong>；</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Delimiters</span>(<span class="params">val left: <span class="type">String</span>, val right: <span class="type">String</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">object</span> <span class="title">Delimiters</span> </span>&#123;</span><br><span class="line">  <span class="keyword">implicit</span> <span class="keyword">val</span> bracket = <span class="keyword">new</span> <span class="type">Delimiters</span>(<span class="string">"("</span>, <span class="string">")"</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 此时执行代码的上下文中不用定义</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaApp</span> <span class="keyword">extends</span> <span class="title">App</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">formatted</span></span>(context: <span class="type">String</span>)(<span class="keyword">implicit</span> deli: <span class="type">Delimiters</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    println(deli.left + context + deli.right)</span><br><span class="line">  &#125;</span><br><span class="line">  formatted(<span class="string">"this is context"</span>) </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>统一定义在一个文件中，在使用时候导入</strong>：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Convert</span> </span>&#123;</span><br><span class="line">  <span class="keyword">implicit</span> <span class="keyword">val</span> bracket = <span class="keyword">new</span> <span class="type">Delimiters</span>(<span class="string">"("</span>, <span class="string">")"</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 在使用的时候导入</span></span><br><span class="line"><span class="keyword">import</span> com.heibaiying.<span class="type">Convert</span>.bracket</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaApp</span> <span class="keyword">extends</span> <span class="title">App</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">formatted</span></span>(context: <span class="type">String</span>)(<span class="keyword">implicit</span> deli: <span class="type">Delimiters</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    println(deli.left + context + deli.right)</span><br><span class="line">  &#125;</span><br><span class="line">  formatted(<span class="string">"this is context"</span>) <span class="comment">// 输出: (this is context)</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="2-3-利用隐式参数进行隐式转换"><a href="#2-3-利用隐式参数进行隐式转换" class="headerlink" title="2.3 利用隐式参数进行隐式转换"></a>2.3 利用隐式参数进行隐式转换</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">smaller</span></span>[<span class="type">T</span>] (a: <span class="type">T</span>, b: <span class="type">T</span>) = <span class="keyword">if</span> (a &lt; b) a <span class="keyword">else</span> b</span><br></pre></td></tr></table></figure>
<p>在 Scala 中如果定义了一个如上所示的比较对象大小的泛型方法，你会发现无法通过编译。对于对象之间进行大小比较，Scala 和 Java 一样，都要求被比较的对象需要实现 java.lang.Comparable 接口。在 Scala 中，直接继承 Java 中 Comparable 接口的是特质 Ordered，它在继承 compareTo 方法的基础上，额外定义了关系符方法，源码如下:</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">Ordered</span>[<span class="type">A</span>] <span class="keyword">extends</span> <span class="title">Any</span> <span class="keyword">with</span> <span class="title">java</span>.<span class="title">lang</span>.<span class="title">Comparable</span>[<span class="type">A</span>] </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">compare</span></span>(that: <span class="type">A</span>): <span class="type">Int</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">&lt;</span>  </span>(that: <span class="type">A</span>): <span class="type">Boolean</span> = (<span class="keyword">this</span> compare that) &lt;  <span class="number">0</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">&gt;</span>  </span>(that: <span class="type">A</span>): <span class="type">Boolean</span> = (<span class="keyword">this</span> compare that) &gt;  <span class="number">0</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">&lt;=</span> </span>(that: <span class="type">A</span>): <span class="type">Boolean</span> = (<span class="keyword">this</span> compare that) &lt;= <span class="number">0</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">&gt;=</span> </span>(that: <span class="type">A</span>): <span class="type">Boolean</span> = (<span class="keyword">this</span> compare that) &gt;= <span class="number">0</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">compareTo</span></span>(that: <span class="type">A</span>): <span class="type">Int</span> = compare(that)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>所以要想在泛型中解决这个问题，有两种方法：</p>
<h4 id="1-使用视图界定"><a href="#1-使用视图界定" class="headerlink" title="1. 使用视图界定"></a>1. 使用视图界定</h4><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Pair</span> <span class="keyword">extends</span> <span class="title">App</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"> <span class="comment">// 视图界定</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">smaller</span></span>[<span class="type">T</span>&lt;% <span class="type">Ordered</span>[<span class="type">T</span>]](a: <span class="type">T</span>, b: <span class="type">T</span>) = <span class="keyword">if</span> (a &lt; b) a <span class="keyword">else</span> b</span><br><span class="line"> </span><br><span class="line">  println(smaller(<span class="number">1</span>,<span class="number">2</span>)) <span class="comment">//输出 1</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>视图限定限制了 T 可以通过隐式转换 <code>Ordered[T]</code>，即对象一定可以进行大小比较。在上面的代码中 <code>smaller(1,2)</code> 中参数 <code>1</code> 和 <code>2</code> 实际上是通过定义在 <code>Predef</code> 中的隐式转换方法 <code>intWrapper</code> 转换为 <code>RichInt</code>。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Predef.scala</span></span><br><span class="line"><span class="meta">@inline</span> <span class="keyword">implicit</span> <span class="function"><span class="keyword">def</span> <span class="title">intWrapper</span></span>(x: <span class="type">Int</span>)   = <span class="keyword">new</span> runtime.<span class="type">RichInt</span>(x)</span><br></pre></td></tr></table></figure>
<p>为什么要这么麻烦执行隐式转换，原因是 Scala 中的 Int 类型并不能直接进行比较，因为其没有实现 <code>Ordered</code> 特质，真正实现 <code>Ordered</code> 特质的是 <code>RichInt</code>。</p>
<div align="center"> <img src="../pictures/scala-richInt.png"> </div>



<h4 id="2-利用隐式参数进行隐式转换"><a href="#2-利用隐式参数进行隐式转换" class="headerlink" title="2. 利用隐式参数进行隐式转换"></a>2. 利用隐式参数进行隐式转换</h4><p>Scala2.11+ 后，视图界定被标识为废弃，官方推荐使用类型限定来解决上面的问题，本质上就是使用隐式参数进行隐式转换。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Pair</span> <span class="keyword">extends</span> <span class="title">App</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">   <span class="comment">// order 既是一个隐式参数也是一个隐式转换，即如果 a 不存在 &lt; 方法，则转换为 order(a)&lt;b</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">smaller</span></span>[<span class="type">T</span>](a: <span class="type">T</span>, b: <span class="type">T</span>)(<span class="keyword">implicit</span> order: <span class="type">T</span> =&gt; <span class="type">Ordered</span>[<span class="type">T</span>]) = <span class="keyword">if</span> (a &lt; b) a <span class="keyword">else</span> b</span><br><span class="line"></span><br><span class="line">  println(smaller(<span class="number">1</span>,<span class="number">2</span>)) <span class="comment">//输出 1</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol>
<li>Martin Odersky . Scala 编程 (第 3 版)[M] . 电子工业出版社 . 2018-1-1  </li>
<li>凯.S.霍斯特曼  . 快学 Scala(第 2 版)[M] . 电子工业出版社 . 2017-7</li>
</ol>
]]></content>
      <categories>
        <category>notes</category>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>Scala</tag>
      </tags>
  </entry>
  <entry>
    <title>集合</title>
    <url>/2021/03/18/Scala%E9%9B%86%E5%90%88%E7%B1%BB%E5%9E%8B/</url>
    <content><![CDATA[<h2 id="一、集合简介"><a href="#一、集合简介" class="headerlink" title="一、集合简介"></a>一、集合简介</h2><p>Scala 中拥有多种集合类型，主要分为可变的和不可变的集合两大类：</p>
<ul>
<li><p><strong>可变集合</strong>： 可以被修改。即可以更改，添加，删除集合中的元素；</p>
</li>
<li><p><strong>不可变集合类</strong>：不能被修改。对集合执行更改，添加或删除操作都会返回一个新的集合，而不是修改原来的集合。</p>
</li>
</ul>
<h2 id="二、集合结构"><a href="#二、集合结构" class="headerlink" title="二、集合结构"></a>二、集合结构</h2><p>Scala 中的大部分集合类都存在三类变体，分别位于 <code>scala.collection</code>, <code>scala.collection.immutable</code>, <code>scala.collection.mutable</code> 包中。还有部分集合类位于 <code>scala.collection.generic</code> 包下。</p>
<ul>
<li><strong>scala.collection.immutable</strong> ：包是中的集合是不可变的；</li>
<li><strong>scala.collection.mutable</strong> ：包中的集合是可变的；</li>
<li><strong>scala.collection</strong> ：包中的集合，既可以是可变的，也可以是不可变的。</li>
</ul>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> sortSet = scala.collection.<span class="type">SortedSet</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>)</span><br><span class="line"><span class="keyword">val</span> mutableSet = collection.mutable.<span class="type">SortedSet</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>)</span><br><span class="line"><span class="keyword">val</span> immutableSet = collection.immutable.<span class="type">SortedSet</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>)</span><br></pre></td></tr></table></figure>
<p>如果你仅写了 <code>Set</code> 而没有加任何前缀也没有进行任何 <code>import</code>，则 Scala 默认采用不可变集合类。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="type">Set</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>)</span><br><span class="line">res0: scala.collection.immutable.<span class="type">Set</span>[<span class="type">Int</span>] = <span class="type">Set</span>(<span class="number">5</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br></pre></td></tr></table></figure>
<h3 id="3-1-scala-collection"><a href="#3-1-scala-collection" class="headerlink" title="3.1 scala.collection"></a>3.1 scala.collection</h3><p>scala.collection 包中所有集合如下图：</p>
<div align="center"> <img src="../pictures/scala-collection.png"> </div>

<h3 id="3-2-scala-collection-mutable"><a href="#3-2-scala-collection-mutable" class="headerlink" title="3.2 scala.collection.mutable"></a>3.2 scala.collection.mutable</h3><p>scala.collection.mutable 包中所有集合如下图：</p>
<div align="center"> <img src="../pictures/scala-collection-m.png"> </div>

<h3 id="3-2-scala-collection-immutable"><a href="#3-2-scala-collection-immutable" class="headerlink" title="3.2 scala.collection.immutable"></a>3.2 scala.collection.immutable</h3><p>scala.collection.immutable 包中所有集合如下图：</p>
<div align="center"> <img src="../pictures/scala-collection-imm.png"> </div>

<h2 id="三、Trait-Traversable"><a href="#三、Trait-Traversable" class="headerlink" title="三、Trait Traversable"></a>三、Trait Traversable</h2><p>Scala 中所有集合的顶层实现是 <code>Traversable</code> 。它唯一的抽象方法是 <code>foreach</code>：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">foreach</span></span>[<span class="type">U</span>](f: <span class="type">Elem</span> =&gt; <span class="type">U</span>)</span><br></pre></td></tr></table></figure>
<p>实现 <code>Traversable</code> 的集合类只需要实现这个抽象方法，其他方法可以从 <code>Traversable</code> 继承。<code>Traversable</code> 中的所有可用方法如下：</p>
<table>
<thead>
<tr>
<th><strong>方法</strong></th>
<th><strong>作用</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Abstract Method:</strong></td>
<td></td>
</tr>
<tr>
<td><code>xs foreach f</code></td>
<td>为 xs 的每个元素执行函数 f</td>
</tr>
<tr>
<td><strong>Addition:</strong></td>
<td></td>
</tr>
<tr>
<td><code>xs ++ ys</code></td>
<td>一个包含 xs 和 ys 中所有元素的新的集合。 ys 是一个 Traversable 或 Iterator。</td>
</tr>
<tr>
<td><strong>Maps:</strong></td>
<td></td>
</tr>
<tr>
<td><code>xs map f</code></td>
<td>对 xs 中每一个元素应用函数 f，并返回一个新的集合</td>
</tr>
<tr>
<td><code>xs flatMap f</code></td>
<td>对 xs 中每一个元素应用函数 f，最后将结果合并成一个新的集合</td>
</tr>
<tr>
<td><code>xs collect f</code></td>
<td>对 xs 中每一个元素调用偏函数 f，并返回一个新的集合</td>
</tr>
<tr>
<td><strong>Conversions:</strong></td>
<td></td>
</tr>
<tr>
<td><code>xs.toArray</code></td>
<td>将集合转化为一个 Array</td>
</tr>
<tr>
<td><code>xs.toList</code></td>
<td>将集合转化为一个 List</td>
</tr>
<tr>
<td><code>xs.toIterable</code></td>
<td>将集合转化为一个 Iterable</td>
</tr>
<tr>
<td><code>xs.toSeq</code></td>
<td>将集合转化为一个 Seq</td>
</tr>
<tr>
<td><code>xs.toIndexedSeq</code></td>
<td>将集合转化为一个 IndexedSeq</td>
</tr>
<tr>
<td><code>xs.toStream</code></td>
<td>将集合转化为一个延迟计算的流</td>
</tr>
<tr>
<td><code>xs.toSet</code></td>
<td>将集合转化为一个 Set</td>
</tr>
<tr>
<td><code>xs.toMap</code></td>
<td>将一个（key, value）对的集合转化为一个 Map。 如果当前集合的元素类型不是（key, value）对形式， 则报静态类型错误。</td>
</tr>
<tr>
<td><strong>Copying:</strong></td>
<td></td>
</tr>
<tr>
<td><code>xs copyToBuffer buf</code></td>
<td>拷贝集合中所有元素到缓存 buf</td>
</tr>
<tr>
<td><code>xs copyToArray(arr,s,n)</code></td>
<td>从索引 s 开始，将集合中最多 n 个元素复制到数组 arr。 最后两个参数是可选的。</td>
</tr>
<tr>
<td><strong>Size info:</strong></td>
<td></td>
</tr>
<tr>
<td><code>xs.isEmpty</code></td>
<td>判断集合是否为空</td>
</tr>
<tr>
<td><code>xs.nonEmpty</code></td>
<td>判断集合是否包含元素</td>
</tr>
<tr>
<td><code>xs.size</code></td>
<td>返回集合中元素的个数</td>
</tr>
<tr>
<td><code>xs.hasDefiniteSize</code></td>
<td>如果 xs 具有有限大小，则为真。</td>
</tr>
<tr>
<td><strong>Element Retrieval:</strong></td>
<td></td>
</tr>
<tr>
<td><code>xs.head</code></td>
<td>返回集合中的第一个元素（如果无序，则随机返回）</td>
</tr>
<tr>
<td><code>xs.headOption</code></td>
<td>以 Option 的方式返回集合中的第一个元素， 如果集合为空则返回 None</td>
</tr>
<tr>
<td><code>xs.last</code></td>
<td>返回集合中的最后一个元素（如果无序，则随机返回）</td>
</tr>
<tr>
<td><code>xs.lastOption</code></td>
<td>以 Option 的方式返回集合中的最后一个元素， 如果集合为空则返回 None</td>
</tr>
<tr>
<td><code>xs find p</code></td>
<td>以 Option 的方式返回满足条件 p 的第一个元素， 如果都不满足则返回 None</td>
</tr>
<tr>
<td><strong>Subcollection:</strong></td>
<td></td>
</tr>
<tr>
<td><code>xs.tail</code></td>
<td>除了第一个元素之外的其他元素组成的集合</td>
</tr>
<tr>
<td><code>xs.init</code></td>
<td>除了最后一个元素之外的其他元素组成的集合</td>
</tr>
<tr>
<td><code>xs slice (from, to)</code></td>
<td>返回给定索引范围之内的元素组成的集合 （包含 from 位置的元素但不包含 to 位置的元素）</td>
</tr>
<tr>
<td><code>xs take n</code></td>
<td>返回 xs 的前 n 个元素组成的集合（如果无序，则返回任意 n 个元素）</td>
</tr>
<tr>
<td><code>xs drop n</code></td>
<td>返回 xs 的后 n 个元素组成的集合（如果无序，则返回任意 n 个元素）</td>
</tr>
<tr>
<td><code>xs takeWhile p</code></td>
<td>从第一个元素开始查找满足条件 p 的元素， 直到遇到一个不满足条件的元素，返回所有遍历到的值。</td>
</tr>
<tr>
<td><code>xs dropWhile p</code></td>
<td>从第一个元素开始查找满足条件 p 的元素， 直到遇到一个不满足条件的元素，返回所有未遍历到的值。</td>
</tr>
<tr>
<td><code>xs filter p</code></td>
<td>返回满足条件 p 的所有元素的集合</td>
</tr>
<tr>
<td><code>xs withFilter p</code></td>
<td>集合的非严格的过滤器。后续对 xs 调用方法 map、flatMap 以及 withFilter 都只用作于满足条件 p 的元素，而忽略其他元素</td>
</tr>
<tr>
<td><code>xs filterNot p</code></td>
<td>返回不满足条件 p 的所有元素组成的集合</td>
</tr>
<tr>
<td><strong>Subdivisions:</strong></td>
<td></td>
</tr>
<tr>
<td><code>xs splitAt n</code></td>
<td>在给定位置拆分集合，返回一个集合对 (xs take n, xs drop n)</td>
</tr>
<tr>
<td><code>xs span p</code></td>
<td>根据给定条件拆分集合，返回一个集合对 (xs takeWhile p, xs dropWhile p)。即遍历元素，直到遇到第一个不符合条件的值则结束遍历，将遍历到的值和未遍历到的值分别放入两个集合返回。</td>
</tr>
<tr>
<td><code>xs partition p</code></td>
<td>按照筛选条件对元素进行分组</td>
</tr>
<tr>
<td><code>xs groupBy f</code></td>
<td>根据鉴别器函数 f 将 xs 划分为集合映射</td>
</tr>
<tr>
<td><strong>Element Conditions:</strong></td>
<td></td>
</tr>
<tr>
<td><code>xs forall p</code></td>
<td>判断集合中所有的元素是否都满足条件 p</td>
</tr>
<tr>
<td><code>xs exists p</code></td>
<td>判断集合中是否存在一个元素满足条件 p</td>
</tr>
<tr>
<td><code>xs count p</code></td>
<td>xs 中满足条件 p 的元素的个数</td>
</tr>
<tr>
<td><strong>Folds:</strong></td>
<td></td>
</tr>
<tr>
<td><code>(z /: xs) (op)</code></td>
<td>以 z 为初始值，从左到右对 xs 中的元素执行操作为 op 的归约操作</td>
</tr>
<tr>
<td><code>(xs :\ z) (op)</code></td>
<td>以 z 为初始值，从右到左对 xs 中的元素执行操作为 op 的归约操作</td>
</tr>
<tr>
<td><code>xs.foldLeft(z) (op)</code></td>
<td>同 (z /: xs) (op)</td>
</tr>
<tr>
<td><code>xs.foldRight(z) (op)</code></td>
<td>同 (xs :\ z) (op)</td>
</tr>
<tr>
<td><code>xs reduceLeft op</code></td>
<td>从左到右对 xs 中的元素执行操作为 op 的归约操作</td>
</tr>
<tr>
<td><code>xs reduceRight op</code></td>
<td>从右到左对 xs 中的元素执行操作为 op 的归约操作</td>
</tr>
<tr>
<td><strong>Specific Folds:</strong></td>
<td></td>
</tr>
<tr>
<td><code>xs.sum</code></td>
<td>累计求和</td>
</tr>
<tr>
<td><code>xs.product</code></td>
<td>累计求积</td>
</tr>
<tr>
<td><code>xs.min</code></td>
<td>xs 中的最小值</td>
</tr>
<tr>
<td><code>xs.max</code></td>
<td>xs 中的最大值</td>
</tr>
<tr>
<td><strong>String:</strong></td>
<td></td>
</tr>
<tr>
<td><code>xs addString (b, start, sep, end)</code></td>
<td>向 StringBuilder  b 中添加一个字符串， 该字符串包含 xs 的所有元素。start、seq 和 end  都是可选的，seq 为分隔符，start 为开始符号，end 为结束符号。</td>
</tr>
<tr>
<td><code>xs mkString (start, seq, end)</code></td>
<td>将集合转化为一个字符串。start、seq 和 end  都是可选的，seq 为分隔符，start 为开始符号，end 为结束符号。</td>
</tr>
<tr>
<td><code>xs.stringPrefix</code></td>
<td>返回 xs.toString 字符串开头的集合名称</td>
</tr>
<tr>
<td><strong>Views:</strong></td>
<td></td>
</tr>
<tr>
<td><code>xs.view</code></td>
<td>生成 xs 的视图</td>
</tr>
<tr>
<td><code>xs view (from, to)</code></td>
<td>生成 xs 上指定索引范围内元素的视图</td>
</tr>
</tbody>
</table>
<p>下面为部分方法的使用示例：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>).collect &#123; <span class="keyword">case</span> i <span class="keyword">if</span> i % <span class="number">2</span> == <span class="number">0</span> =&gt; i * <span class="number">10</span> &#125;</span><br><span class="line">res0: <span class="type">List</span>[<span class="type">Int</span>] = <span class="type">List</span>(<span class="number">20</span>, <span class="number">40</span>, <span class="number">60</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>).withFilter(_ % <span class="number">2</span> == <span class="number">0</span>).map(_ * <span class="number">10</span>)</span><br><span class="line">res1: <span class="type">List</span>[<span class="type">Int</span>] = <span class="type">List</span>(<span class="number">20</span>, <span class="number">40</span>, <span class="number">60</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; (<span class="number">10</span> /: <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)) (_ + _)</span><br><span class="line">res2: <span class="type">Int</span> = <span class="number">16</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">-4</span>, <span class="number">5</span>) takeWhile (_ &gt; <span class="number">0</span>)</span><br><span class="line">res3: <span class="type">List</span>[<span class="type">Int</span>] = <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">-4</span>, <span class="number">5</span>) span (_ &gt; <span class="number">0</span>)</span><br><span class="line">res4: (<span class="type">List</span>[<span class="type">Int</span>], <span class="type">List</span>[<span class="type">Int</span>]) = (<span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>),<span class="type">List</span>(<span class="number">-4</span>, <span class="number">5</span>))</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>).mkString(<span class="string">"["</span>,<span class="string">"-"</span>,<span class="string">"]"</span>)</span><br><span class="line">res5: <span class="type">String</span> = [<span class="number">1</span><span class="number">-2</span><span class="number">-3</span>]</span><br></pre></td></tr></table></figure>
<h2 id="四、Trait-Iterable"><a href="#四、Trait-Iterable" class="headerlink" title="四、Trait Iterable"></a>四、Trait Iterable</h2><p>Scala 中所有的集合都直接或者间接实现了 <code>Iterable</code> 特质，<code>Iterable</code> 拓展自 <code>Traversable</code>，并额外定义了部分方法：</p>
<table>
<thead>
<tr>
<th><strong>方法</strong></th>
<th><strong>作用</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Abstract Method:</strong></td>
<td></td>
</tr>
<tr>
<td><code>xs.iterator</code></td>
<td>返回一个迭代器，用于遍历 xs 中的元素， 与 foreach 遍历元素的顺序相同。</td>
</tr>
<tr>
<td><strong>Other Iterators:</strong></td>
<td></td>
</tr>
<tr>
<td><code>xs grouped size</code></td>
<td>返回一个固定大小的迭代器</td>
</tr>
<tr>
<td><code>xs sliding size</code></td>
<td>返回一个固定大小的滑动窗口的迭代器</td>
</tr>
<tr>
<td><strong>Subcollections:</strong></td>
<td></td>
</tr>
<tr>
<td><code>xs takeRigtht n</code></td>
<td>返回 xs 中最后 n 个元素组成的集合（如果无序，则返回任意 n 个元素组成的集合）</td>
</tr>
<tr>
<td><code>xs dropRight n</code></td>
<td>返回 xs 中除了最后 n 个元素外的部分</td>
</tr>
<tr>
<td><strong>Zippers:</strong></td>
<td></td>
</tr>
<tr>
<td><code>xs zip ys</code></td>
<td>返回 xs 和 ys 的对应位置上的元素对组成的集合</td>
</tr>
<tr>
<td><code>xs zipAll (ys, x, y)</code></td>
<td>返回 xs 和 ys 的对应位置上的元素对组成的集合。其中较短的序列通过附加元素 x 或 y 来扩展以匹配较长的序列。</td>
</tr>
<tr>
<td><code>xs.zipWithIndex</code></td>
<td>返回一个由 xs 中元素及其索引所组成的元素对的集合</td>
</tr>
<tr>
<td><strong>Comparison:</strong></td>
<td></td>
</tr>
<tr>
<td><code>xs sameElements ys</code></td>
<td>测试 xs 和 ys 是否包含相同顺序的相同元素</td>
</tr>
</tbody>
</table>
<p>所有方法示例如下：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>).iterator.reduce(_ * _ * <span class="number">10</span>)</span><br><span class="line">res0: <span class="type">Int</span> = <span class="number">600</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="type">List</span>(<span class="string">"a"</span>,<span class="string">"b"</span>,<span class="string">"c"</span>,<span class="string">"d"</span>,<span class="string">"e"</span>) grouped <span class="number">2</span> foreach println</span><br><span class="line"><span class="type">List</span>(a, b)</span><br><span class="line"><span class="type">List</span>(c, d)</span><br><span class="line"><span class="type">List</span>(e)</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="type">List</span>(<span class="string">"a"</span>,<span class="string">"b"</span>,<span class="string">"c"</span>,<span class="string">"d"</span>,<span class="string">"e"</span>) sliding <span class="number">2</span> foreach println</span><br><span class="line"><span class="type">List</span>(a, b)</span><br><span class="line"><span class="type">List</span>(b, c)</span><br><span class="line"><span class="type">List</span>(c, d)</span><br><span class="line"><span class="type">List</span>(d, e)</span><br><span class="line"></span><br><span class="line">scala&gt;  <span class="type">List</span>(<span class="string">"a"</span>,<span class="string">"b"</span>,<span class="string">"c"</span>,<span class="string">"d"</span>,<span class="string">"e"</span>).takeRight(<span class="number">3</span>)</span><br><span class="line">res1: <span class="type">List</span>[<span class="type">String</span>] = <span class="type">List</span>(c, d, e)</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="type">List</span>(<span class="string">"a"</span>,<span class="string">"b"</span>,<span class="string">"c"</span>,<span class="string">"d"</span>,<span class="string">"e"</span>).dropRight(<span class="number">3</span>)</span><br><span class="line">res2: <span class="type">List</span>[<span class="type">String</span>] = <span class="type">List</span>(a, b)</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="type">List</span>(<span class="string">"a"</span>,<span class="string">"b"</span>,<span class="string">"c"</span>).zip(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>))</span><br><span class="line">res3: <span class="type">List</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">List</span>((a,<span class="number">1</span>), (b,<span class="number">2</span>), (c,<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="type">List</span>(<span class="string">"a"</span>,<span class="string">"b"</span>,<span class="string">"c"</span>,<span class="string">"d"</span>).zipAll(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>),<span class="string">""</span>,<span class="number">4</span>)</span><br><span class="line">res4: <span class="type">List</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">List</span>((a,<span class="number">1</span>), (b,<span class="number">2</span>), (c,<span class="number">3</span>), (d,<span class="number">4</span>))</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="type">List</span>(<span class="string">"a"</span>,<span class="string">"b"</span>,<span class="string">"c"</span>).zipAll(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>),<span class="string">"d"</span>,<span class="string">""</span>)</span><br><span class="line">res5: <span class="type">List</span>[(<span class="type">String</span>, <span class="type">Any</span>)] = <span class="type">List</span>((a,<span class="number">1</span>), (b,<span class="number">2</span>), (c,<span class="number">3</span>), (d,<span class="number">4</span>))</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="type">List</span>(<span class="string">"a"</span>, <span class="string">"b"</span>, <span class="string">"c"</span>).zipWithIndex</span><br><span class="line">res6: <span class="type">List</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">List</span>((a,<span class="number">0</span>), (b,<span class="number">1</span>), (c,<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="type">List</span>(<span class="string">"a"</span>, <span class="string">"b"</span>) sameElements <span class="type">List</span>(<span class="string">"a"</span>, <span class="string">"b"</span>)</span><br><span class="line">res7: <span class="type">Boolean</span> = <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="type">List</span>(<span class="string">"a"</span>, <span class="string">"b"</span>) sameElements <span class="type">List</span>(<span class="string">"b"</span>, <span class="string">"a"</span>)</span><br><span class="line">res8: <span class="type">Boolean</span> = <span class="literal">false</span></span><br></pre></td></tr></table></figure>
<h2 id="五、修改集合"><a href="#五、修改集合" class="headerlink" title="五、修改集合"></a>五、修改集合</h2><p>当你想对集合添加或者删除元素，需要根据不同的集合类型选择不同的操作符号：</p>
<table>
<thead>
<tr>
<th>操作符</th>
<th>描述</th>
<th>集合类型</th>
</tr>
</thead>
<tbody>
<tr>
<td>coll(k)<br>即 coll.apply(k)</td>
<td>获取指定位置的元素</td>
<td>Seq, Map</td>
</tr>
<tr>
<td>coll :+ elem<br>elem +: coll</td>
<td>向集合末尾或者集合头增加元素</td>
<td>Seq</td>
</tr>
<tr>
<td>coll + elem<br>coll + (e1, e2, …)</td>
<td>追加元素</td>
<td>Seq, Map</td>
</tr>
<tr>
<td>coll - elem<br>coll - (e1, e2, …)</td>
<td>删除元素</td>
<td>Set, Map, ArrayBuffer</td>
</tr>
<tr>
<td>coll ++ coll2<br>coll2 ++: coll</td>
<td>合并集合</td>
<td>Iterable</td>
</tr>
<tr>
<td>coll – coll2</td>
<td>移除 coll 中包含的 coll2 中的元素</td>
<td>Set, Map, ArrayBuffer</td>
</tr>
<tr>
<td>elem :: lst<br>lst2 :: lst</td>
<td>把指定列表 (lst2) 或者元素 (elem) 添加到列表 (lst) 头部</td>
<td>List</td>
</tr>
<tr>
<td>list ::: list2</td>
<td>合并 List</td>
<td>List</td>
</tr>
<tr>
<td>set \</td>
<td>set2<br>set &amp; set2<br>set &amp;~ set2</td>
<td>并集、交集、差集</td>
<td>Set</td>
</tr>
<tr>
<td>coll += elem<br>coll += (e1, e2, …)<br>coll ++= coll2<br>coll -= elem<br>coll -= (e1, e2, …)<br>coll –= coll2</td>
<td>添加或者删除元素，并将修改后的结果赋值给集合本身</td>
<td>可变集合</td>
</tr>
<tr>
<td>elem +=: coll<br>coll2 ++=: coll</td>
<td>在集合头部追加元素或集合</td>
<td>ArrayBuffer</td>
</tr>
</tbody>
</table>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol>
<li><a href="https://docs.scala-lang.org/overviews/collections/overview.html" target="_blank" rel="noopener">https://docs.scala-lang.org/overviews/collections/overview.html</a></li>
<li><a href="https://docs.scala-lang.org/overviews/collections/trait-traversable.html" target="_blank" rel="noopener">https://docs.scala-lang.org/overviews/collections/trait-traversable.html</a></li>
<li><a href="https://docs.scala-lang.org/overviews/collections/trait-iterable.html" target="_blank" rel="noopener">https://docs.scala-lang.org/overviews/collections/trait-iterable.html</a></li>
</ol>
]]></content>
      <categories>
        <category>notes</category>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>Scala</tag>
      </tags>
  </entry>
  <entry>
    <title>DataFrame和Dataset简介</title>
    <url>/2021/03/17/SparkSQL_Dataset%E5%92%8CDataFrame%E7%AE%80%E4%BB%8B/</url>
    <content><![CDATA[<h2 id="一、Spark-SQL简介"><a href="#一、Spark-SQL简介" class="headerlink" title="一、Spark SQL简介"></a>一、Spark SQL简介</h2><p>Spark SQL 是 Spark 中的一个子模块，主要用于操作结构化数据。它具有以下特点：</p>
<ul>
<li>能够将 SQL 查询与 Spark 程序无缝混合，允许您使用 SQL 或 DataFrame API 对结构化数据进行查询；</li>
<li>支持多种开发语言；</li>
<li>支持多达上百种的外部数据源，包括 Hive，Avro，Parquet，ORC，JSON 和 JDBC 等；</li>
<li>支持 HiveQL 语法以及 Hive SerDes 和 UDF，允许你访问现有的 Hive 仓库；</li>
<li>支持标准的 JDBC 和 ODBC 连接；</li>
<li>支持优化器，列式存储和代码生成等特性；</li>
<li>支持扩展并能保证容错。</li>
</ul>
<div align="center"> <img src="../pictures/sql-hive-arch.png"> </div>

<h2 id="二、DataFrame-amp-DataSet"><a href="#二、DataFrame-amp-DataSet" class="headerlink" title="二、DataFrame &amp; DataSet"></a>二、DataFrame &amp; DataSet</h2><h3 id="2-1-DataFrame"><a href="#2-1-DataFrame" class="headerlink" title="2.1 DataFrame"></a>2.1 DataFrame</h3><p>为了支持结构化数据的处理，Spark SQL 提供了新的数据结构 DataFrame。DataFrame 是一个由具名列组成的数据集。它在概念上等同于关系数据库中的表或 R/Python 语言中的 <code>data frame</code>。 由于 Spark SQL 支持多种语言的开发，所以每种语言都定义了 <code>DataFrame</code> 的抽象，主要如下：</p>
<table>
<thead>
<tr>
<th>语言</th>
<th>主要抽象</th>
</tr>
</thead>
<tbody>
<tr>
<td>Scala</td>
<td>Dataset[T] &amp; DataFrame (Dataset[Row] 的别名)</td>
</tr>
<tr>
<td>Java</td>
<td>Dataset[T]</td>
</tr>
<tr>
<td>Python</td>
<td>DataFrame</td>
</tr>
<tr>
<td>R</td>
<td>DataFrame</td>
</tr>
</tbody>
</table>
<h3 id="2-2-DataFrame-对比-RDDs"><a href="#2-2-DataFrame-对比-RDDs" class="headerlink" title="2.2 DataFrame 对比 RDDs"></a>2.2 DataFrame 对比 RDDs</h3><p>DataFrame 和 RDDs 最主要的区别在于一个面向的是结构化数据，一个面向的是非结构化数据，它们内部的数据结构如下：</p>
<div align="center"> <img src="../pictures/spark-dataFrame+RDDs.png"> </div>

<p>DataFrame 内部的有明确 Scheme 结构，即列名、列字段类型都是已知的，这带来的好处是可以减少数据读取以及更好地优化执行计划，从而保证查询效率。</p>
<p><strong>DataFrame 和 RDDs 应该如何选择？</strong></p>
<ul>
<li>如果你想使用函数式编程而不是 DataFrame API，则使用 RDDs；</li>
<li>如果你的数据是非结构化的 (比如流媒体或者字符流)，则使用 RDDs，</li>
<li>如果你的数据是结构化的 (如 RDBMS 中的数据) 或者半结构化的 (如日志)，出于性能上的考虑，应优先使用 DataFrame。</li>
</ul>
<h3 id="2-3-DataSet"><a href="#2-3-DataSet" class="headerlink" title="2.3 DataSet"></a>2.3 DataSet</h3><p>Dataset 也是分布式的数据集合，在 Spark 1.6 版本被引入，它集成了 RDD 和 DataFrame 的优点，具备强类型的特点，同时支持 Lambda 函数，但只能在 Scala 和 Java 语言中使用。在 Spark 2.0 后，为了方便开发者，Spark 将 DataFrame 和 Dataset 的 API 融合到一起，提供了结构化的 API(Structured API)，即用户可以通过一套标准的 API 就能完成对两者的操作。</p>
<blockquote>
<p>这里注意一下：DataFrame 被标记为 Untyped API，而 DataSet 被标记为 Typed API，后文会对两者做出解释。</p>
</blockquote>
<div align="center"> <img width="600px" src="../pictures/spark-unifed.png"> </div>

<h3 id="2-4-静态类型与运行时类型安全"><a href="#2-4-静态类型与运行时类型安全" class="headerlink" title="2.4 静态类型与运行时类型安全"></a>2.4 静态类型与运行时类型安全</h3><p>静态类型 (Static-typing) 与运行时类型安全 (runtime type-safety) 主要表现如下:</p>
<p>在实际使用中，如果你用的是 Spark SQL 的查询语句，则直到运行时你才会发现有语法错误，而如果你用的是 DataFrame 和 Dataset，则在编译时就可以发现错误 (这节省了开发时间和整体代价)。DataFrame 和 Dataset 主要区别在于：</p>
<p>在 DataFrame 中，当你调用了 API 之外的函数，编译器就会报错，但如果你使用了一个不存在的字段名字，编译器依然无法发现。而 Dataset 的 API 都是用 Lambda 函数和 JVM 类型对象表示的，所有不匹配的类型参数在编译时就会被发现。</p>
<p>以上这些最终都被解释成关于类型安全图谱，对应开发中的语法和分析错误。在图谱中，Dataset 最严格，但对于开发者来说效率最高。</p>
<div align="center"> <img width="600px" src="../pictures/spark-运行安全.png"> </div>

<p>上面的描述可能并没有那么直观，下面的给出一个 IDEA 中代码编译的示例：</p>
<div align="center"> <img src="../pictures/spark-运行时类型安全.png"> </div>

<p>这里一个可能的疑惑是 DataFrame 明明是有确定的 Scheme 结构 (即列名、列字段类型都是已知的)，但是为什么还是无法对列名进行推断和错误判断，这是因为 DataFrame 是 Untyped 的。</p>
<h3 id="2-5-Untyped-amp-Typed"><a href="#2-5-Untyped-amp-Typed" class="headerlink" title="2.5 Untyped &amp; Typed"></a>2.5 Untyped &amp; Typed</h3><p>在上面我们介绍过 DataFrame API 被标记为 <code>Untyped API</code>，而 DataSet API 被标记为 <code>Typed API</code>。DataFrame 的 <code>Untyped</code> 是相对于语言或 API 层面而言，它确实有明确的 Scheme 结构，即列名，列类型都是确定的，但这些信息完全由 Spark 来维护，Spark 只会在运行时检查这些类型和指定类型是否一致。这也就是为什么在 Spark 2.0 之后，官方推荐把 DataFrame 看做是 <code>DatSet[Row]</code>，Row 是 Spark 中定义的一个 <code>trait</code>，其子类中封装了列字段的信息。</p>
<p>相对而言，DataSet 是 <code>Typed</code> 的，即强类型。如下面代码，DataSet 的类型由 Case Class(Scala) 或者 Java Bean(Java) 来明确指定的，在这里即每一行数据代表一个 <code>Person</code>，这些信息由 JVM 来保证正确性，所以字段名错误和类型错误在编译的时候就会被 IDE 所发现。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">name: <span class="type">String</span>, age: <span class="type">Long</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">dataSet</span></span>: <span class="type">Dataset</span>[<span class="type">Person</span>] = spark.read.json(<span class="string">"people.json"</span>).as[<span class="type">Person</span>]</span><br></pre></td></tr></table></figure>
<h2 id="三、DataFrame-amp-DataSet-amp-RDDs-总结"><a href="#三、DataFrame-amp-DataSet-amp-RDDs-总结" class="headerlink" title="三、DataFrame &amp; DataSet  &amp; RDDs 总结"></a>三、DataFrame &amp; DataSet  &amp; RDDs 总结</h2><p>这里对三者做一下简单的总结：</p>
<ul>
<li>RDDs 适合非结构化数据的处理，而 DataFrame &amp; DataSet 更适合结构化数据和半结构化的处理；</li>
<li>DataFrame &amp; DataSet 可以通过统一的 Structured API 进行访问，而 RDDs 则更适合函数式编程的场景；</li>
<li>相比于 DataFrame 而言，DataSet 是强类型的 (Typed)，有着更为严格的静态类型检查；</li>
<li>DataSets、DataFrames、SQL 的底层都依赖了 RDDs API，并对外提供结构化的访问接口。</li>
</ul>
<div align="center"> <img width="600px" src="../pictures/spark-structure-api.png"> </div>



<h2 id="四、Spark-SQL的运行原理"><a href="#四、Spark-SQL的运行原理" class="headerlink" title="四、Spark SQL的运行原理"></a>四、Spark SQL的运行原理</h2><p>DataFrame、DataSet 和 Spark SQL 的实际执行流程都是相同的：</p>
<ol>
<li>进行 DataFrame/Dataset/SQL 编程；</li>
<li>如果是有效的代码，即代码没有编译错误，Spark 会将其转换为一个逻辑计划；</li>
<li>Spark 将此逻辑计划转换为物理计划，同时进行代码优化；</li>
<li>Spark 然后在集群上执行这个物理计划 (基于 RDD 操作) 。</li>
</ol>
<h3 id="4-1-逻辑计划-Logical-Plan"><a href="#4-1-逻辑计划-Logical-Plan" class="headerlink" title="4.1 逻辑计划(Logical Plan)"></a>4.1 逻辑计划(Logical Plan)</h3><p>执行的第一个阶段是将用户代码转换成一个逻辑计划。它首先将用户代码转换成 <code>unresolved logical plan</code>(未解决的逻辑计划)，之所以这个计划是未解决的，是因为尽管您的代码在语法上是正确的，但是它引用的表或列可能不存在。 Spark 使用 <code>analyzer</code>(分析器) 基于 <code>catalog</code>(存储的所有表和 <code>DataFrames</code> 的信息) 进行解析。解析失败则拒绝执行，解析成功则将结果传给 <code>Catalyst</code> 优化器 (<code>Catalyst Optimizer</code>)，优化器是一组规则的集合，用于优化逻辑计划，通过谓词下推等方式进行优化，最终输出优化后的逻辑执行计划。</p>
<div align="center"> <img src="../pictures/spark-Logical-Planning.png"> </div>



<h3 id="4-2-物理计划-Physical-Plan"><a href="#4-2-物理计划-Physical-Plan" class="headerlink" title="4.2 物理计划(Physical Plan)"></a>4.2 物理计划(Physical Plan)</h3><p>得到优化后的逻辑计划后，Spark 就开始了物理计划过程。 它通过生成不同的物理执行策略，并通过成本模型来比较它们，从而选择一个最优的物理计划在集群上面执行的。物理规划的输出结果是一系列的 RDDs 和转换关系 (transformations)。</p>
<div align="center"> <img src="../pictures/spark-Physical-Planning.png"> </div>

<h3 id="4-3-执行"><a href="#4-3-执行" class="headerlink" title="4.3 执行"></a>4.3 执行</h3><p>在选择一个物理计划后，Spark 运行其 RDDs 代码，并在运行时执行进一步的优化，生成本地 Java 字节码，最后将运行结果返回给用户。 </p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol>
<li>Matei Zaharia, Bill Chambers . Spark: The Definitive Guide[M] . 2018-02 </li>
<li><a href="https://spark.apache.org/docs/latest/sql-programming-guide.html" target="_blank" rel="noopener">Spark SQL, DataFrames and Datasets Guide</a></li>
<li><a href="https://www.infoq.cn/article/three-apache-spark-apis-rdds-dataframes-and-datasets" target="_blank" rel="noopener">且谈 Apache Spark 的 API 三剑客：RDD、DataFrame 和 Dataset(译文)</a></li>
<li><a href="https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html" target="_blank" rel="noopener">A Tale of Three Apache Spark APIs: RDDs vs DataFrames and Datasets(原文)</a></li>
</ol>
]]></content>
      <categories>
        <category>notes</category>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark SQL 外部数据源</title>
    <url>/2021/03/17/SparkSQL%E5%A4%96%E9%83%A8%E6%95%B0%E6%8D%AE%E6%BA%90/</url>
    <content><![CDATA[<h2 id="一、简介"><a href="#一、简介" class="headerlink" title="一、简介"></a>一、简介</h2><h3 id="1-1-多数据源支持"><a href="#1-1-多数据源支持" class="headerlink" title="1.1 多数据源支持"></a>1.1 多数据源支持</h3><p>Spark 支持以下六个核心数据源，同时 Spark 社区还提供了多达上百种数据源的读取方式，能够满足绝大部分使用场景。</p>
<ul>
<li>CSV</li>
<li>JSON</li>
<li>Parquet</li>
<li>ORC</li>
<li>JDBC/ODBC connections</li>
<li>Plain-text files</li>
</ul>
<blockquote>
<p>注：以下所有测试文件均可从本仓库的<a href="https://github.com/heibaiying/BigData-Notes/tree/master/resources" target="_blank" rel="noopener">resources</a> 目录进行下载</p>
</blockquote>
<h3 id="1-2-读数据格式"><a href="#1-2-读数据格式" class="headerlink" title="1.2 读数据格式"></a>1.2 读数据格式</h3><p>所有读取 API 遵循以下调用格式：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 格式</span></span><br><span class="line"><span class="type">DataFrameReader</span>.format(...).option(<span class="string">"key"</span>, <span class="string">"value"</span>).schema(...).load()</span><br><span class="line"></span><br><span class="line"><span class="comment">// 示例</span></span><br><span class="line">spark.read.format(<span class="string">"csv"</span>)</span><br><span class="line">.option(<span class="string">"mode"</span>, <span class="string">"FAILFAST"</span>)          <span class="comment">// 读取模式</span></span><br><span class="line">.option(<span class="string">"inferSchema"</span>, <span class="string">"true"</span>)       <span class="comment">// 是否自动推断 schema</span></span><br><span class="line">.option(<span class="string">"path"</span>, <span class="string">"path/to/file(s)"</span>)   <span class="comment">// 文件路径</span></span><br><span class="line">.schema(someSchema)                  <span class="comment">// 使用预定义的 schema      </span></span><br><span class="line">.load()</span><br></pre></td></tr></table></figure>
<p>读取模式有以下三种可选项：</p>
<table>
<thead>
<tr>
<th>读模式</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>permissive</code></td>
<td>当遇到损坏的记录时，将其所有字段设置为 null，并将所有损坏的记录放在名为 _corruption t_record 的字符串列中</td>
</tr>
<tr>
<td><code>dropMalformed</code></td>
<td>删除格式不正确的行</td>
</tr>
<tr>
<td><code>failFast</code></td>
<td>遇到格式不正确的数据时立即失败</td>
</tr>
</tbody>
</table>
<h3 id="1-3-写数据格式"><a href="#1-3-写数据格式" class="headerlink" title="1.3 写数据格式"></a>1.3 写数据格式</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 格式</span></span><br><span class="line"><span class="type">DataFrameWriter</span>.format(...).option(...).partitionBy(...).bucketBy(...).sortBy(...).save()</span><br><span class="line"></span><br><span class="line"><span class="comment">//示例</span></span><br><span class="line">dataframe.write.format(<span class="string">"csv"</span>)</span><br><span class="line">.option(<span class="string">"mode"</span>, <span class="string">"OVERWRITE"</span>)         <span class="comment">//写模式</span></span><br><span class="line">.option(<span class="string">"dateFormat"</span>, <span class="string">"yyyy-MM-dd"</span>)  <span class="comment">//日期格式</span></span><br><span class="line">.option(<span class="string">"path"</span>, <span class="string">"path/to/file(s)"</span>)</span><br><span class="line">.save()</span><br></pre></td></tr></table></figure>
<p>写数据模式有以下四种可选项：</p>
<table>
<thead>
<tr>
<th style="text-align:left">Scala/Java</th>
<th style="text-align:left">描述</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>SaveMode.ErrorIfExists</code></td>
<td style="text-align:left">如果给定的路径已经存在文件，则抛出异常，这是写数据默认的模式</td>
</tr>
<tr>
<td style="text-align:left"><code>SaveMode.Append</code></td>
<td style="text-align:left">数据以追加的方式写入</td>
</tr>
<tr>
<td style="text-align:left"><code>SaveMode.Overwrite</code></td>
<td style="text-align:left">数据以覆盖的方式写入</td>
</tr>
<tr>
<td style="text-align:left"><code>SaveMode.Ignore</code></td>
<td style="text-align:left">如果给定的路径已经存在文件，则不做任何操作</td>
</tr>
</tbody>
</table>
<p><br></p>
<h2 id="二、CSV"><a href="#二、CSV" class="headerlink" title="二、CSV"></a>二、CSV</h2><p>CSV 是一种常见的文本文件格式，其中每一行表示一条记录，记录中的每个字段用逗号分隔。</p>
<h3 id="2-1-读取CSV文件"><a href="#2-1-读取CSV文件" class="headerlink" title="2.1 读取CSV文件"></a>2.1 读取CSV文件</h3><p>自动推断类型读取读取示例：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">spark.read.format(<span class="string">"csv"</span>)</span><br><span class="line">.option(<span class="string">"header"</span>, <span class="string">"false"</span>)        <span class="comment">// 文件中的第一行是否为列的名称</span></span><br><span class="line">.option(<span class="string">"mode"</span>, <span class="string">"FAILFAST"</span>)      <span class="comment">// 是否快速失败</span></span><br><span class="line">.option(<span class="string">"inferSchema"</span>, <span class="string">"true"</span>)   <span class="comment">// 是否自动推断 schema</span></span><br><span class="line">.load(<span class="string">"/usr/file/csv/dept.csv"</span>)</span><br><span class="line">.show()</span><br></pre></td></tr></table></figure>
<p>使用预定义类型：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.&#123;<span class="type">StructField</span>, <span class="type">StructType</span>, <span class="type">StringType</span>,<span class="type">LongType</span>&#125;</span><br><span class="line"><span class="comment">//预定义数据格式</span></span><br><span class="line"><span class="keyword">val</span> myManualSchema = <span class="keyword">new</span> <span class="type">StructType</span>(<span class="type">Array</span>(</span><br><span class="line">    <span class="type">StructField</span>(<span class="string">"deptno"</span>, <span class="type">LongType</span>, nullable = <span class="literal">false</span>),</span><br><span class="line">    <span class="type">StructField</span>(<span class="string">"dname"</span>, <span class="type">StringType</span>,nullable = <span class="literal">true</span>),</span><br><span class="line">    <span class="type">StructField</span>(<span class="string">"loc"</span>, <span class="type">StringType</span>,nullable = <span class="literal">true</span>)</span><br><span class="line">))</span><br><span class="line">spark.read.format(<span class="string">"csv"</span>)</span><br><span class="line">.option(<span class="string">"mode"</span>, <span class="string">"FAILFAST"</span>)</span><br><span class="line">.schema(myManualSchema)</span><br><span class="line">.load(<span class="string">"/usr/file/csv/dept.csv"</span>)</span><br><span class="line">.show()</span><br></pre></td></tr></table></figure>
<h3 id="2-2-写入CSV文件"><a href="#2-2-写入CSV文件" class="headerlink" title="2.2 写入CSV文件"></a>2.2 写入CSV文件</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">df.write.format(<span class="string">"csv"</span>).mode(<span class="string">"overwrite"</span>).save(<span class="string">"/tmp/csv/dept2"</span>)</span><br></pre></td></tr></table></figure>
<p>也可以指定具体的分隔符：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">df.write.format(<span class="string">"csv"</span>).mode(<span class="string">"overwrite"</span>).option(<span class="string">"sep"</span>, <span class="string">"\t"</span>).save(<span class="string">"/tmp/csv/dept2"</span>)</span><br></pre></td></tr></table></figure>
<h3 id="2-3-可选配置"><a href="#2-3-可选配置" class="headerlink" title="2.3 可选配置"></a>2.3 可选配置</h3><p>为节省主文篇幅，所有读写配置项见文末 9.1 小节。</p>
<p><br></p>
<h2 id="三、JSON"><a href="#三、JSON" class="headerlink" title="三、JSON"></a>三、JSON</h2><h3 id="3-1-读取JSON文件"><a href="#3-1-读取JSON文件" class="headerlink" title="3.1 读取JSON文件"></a>3.1 读取JSON文件</h3><figure class="highlight"><table><tr><td class="code"><pre><span class="line">spark.read.format("json").option("mode", "FAILFAST").load("/usr/file/json/dept.json").show(5)</span><br></pre></td></tr></table></figure>
<p>需要注意的是：默认不支持一条数据记录跨越多行 (如下)，可以通过配置 <code>multiLine</code> 为 <code>true</code> 来进行更改，其默认值为 <code>false</code>。</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 默认支持单行</span></span><br><span class="line">&#123;<span class="attr">"DEPTNO"</span>: <span class="number">10</span>,<span class="attr">"DNAME"</span>: <span class="string">"ACCOUNTING"</span>,<span class="attr">"LOC"</span>: <span class="string">"NEW YORK"</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//默认不支持多行</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">"DEPTNO"</span>: <span class="number">10</span>,</span><br><span class="line">  <span class="attr">"DNAME"</span>: <span class="string">"ACCOUNTING"</span>,</span><br><span class="line">  <span class="attr">"LOC"</span>: <span class="string">"NEW YORK"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="3-2-写入JSON文件"><a href="#3-2-写入JSON文件" class="headerlink" title="3.2 写入JSON文件"></a>3.2 写入JSON文件</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">df.write.format(<span class="string">"json"</span>).mode(<span class="string">"overwrite"</span>).save(<span class="string">"/tmp/spark/json/dept"</span>)</span><br></pre></td></tr></table></figure>
<h3 id="3-3-可选配置"><a href="#3-3-可选配置" class="headerlink" title="3.3 可选配置"></a>3.3 可选配置</h3><p>为节省主文篇幅，所有读写配置项见文末 9.2 小节。</p>
<p><br></p>
<h2 id="四、Parquet"><a href="#四、Parquet" class="headerlink" title="四、Parquet"></a>四、Parquet</h2><p> Parquet 是一个开源的面向列的数据存储，它提供了多种存储优化，允许读取单独的列非整个文件，这不仅节省了存储空间而且提升了读取效率，它是 Spark 是默认的文件格式。</p>
<h3 id="4-1-读取Parquet文件"><a href="#4-1-读取Parquet文件" class="headerlink" title="4.1 读取Parquet文件"></a>4.1 读取Parquet文件</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">spark.read.format(<span class="string">"parquet"</span>).load(<span class="string">"/usr/file/parquet/dept.parquet"</span>).show(<span class="number">5</span>)</span><br></pre></td></tr></table></figure>
<h3 id="2-2-写入Parquet文件"><a href="#2-2-写入Parquet文件" class="headerlink" title="2.2 写入Parquet文件"></a>2.2 写入Parquet文件</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">df.write.format(<span class="string">"parquet"</span>).mode(<span class="string">"overwrite"</span>).save(<span class="string">"/tmp/spark/parquet/dept"</span>)</span><br></pre></td></tr></table></figure>
<h3 id="2-3-可选配置-1"><a href="#2-3-可选配置-1" class="headerlink" title="2.3 可选配置"></a>2.3 可选配置</h3><p>Parquet 文件有着自己的存储规则，因此其可选配置项比较少，常用的有如下两个：</p>
<table>
<thead>
<tr>
<th>读写操作</th>
<th>配置项</th>
<th>可选值</th>
<th>默认值</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>Write</td>
<td>compression or codec</td>
<td>None,<br>uncompressed,<br>bzip2,<br>deflate, gzip,<br>lz4, or snappy</td>
<td>None</td>
<td>压缩文件格式</td>
</tr>
<tr>
<td>Read</td>
<td>mergeSchema</td>
<td>true, false</td>
<td>取决于配置项 <code>spark.sql.parquet.mergeSchema</code></td>
<td>当为真时，Parquet 数据源将所有数据文件收集的 Schema 合并在一起，否则将从摘要文件中选择 Schema，如果没有可用的摘要文件，则从随机数据文件中选择 Schema。</td>
</tr>
</tbody>
</table>
<blockquote>
<p>更多可选配置可以参阅官方文档：<a href="https://spark.apache.org/docs/latest/sql-data-sources-parquet.html" target="_blank" rel="noopener">https://spark.apache.org/docs/latest/sql-data-sources-parquet.html</a></p>
</blockquote>
<p><br></p>
<h2 id="五、ORC"><a href="#五、ORC" class="headerlink" title="五、ORC"></a>五、ORC</h2><p>ORC 是一种自描述的、类型感知的列文件格式，它针对大型数据的读写进行了优化，也是大数据中常用的文件格式。</p>
<h3 id="5-1-读取ORC文件"><a href="#5-1-读取ORC文件" class="headerlink" title="5.1 读取ORC文件"></a>5.1 读取ORC文件</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">spark.read.format(<span class="string">"orc"</span>).load(<span class="string">"/usr/file/orc/dept.orc"</span>).show(<span class="number">5</span>)</span><br></pre></td></tr></table></figure>
<h3 id="4-2-写入ORC文件"><a href="#4-2-写入ORC文件" class="headerlink" title="4.2 写入ORC文件"></a>4.2 写入ORC文件</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">csvFile.write.format(<span class="string">"orc"</span>).mode(<span class="string">"overwrite"</span>).save(<span class="string">"/tmp/spark/orc/dept"</span>)</span><br></pre></td></tr></table></figure>
<p><br></p>
<h2 id="六、SQL-Databases"><a href="#六、SQL-Databases" class="headerlink" title="六、SQL Databases"></a>六、SQL Databases</h2><p>Spark 同样支持与传统的关系型数据库进行数据读写。但是 Spark 程序默认是没有提供数据库驱动的，所以在使用前需要将对应的数据库驱动上传到安装目录下的 <code>jars</code> 目录中。下面示例使用的是 Mysql 数据库，使用前需要将对应的 <code>mysql-connector-java-x.x.x.jar</code> 上传到 <code>jars</code> 目录下。</p>
<h3 id="6-1-读取数据"><a href="#6-1-读取数据" class="headerlink" title="6.1 读取数据"></a>6.1 读取数据</h3><p>读取全表数据示例如下，这里的 <code>help_keyword</code> 是 mysql 内置的字典表，只有 <code>help_keyword_id</code> 和 <code>name</code> 两个字段。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">spark.read</span><br><span class="line">.format(<span class="string">"jdbc"</span>)</span><br><span class="line">.option(<span class="string">"driver"</span>, <span class="string">"com.mysql.jdbc.Driver"</span>)            <span class="comment">//驱动</span></span><br><span class="line">.option(<span class="string">"url"</span>, <span class="string">"jdbc:mysql://127.0.0.1:3306/mysql"</span>)   <span class="comment">//数据库地址</span></span><br><span class="line">.option(<span class="string">"dbtable"</span>, <span class="string">"help_keyword"</span>)                    <span class="comment">//表名</span></span><br><span class="line">.option(<span class="string">"user"</span>, <span class="string">"root"</span>).option(<span class="string">"password"</span>,<span class="string">"root"</span>).load().show(<span class="number">10</span>)</span><br></pre></td></tr></table></figure>
<p>从查询结果读取数据：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> pushDownQuery = <span class="string">""</span><span class="string">"(SELECT * FROM help_keyword WHERE help_keyword_id &lt;20) AS help_keywords"</span><span class="string">""</span></span><br><span class="line">spark.read.format(<span class="string">"jdbc"</span>)</span><br><span class="line">.option(<span class="string">"url"</span>, <span class="string">"jdbc:mysql://127.0.0.1:3306/mysql"</span>)</span><br><span class="line">.option(<span class="string">"driver"</span>, <span class="string">"com.mysql.jdbc.Driver"</span>)</span><br><span class="line">.option(<span class="string">"user"</span>, <span class="string">"root"</span>).option(<span class="string">"password"</span>, <span class="string">"root"</span>)</span><br><span class="line">.option(<span class="string">"dbtable"</span>, pushDownQuery)</span><br><span class="line">.load().show()</span><br><span class="line"></span><br><span class="line"><span class="comment">//输出</span></span><br><span class="line">+---------------+-----------+</span><br><span class="line">|help_keyword_id|       name|</span><br><span class="line">+---------------+-----------+</span><br><span class="line">|              <span class="number">0</span>|         &lt;&gt;|</span><br><span class="line">|              <span class="number">1</span>|     <span class="type">ACTION</span>|</span><br><span class="line">|              <span class="number">2</span>|        <span class="type">ADD</span>|</span><br><span class="line">|              <span class="number">3</span>|<span class="type">AES_DECRYPT</span>|</span><br><span class="line">|              <span class="number">4</span>|<span class="type">AES_ENCRYPT</span>|</span><br><span class="line">|              <span class="number">5</span>|      <span class="type">AFTER</span>|</span><br><span class="line">|              <span class="number">6</span>|    <span class="type">AGAINST</span>|</span><br><span class="line">|              <span class="number">7</span>|  <span class="type">AGGREGATE</span>|</span><br><span class="line">|              <span class="number">8</span>|  <span class="type">ALGORITHM</span>|</span><br><span class="line">|              <span class="number">9</span>|        <span class="type">ALL</span>|</span><br><span class="line">|             <span class="number">10</span>|      <span class="type">ALTER</span>|</span><br><span class="line">|             <span class="number">11</span>|    <span class="type">ANALYSE</span>|</span><br><span class="line">|             <span class="number">12</span>|    <span class="type">ANALYZE</span>|</span><br><span class="line">|             <span class="number">13</span>|        <span class="type">AND</span>|</span><br><span class="line">|             <span class="number">14</span>|    <span class="type">ARCHIVE</span>|</span><br><span class="line">|             <span class="number">15</span>|       <span class="type">AREA</span>|</span><br><span class="line">|             <span class="number">16</span>|         <span class="type">AS</span>|</span><br><span class="line">|             <span class="number">17</span>|   <span class="type">ASBINARY</span>|</span><br><span class="line">|             <span class="number">18</span>|        <span class="type">ASC</span>|</span><br><span class="line">|             <span class="number">19</span>|     <span class="type">ASTEXT</span>|</span><br><span class="line">+---------------+-----------+</span><br></pre></td></tr></table></figure>
<p>也可以使用如下的写法进行数据的过滤：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> props = <span class="keyword">new</span> java.util.<span class="type">Properties</span></span><br><span class="line">props.setProperty(<span class="string">"driver"</span>, <span class="string">"com.mysql.jdbc.Driver"</span>)</span><br><span class="line">props.setProperty(<span class="string">"user"</span>, <span class="string">"root"</span>)</span><br><span class="line">props.setProperty(<span class="string">"password"</span>, <span class="string">"root"</span>)</span><br><span class="line"><span class="keyword">val</span> predicates = <span class="type">Array</span>(<span class="string">"help_keyword_id &lt; 10  OR name = 'WHEN'"</span>)   <span class="comment">//指定数据过滤条件</span></span><br><span class="line">spark.read.jdbc(<span class="string">"jdbc:mysql://127.0.0.1:3306/mysql"</span>, <span class="string">"help_keyword"</span>, predicates, props).show() </span><br><span class="line"></span><br><span class="line"><span class="comment">//输出：</span></span><br><span class="line">+---------------+-----------+</span><br><span class="line">|help_keyword_id|       name|</span><br><span class="line">+---------------+-----------+</span><br><span class="line">|              <span class="number">0</span>|         &lt;&gt;|</span><br><span class="line">|              <span class="number">1</span>|     <span class="type">ACTION</span>|</span><br><span class="line">|              <span class="number">2</span>|        <span class="type">ADD</span>|</span><br><span class="line">|              <span class="number">3</span>|<span class="type">AES_DECRYPT</span>|</span><br><span class="line">|              <span class="number">4</span>|<span class="type">AES_ENCRYPT</span>|</span><br><span class="line">|              <span class="number">5</span>|      <span class="type">AFTER</span>|</span><br><span class="line">|              <span class="number">6</span>|    <span class="type">AGAINST</span>|</span><br><span class="line">|              <span class="number">7</span>|  <span class="type">AGGREGATE</span>|</span><br><span class="line">|              <span class="number">8</span>|  <span class="type">ALGORITHM</span>|</span><br><span class="line">|              <span class="number">9</span>|        <span class="type">ALL</span>|</span><br><span class="line">|            <span class="number">604</span>|       <span class="type">WHEN</span>|</span><br><span class="line">+---------------+-----------+</span><br></pre></td></tr></table></figure>
<p>可以使用 <code>numPartitions</code> 指定读取数据的并行度：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">option(<span class="string">"numPartitions"</span>, <span class="number">10</span>)</span><br></pre></td></tr></table></figure>
<p>在这里，除了可以指定分区外，还可以设置上界和下界，任何小于下界的值都会被分配在第一个分区中，任何大于上界的值都会被分配在最后一个分区中。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> colName = <span class="string">"help_keyword_id"</span>   <span class="comment">//用于判断上下界的列</span></span><br><span class="line"><span class="keyword">val</span> lowerBound = <span class="number">300</span>L    <span class="comment">//下界</span></span><br><span class="line"><span class="keyword">val</span> upperBound = <span class="number">500</span>L    <span class="comment">//上界</span></span><br><span class="line"><span class="keyword">val</span> numPartitions = <span class="number">10</span>   <span class="comment">//分区综述</span></span><br><span class="line"><span class="keyword">val</span> jdbcDf = spark.read.jdbc(<span class="string">"jdbc:mysql://127.0.0.1:3306/mysql"</span>,<span class="string">"help_keyword"</span>,</span><br><span class="line">                             colName,lowerBound,upperBound,numPartitions,props)</span><br></pre></td></tr></table></figure>
<p>想要验证分区内容，可以使用 <code>mapPartitionsWithIndex</code> 这个算子，代码如下：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">jdbcDf.rdd.mapPartitionsWithIndex((index, iterator) =&gt; &#123;</span><br><span class="line">    <span class="keyword">val</span> buffer = <span class="keyword">new</span> <span class="type">ListBuffer</span>[<span class="type">String</span>]</span><br><span class="line">    <span class="keyword">while</span> (iterator.hasNext) &#123;</span><br><span class="line">        buffer.append(index + <span class="string">"分区:"</span> + iterator.next())</span><br><span class="line">    &#125;</span><br><span class="line">    buffer.toIterator</span><br><span class="line">&#125;).foreach(println)</span><br></pre></td></tr></table></figure>
<p>执行结果如下：<code>help_keyword</code> 这张表只有 600 条左右的数据，本来数据应该均匀分布在 10 个分区，但是 0 分区里面却有 319 条数据，这是因为设置了下限，所有小于 300 的数据都会被限制在第一个分区，即 0 分区。同理所有大于 500 的数据被分配在 9 分区，即最后一个分区。</p>
<div align="center"> <img src="../pictures/spark-mysql-分区上下限.png"> </div>

<h3 id="6-2-写入数据"><a href="#6-2-写入数据" class="headerlink" title="6.2 写入数据"></a>6.2 写入数据</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> df = spark.read.format(<span class="string">"json"</span>).load(<span class="string">"/usr/file/json/emp.json"</span>)</span><br><span class="line">df.write</span><br><span class="line">.format(<span class="string">"jdbc"</span>)</span><br><span class="line">.option(<span class="string">"url"</span>, <span class="string">"jdbc:mysql://127.0.0.1:3306/mysql"</span>)</span><br><span class="line">.option(<span class="string">"user"</span>, <span class="string">"root"</span>).option(<span class="string">"password"</span>, <span class="string">"root"</span>)</span><br><span class="line">.option(<span class="string">"dbtable"</span>, <span class="string">"emp"</span>)</span><br><span class="line">.save()</span><br></pre></td></tr></table></figure>
<p><br></p>
<h2 id="七、Text"><a href="#七、Text" class="headerlink" title="七、Text"></a>七、Text</h2><p>Text 文件在读写性能方面并没有任何优势，且不能表达明确的数据结构，所以其使用的比较少，读写操作如下：</p>
<h3 id="7-1-读取Text数据"><a href="#7-1-读取Text数据" class="headerlink" title="7.1 读取Text数据"></a>7.1 读取Text数据</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">spark.read.textFile(<span class="string">"/usr/file/txt/dept.txt"</span>).show()</span><br></pre></td></tr></table></figure>
<h3 id="7-2-写入Text数据"><a href="#7-2-写入Text数据" class="headerlink" title="7.2 写入Text数据"></a>7.2 写入Text数据</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">df.write.text(<span class="string">"/tmp/spark/txt/dept"</span>)</span><br></pre></td></tr></table></figure>
<p><br></p>
<h2 id="八、数据读写高级特性"><a href="#八、数据读写高级特性" class="headerlink" title="八、数据读写高级特性"></a>八、数据读写高级特性</h2><h3 id="8-1-并行读"><a href="#8-1-并行读" class="headerlink" title="8.1 并行读"></a>8.1 并行读</h3><p>多个 Executors 不能同时读取同一个文件，但它们可以同时读取不同的文件。这意味着当您从一个包含多个文件的文件夹中读取数据时，这些文件中的每一个都将成为 DataFrame 中的一个分区，并由可用的 Executors 并行读取。</p>
<h3 id="8-2-并行写"><a href="#8-2-并行写" class="headerlink" title="8.2 并行写"></a>8.2 并行写</h3><p>写入的文件或数据的数量取决于写入数据时 DataFrame 拥有的分区数量。默认情况下，每个数据分区写一个文件。</p>
<h3 id="8-3-分区写入"><a href="#8-3-分区写入" class="headerlink" title="8.3 分区写入"></a>8.3 分区写入</h3><p>分区和分桶这两个概念和 Hive 中分区表和分桶表是一致的。都是将数据按照一定规则进行拆分存储。需要注意的是 <code>partitionBy</code> 指定的分区和 RDD 中分区不是一个概念：这里的<strong>分区表现为输出目录的子目录</strong>，数据分别存储在对应的子目录中。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> df = spark.read.format(<span class="string">"json"</span>).load(<span class="string">"/usr/file/json/emp.json"</span>)</span><br><span class="line">df.write.mode(<span class="string">"overwrite"</span>).partitionBy(<span class="string">"deptno"</span>).save(<span class="string">"/tmp/spark/partitions"</span>)</span><br></pre></td></tr></table></figure>
<p>输出结果如下：可以看到输出被按照部门编号分为三个子目录，子目录中才是对应的输出文件。</p>
<div align="center"> <img src="../pictures/spark-分区.png"> </div>

<h3 id="8-3-分桶写入"><a href="#8-3-分桶写入" class="headerlink" title="8.3 分桶写入"></a>8.3 分桶写入</h3><p>分桶写入就是将数据按照指定的列和桶数进行散列，目前分桶写入只支持保存为表，实际上这就是 Hive 的分桶表。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> numberBuckets = <span class="number">10</span></span><br><span class="line"><span class="keyword">val</span> columnToBucketBy = <span class="string">"empno"</span></span><br><span class="line">df.write.format(<span class="string">"parquet"</span>).mode(<span class="string">"overwrite"</span>)</span><br><span class="line">.bucketBy(numberBuckets, columnToBucketBy).saveAsTable(<span class="string">"bucketedFiles"</span>)</span><br></pre></td></tr></table></figure>
<h3 id="8-5-文件大小管理"><a href="#8-5-文件大小管理" class="headerlink" title="8.5 文件大小管理"></a>8.5 文件大小管理</h3><p>如果写入产生小文件数量过多，这时会产生大量的元数据开销。Spark 和 HDFS 一样，都不能很好的处理这个问题，这被称为“small file problem”。同时数据文件也不能过大，否则在查询时会有不必要的性能开销，因此要把文件大小控制在一个合理的范围内。</p>
<p>在上文我们已经介绍过可以通过分区数量来控制生成文件的数量，从而间接控制文件大小。Spark 2.2 引入了一种新的方法，以更自动化的方式控制文件大小，这就是 <code>maxRecordsPerFile</code> 参数，它允许你通过控制写入文件的记录数来控制文件大小。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"> <span class="comment">// Spark 将确保文件最多包含 5000 条记录</span></span><br><span class="line">df.write.option(“maxRecordsPerFile”, <span class="number">5000</span>)</span><br></pre></td></tr></table></figure>
<p><br></p>
<h2 id="九、可选配置附录"><a href="#九、可选配置附录" class="headerlink" title="九、可选配置附录"></a>九、可选配置附录</h2><h3 id="9-1-CSV读写可选配置"><a href="#9-1-CSV读写可选配置" class="headerlink" title="9.1 CSV读写可选配置"></a>9.1 CSV读写可选配置</h3><table>
<thead>
<tr>
<th>读\写操作</th>
<th>配置项</th>
<th>可选值</th>
<th>默认值</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>Both</td>
<td>seq</td>
<td>任意字符</td>
<td><code>,</code>(逗号)</td>
<td>分隔符</td>
</tr>
<tr>
<td>Both</td>
<td>header</td>
<td>true, false</td>
<td>false</td>
<td>文件中的第一行是否为列的名称。</td>
</tr>
<tr>
<td>Read</td>
<td>escape</td>
<td>任意字符</td>
<td>\</td>
<td>转义字符</td>
</tr>
<tr>
<td>Read</td>
<td>inferSchema</td>
<td>true, false</td>
<td>false</td>
<td>是否自动推断列类型</td>
</tr>
<tr>
<td>Read</td>
<td>ignoreLeadingWhiteSpace</td>
<td>true, false</td>
<td>false</td>
<td>是否跳过值前面的空格</td>
</tr>
<tr>
<td>Both</td>
<td>ignoreTrailingWhiteSpace</td>
<td>true, false</td>
<td>false</td>
<td>是否跳过值后面的空格</td>
</tr>
<tr>
<td>Both</td>
<td>nullValue</td>
<td>任意字符</td>
<td>“”</td>
<td>声明文件中哪个字符表示空值</td>
</tr>
<tr>
<td>Both</td>
<td>nanValue</td>
<td>任意字符</td>
<td>NaN</td>
<td>声明哪个值表示 NaN 或者缺省值</td>
</tr>
<tr>
<td>Both</td>
<td>positiveInf</td>
<td>任意字符</td>
<td>Inf</td>
<td>正无穷</td>
</tr>
<tr>
<td>Both</td>
<td>negativeInf</td>
<td>任意字符</td>
<td>-Inf</td>
<td>负无穷</td>
</tr>
<tr>
<td>Both</td>
<td>compression or codec</td>
<td>None,<br>uncompressed,<br>bzip2, deflate,<br>gzip, lz4, or<br>snappy</td>
<td>none</td>
<td>文件压缩格式</td>
</tr>
<tr>
<td>Both</td>
<td>dateFormat</td>
<td>任何能转换为 Java 的 <br>SimpleDataFormat 的字符串</td>
<td>yyyy-MM-dd</td>
<td>日期格式</td>
</tr>
<tr>
<td>Both</td>
<td>timestampFormat</td>
<td>任何能转换为 Java 的 <br>SimpleDataFormat 的字符串</td>
<td>yyyy-MMdd’T’HH:mm:ss.SSSZZ</td>
<td>时间戳格式</td>
</tr>
<tr>
<td>Read</td>
<td>maxColumns</td>
<td>任意整数</td>
<td>20480</td>
<td>声明文件中的最大列数</td>
</tr>
<tr>
<td>Read</td>
<td>maxCharsPerColumn</td>
<td>任意整数</td>
<td>1000000</td>
<td>声明一个列中的最大字符数。</td>
</tr>
<tr>
<td>Read</td>
<td>escapeQuotes</td>
<td>true, false</td>
<td>true</td>
<td>是否应该转义行中的引号。</td>
</tr>
<tr>
<td>Read</td>
<td>maxMalformedLogPerPartition</td>
<td>任意整数</td>
<td>10</td>
<td>声明每个分区中最多允许多少条格式错误的数据，超过这个值后格式错误的数据将不会被读取</td>
</tr>
<tr>
<td>Write</td>
<td>quoteAll</td>
<td>true, false</td>
<td>false</td>
<td>指定是否应该将所有值都括在引号中，而不只是转义具有引号字符的值。</td>
</tr>
<tr>
<td>Read</td>
<td>multiLine</td>
<td>true, false</td>
<td>false</td>
<td>是否允许每条完整记录跨域多行</td>
</tr>
</tbody>
</table>
<h3 id="9-2-JSON读写可选配置"><a href="#9-2-JSON读写可选配置" class="headerlink" title="9.2 JSON读写可选配置"></a>9.2 JSON读写可选配置</h3><table>
<thead>
<tr>
<th>读\写操作</th>
<th>配置项</th>
<th>可选值</th>
<th>默认值</th>
</tr>
</thead>
<tbody>
<tr>
<td>Both</td>
<td>compression or codec</td>
<td>None,<br>uncompressed,<br>bzip2, deflate,<br>gzip, lz4, or<br>snappy</td>
<td>none</td>
</tr>
<tr>
<td>Both</td>
<td>dateFormat</td>
<td>任何能转换为 Java 的 SimpleDataFormat 的字符串</td>
<td>yyyy-MM-dd</td>
</tr>
<tr>
<td>Both</td>
<td>timestampFormat</td>
<td>任何能转换为 Java 的 SimpleDataFormat 的字符串</td>
<td>yyyy-MMdd’T’HH:mm:ss.SSSZZ</td>
</tr>
<tr>
<td>Read</td>
<td>primitiveAsString</td>
<td>true, false</td>
<td>false</td>
</tr>
<tr>
<td>Read</td>
<td>allowComments</td>
<td>true, false</td>
<td>false</td>
</tr>
<tr>
<td>Read</td>
<td>allowUnquotedFieldNames</td>
<td>true, false</td>
<td>false</td>
</tr>
<tr>
<td>Read</td>
<td>allowSingleQuotes</td>
<td>true, false</td>
<td>true</td>
</tr>
<tr>
<td>Read</td>
<td>allowNumericLeadingZeros</td>
<td>true, false</td>
<td>false</td>
</tr>
<tr>
<td>Read</td>
<td>allowBackslashEscapingAnyCharacter</td>
<td>true, false</td>
<td>false</td>
</tr>
<tr>
<td>Read</td>
<td>columnNameOfCorruptRecord</td>
<td>true, false</td>
<td>Value of spark.sql.column&amp;NameOf</td>
</tr>
<tr>
<td>Read</td>
<td>multiLine</td>
<td>true, false</td>
<td>false</td>
</tr>
</tbody>
</table>
<h3 id="9-3-数据库读写可选配置"><a href="#9-3-数据库读写可选配置" class="headerlink" title="9.3 数据库读写可选配置"></a>9.3 数据库读写可选配置</h3><table>
<thead>
<tr>
<th>属性名称</th>
<th>含义</th>
</tr>
</thead>
<tbody>
<tr>
<td>url</td>
<td>数据库地址</td>
</tr>
<tr>
<td>dbtable</td>
<td>表名称</td>
</tr>
<tr>
<td>driver</td>
<td>数据库驱动</td>
</tr>
<tr>
<td>partitionColumn,<br>lowerBound, upperBoun</td>
<td>分区总数，上界，下界</td>
</tr>
<tr>
<td>numPartitions</td>
<td>可用于表读写并行性的最大分区数。如果要写的分区数量超过这个限制，那么可以调用 coalesce(numpartition) 重置分区数。</td>
</tr>
<tr>
<td>fetchsize</td>
<td>每次往返要获取多少行数据。此选项仅适用于读取数据。</td>
</tr>
<tr>
<td>batchsize</td>
<td>每次往返插入多少行数据，这个选项只适用于写入数据。默认值是 1000。</td>
</tr>
<tr>
<td>isolationLevel</td>
<td>事务隔离级别：可以是 NONE，READ_COMMITTED, READ_UNCOMMITTED，REPEATABLE_READ 或 SERIALIZABLE，即标准事务隔离级别。<br>默认值是 READ_UNCOMMITTED。这个选项只适用于数据读取。</td>
</tr>
<tr>
<td>createTableOptions</td>
<td>写入数据时自定义创建表的相关配置</td>
</tr>
<tr>
<td>createTableColumnTypes</td>
<td>写入数据时自定义创建列的列类型</td>
</tr>
</tbody>
</table>
<blockquote>
<p>数据库读写更多配置可以参阅官方文档：<a href="https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html" target="_blank" rel="noopener">https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html</a></p>
</blockquote>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol>
<li>Matei Zaharia, Bill Chambers . Spark: The Definitive Guide[M] . 2018-02 </li>
<li><a href="https://spark.apache.org/docs/latest/sql-data-sources.html" target="_blank" rel="noopener">https://spark.apache.org/docs/latest/sql-data-sources.html</a></li>
</ol>
]]></content>
      <categories>
        <category>notes</category>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark SQL JOIN</title>
    <url>/2021/03/17/SparkSQL%E8%81%94%E7%BB%93%E6%93%8D%E4%BD%9C/</url>
    <content><![CDATA[<h2 id="一、-数据准备"><a href="#一、-数据准备" class="headerlink" title="一、 数据准备"></a>一、 数据准备</h2><p>本文主要介绍 Spark SQL 的多表连接，需要预先准备测试数据。分别创建员工和部门的 Datafame，并注册为临时视图，代码如下：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder().appName(<span class="string">"aggregations"</span>).master(<span class="string">"local[2]"</span>).getOrCreate()</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> empDF = spark.read.json(<span class="string">"/usr/file/json/emp.json"</span>)</span><br><span class="line">empDF.createOrReplaceTempView(<span class="string">"emp"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> deptDF = spark.read.json(<span class="string">"/usr/file/json/dept.json"</span>)</span><br><span class="line">deptDF.createOrReplaceTempView(<span class="string">"dept"</span>)</span><br></pre></td></tr></table></figure>
<p>两表的主要字段如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">emp 员工表</span><br><span class="line"> |-- ENAME: 员工姓名</span><br><span class="line"> |-- DEPTNO: 部门编号</span><br><span class="line"> |-- EMPNO: 员工编号</span><br><span class="line"> |-- HIREDATE: 入职时间</span><br><span class="line"> |-- JOB: 职务</span><br><span class="line"> |-- MGR: 上级编号</span><br><span class="line"> |-- SAL: 薪资</span><br><span class="line"> |-- COMM: 奖金</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">dept 部门表</span><br><span class="line"> |-- DEPTNO: 部门编号</span><br><span class="line"> |-- DNAME:  部门名称</span><br><span class="line"> |-- LOC:    部门所在城市</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注：emp.json，dept.json 可以在本仓库的<a href="https://github.com/heibaiying/BigData-Notes/tree/master/resources" target="_blank" rel="noopener">resources</a> 目录进行下载。</p>
</blockquote>
<h2 id="二、连接类型"><a href="#二、连接类型" class="headerlink" title="二、连接类型"></a>二、连接类型</h2><p>Spark 中支持多种连接类型：</p>
<ul>
<li><strong>Inner Join</strong> : 内连接；</li>
<li><strong>Full Outer Join</strong> :  全外连接；</li>
<li><strong>Left Outer Join</strong> :  左外连接；</li>
<li><strong>Right Outer Join</strong> :  右外连接；</li>
<li><strong>Left Semi Join</strong> :  左半连接；</li>
<li><strong>Left Anti Join</strong> :  左反连接；</li>
<li><strong>Natural Join</strong> :  自然连接；</li>
<li><strong>Cross (or Cartesian) Join</strong> :  交叉 (或笛卡尔) 连接。</li>
</ul>
<p>其中内，外连接，笛卡尔积均与普通关系型数据库中的相同，如下图所示：</p>
<div align="center"> <img src="../pictures/sql-join.jpg"> </div>

<p>这里解释一下左半连接和左反连接，这两个连接等价于关系型数据库中的 <code>IN</code> 和 <code>NOT IN</code> 字句：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- LEFT SEMI JOIN</span></span><br><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> emp <span class="keyword">LEFT</span> <span class="keyword">SEMI</span> <span class="keyword">JOIN</span> dept <span class="keyword">ON</span> emp.deptno = dept.deptno</span><br><span class="line"><span class="comment">-- 等价于如下的 IN 语句</span></span><br><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> emp <span class="keyword">WHERE</span> deptno <span class="keyword">IN</span> (<span class="keyword">SELECT</span> deptno <span class="keyword">FROM</span> dept)</span><br><span class="line"></span><br><span class="line"><span class="comment">-- LEFT ANTI JOIN</span></span><br><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> emp <span class="keyword">LEFT</span> <span class="keyword">ANTI</span> <span class="keyword">JOIN</span> dept <span class="keyword">ON</span> emp.deptno = dept.deptno</span><br><span class="line"><span class="comment">-- 等价于如下的 IN 语句</span></span><br><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> emp <span class="keyword">WHERE</span> deptno <span class="keyword">NOT</span> <span class="keyword">IN</span> (<span class="keyword">SELECT</span> deptno <span class="keyword">FROM</span> dept)</span><br></pre></td></tr></table></figure>
<p>所有连接类型的示例代码如下：</p>
<h3 id="2-1-INNER-JOIN"><a href="#2-1-INNER-JOIN" class="headerlink" title="2.1 INNER JOIN"></a>2.1 INNER JOIN</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 1.定义连接表达式</span></span><br><span class="line"><span class="keyword">val</span> joinExpression = empDF.col(<span class="string">"deptno"</span>) === deptDF.col(<span class="string">"deptno"</span>)</span><br><span class="line"><span class="comment">// 2.连接查询 </span></span><br><span class="line">empDF.join(deptDF,joinExpression).select(<span class="string">"ename"</span>,<span class="string">"dname"</span>).show()</span><br><span class="line"></span><br><span class="line"><span class="comment">// 等价 SQL 如下：</span></span><br><span class="line">spark.sql(<span class="string">"SELECT ename,dname FROM emp JOIN dept ON emp.deptno = dept.deptno"</span>).show()</span><br></pre></td></tr></table></figure>
<h3 id="2-2-FULL-OUTER-JOIN"><a href="#2-2-FULL-OUTER-JOIN" class="headerlink" title="2.2 FULL OUTER JOIN"></a>2.2 FULL OUTER JOIN</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">empDF.join(deptDF, joinExpression, <span class="string">"outer"</span>).show()</span><br><span class="line">spark.sql(<span class="string">"SELECT * FROM emp FULL OUTER JOIN dept ON emp.deptno = dept.deptno"</span>).show()</span><br></pre></td></tr></table></figure>
<h3 id="2-3-LEFT-OUTER-JOIN"><a href="#2-3-LEFT-OUTER-JOIN" class="headerlink" title="2.3 LEFT OUTER JOIN"></a>2.3 LEFT OUTER JOIN</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">empDF.join(deptDF, joinExpression, <span class="string">"left_outer"</span>).show()</span><br><span class="line">spark.sql(<span class="string">"SELECT * FROM emp LEFT OUTER JOIN dept ON emp.deptno = dept.deptno"</span>).show()</span><br></pre></td></tr></table></figure>
<h3 id="2-4-RIGHT-OUTER-JOIN"><a href="#2-4-RIGHT-OUTER-JOIN" class="headerlink" title="2.4 RIGHT OUTER JOIN"></a>2.4 RIGHT OUTER JOIN</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">empDF.join(deptDF, joinExpression, <span class="string">"right_outer"</span>).show()</span><br><span class="line">spark.sql(<span class="string">"SELECT * FROM emp RIGHT OUTER JOIN dept ON emp.deptno = dept.deptno"</span>).show()</span><br></pre></td></tr></table></figure>
<h3 id="2-5-LEFT-SEMI-JOIN"><a href="#2-5-LEFT-SEMI-JOIN" class="headerlink" title="2.5 LEFT SEMI JOIN"></a>2.5 LEFT SEMI JOIN</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">empDF.join(deptDF, joinExpression, <span class="string">"left_semi"</span>).show()</span><br><span class="line">spark.sql(<span class="string">"SELECT * FROM emp LEFT SEMI JOIN dept ON emp.deptno = dept.deptno"</span>).show()</span><br></pre></td></tr></table></figure>
<h3 id="2-6-LEFT-ANTI-JOIN"><a href="#2-6-LEFT-ANTI-JOIN" class="headerlink" title="2.6 LEFT ANTI JOIN"></a>2.6 LEFT ANTI JOIN</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">empDF.join(deptDF, joinExpression, <span class="string">"left_anti"</span>).show()</span><br><span class="line">spark.sql(<span class="string">"SELECT * FROM emp LEFT ANTI JOIN dept ON emp.deptno = dept.deptno"</span>).show()</span><br></pre></td></tr></table></figure>
<h3 id="2-7-CROSS-JOIN"><a href="#2-7-CROSS-JOIN" class="headerlink" title="2.7 CROSS JOIN"></a>2.7 CROSS JOIN</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">empDF.join(deptDF, joinExpression, <span class="string">"cross"</span>).show()</span><br><span class="line">spark.sql(<span class="string">"SELECT * FROM emp CROSS JOIN dept ON emp.deptno = dept.deptno"</span>).show()</span><br></pre></td></tr></table></figure>
<h3 id="2-8-NATURAL-JOIN"><a href="#2-8-NATURAL-JOIN" class="headerlink" title="2.8 NATURAL JOIN"></a>2.8 NATURAL JOIN</h3><p>自然连接是在两张表中寻找那些数据类型和列名都相同的字段，然后自动地将他们连接起来，并返回所有符合条件的结果。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">spark.sql(<span class="string">"SELECT * FROM emp NATURAL JOIN dept"</span>).show()</span><br></pre></td></tr></table></figure>
<p>以下是一个自然连接的查询结果，程序自动推断出使用两张表都存在的 dept 列进行连接，其实际等价于：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">spark.sql("<span class="keyword">SELECT</span> * <span class="keyword">FROM</span> emp <span class="keyword">JOIN</span> dept <span class="keyword">ON</span> emp.deptno = dept.deptno<span class="string">").show()</span></span><br></pre></td></tr></table></figure>
<div align="center"> <img src="../pictures/spark-sql-NATURAL-JOIN.png"> </div>

<p>由于自然连接常常会产生不可预期的结果，所以并不推荐使用。</p>
<h2 id="三、连接的执行"><a href="#三、连接的执行" class="headerlink" title="三、连接的执行"></a>三、连接的执行</h2><p>在对大表与大表之间进行连接操作时，通常都会触发 <code>Shuffle Join</code>，两表的所有分区节点会进行 <code>All-to-All</code> 的通讯，这种查询通常比较昂贵，会对网络 IO 会造成比较大的负担。</p>
<div align="center"> <img width="600px" src="../pictures/spark-Big-table–to–big-table.png"> </div>



<p>而对于大表和小表的连接操作，Spark 会在一定程度上进行优化，如果小表的数据量小于 Worker Node 的内存空间，Spark 会考虑将小表的数据广播到每一个 Worker Node，在每个工作节点内部执行连接计算，这可以降低网络的 IO，但会加大每个 Worker Node 的 CPU 负担。</p>
<div align="center"> <img width="600px" src="../pictures/spark-Big-table–to–small-table.png"> </div>

<p>是否采用广播方式进行 <code>Join</code> 取决于程序内部对小表的判断，如果想明确使用广播方式进行 <code>Join</code>，则可以在 DataFrame API 中使用 <code>broadcast</code> 方法指定需要广播的小表：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">empDF.join(broadcast(deptDF), joinExpression).show()</span><br></pre></td></tr></table></figure>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol>
<li>Matei Zaharia, Bill Chambers . Spark: The Definitive Guide[M] . 2018-02 </li>
</ol>
]]></content>
      <categories>
        <category>notes</category>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title>聚合函数Aggregations</title>
    <url>/2021/03/17/SparkSQL%E5%B8%B8%E7%94%A8%E8%81%9A%E5%90%88%E5%87%BD%E6%95%B0/</url>
    <content><![CDATA[<h2 id="一、简单聚合"><a href="#一、简单聚合" class="headerlink" title="一、简单聚合"></a>一、简单聚合</h2><h3 id="1-1-数据准备"><a href="#1-1-数据准备" class="headerlink" title="1.1 数据准备"></a>1.1 数据准备</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 需要导入 spark sql 内置的函数包</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions._</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder().appName(<span class="string">"aggregations"</span>).master(<span class="string">"local[2]"</span>).getOrCreate()</span><br><span class="line"><span class="keyword">val</span> empDF = spark.read.json(<span class="string">"/usr/file/json/emp.json"</span>)</span><br><span class="line"><span class="comment">// 注册为临时视图，用于后面演示 SQL 查询</span></span><br><span class="line">empDF.createOrReplaceTempView(<span class="string">"emp"</span>)</span><br><span class="line">empDF.show()</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注：emp.json 可以从本仓库的<a href="https://github.com/heibaiying/BigData-Notes/tree/master/resources" target="_blank" rel="noopener">resources</a> 目录下载。</p>
</blockquote>
<h3 id="1-2-count"><a href="#1-2-count" class="headerlink" title="1.2 count"></a>1.2 count</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 计算员工人数</span></span><br><span class="line">empDF.select(count(<span class="string">"ename"</span>)).show()</span><br></pre></td></tr></table></figure>
<h3 id="1-3-countDistinct"><a href="#1-3-countDistinct" class="headerlink" title="1.3 countDistinct"></a>1.3 countDistinct</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 计算姓名不重复的员工人数</span></span><br><span class="line">empDF.select(countDistinct(<span class="string">"deptno"</span>)).show()</span><br></pre></td></tr></table></figure>
<h3 id="1-4-approx-count-distinct"><a href="#1-4-approx-count-distinct" class="headerlink" title="1.4 approx_count_distinct"></a>1.4 approx_count_distinct</h3><p>通常在使用大型数据集时，你可能关注的只是近似值而不是准确值，这时可以使用 approx_count_distinct 函数，并可以使用第二个参数指定最大允许误差。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">empDF.select(approx_count_distinct (<span class="string">"ename"</span>,<span class="number">0.1</span>)).show()</span><br></pre></td></tr></table></figure>
<h3 id="1-5-first-amp-last"><a href="#1-5-first-amp-last" class="headerlink" title="1.5 first &amp; last"></a>1.5 first &amp; last</h3><p>获取 DataFrame 中指定列的第一个值或者最后一个值。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">empDF.select(first(<span class="string">"ename"</span>),last(<span class="string">"job"</span>)).show()</span><br></pre></td></tr></table></figure>
<h3 id="1-6-min-amp-max"><a href="#1-6-min-amp-max" class="headerlink" title="1.6 min &amp; max"></a>1.6 min &amp; max</h3><p>获取 DataFrame 中指定列的最小值或者最大值。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">empDF.select(min(<span class="string">"sal"</span>),max(<span class="string">"sal"</span>)).show()</span><br></pre></td></tr></table></figure>
<h3 id="1-7-sum-amp-sumDistinct"><a href="#1-7-sum-amp-sumDistinct" class="headerlink" title="1.7 sum &amp; sumDistinct"></a>1.7 sum &amp; sumDistinct</h3><p>求和以及求指定列所有不相同的值的和。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">empDF.select(sum(<span class="string">"sal"</span>)).show()</span><br><span class="line">empDF.select(sumDistinct(<span class="string">"sal"</span>)).show()</span><br></pre></td></tr></table></figure>
<h3 id="1-8-avg"><a href="#1-8-avg" class="headerlink" title="1.8 avg"></a>1.8 avg</h3><p>内置的求平均数的函数。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">empDF.select(avg(<span class="string">"sal"</span>)).show()</span><br></pre></td></tr></table></figure>
<h3 id="1-9-数学函数"><a href="#1-9-数学函数" class="headerlink" title="1.9 数学函数"></a>1.9 数学函数</h3><p>Spark SQL 中还支持多种数学聚合函数，用于通常的数学计算，以下是一些常用的例子：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 1.计算总体方差、均方差、总体标准差、样本标准差</span></span><br><span class="line">empDF.select(var_pop(<span class="string">"sal"</span>), var_samp(<span class="string">"sal"</span>), stddev_pop(<span class="string">"sal"</span>), stddev_samp(<span class="string">"sal"</span>)).show()</span><br><span class="line"></span><br><span class="line"><span class="comment">// 2.计算偏度和峰度</span></span><br><span class="line">empDF.select(skewness(<span class="string">"sal"</span>), kurtosis(<span class="string">"sal"</span>)).show()</span><br><span class="line"></span><br><span class="line"><span class="comment">// 3. 计算两列的皮尔逊相关系数、样本协方差、总体协方差。(这里只是演示，员工编号和薪资两列实际上并没有什么关联关系)</span></span><br><span class="line">empDF.select(corr(<span class="string">"empno"</span>, <span class="string">"sal"</span>), covar_samp(<span class="string">"empno"</span>, <span class="string">"sal"</span>),covar_pop(<span class="string">"empno"</span>, <span class="string">"sal"</span>)).show()</span><br></pre></td></tr></table></figure>
<h3 id="1-10-聚合数据到集合"><a href="#1-10-聚合数据到集合" class="headerlink" title="1.10 聚合数据到集合"></a>1.10 聚合数据到集合</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt;  empDF.agg(collect_set(<span class="string">"job"</span>), collect_list(<span class="string">"ename"</span>)).show()</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">+--------------------+--------------------+</span><br><span class="line">|    collect_set(job)| collect_list(ename)|</span><br><span class="line">+--------------------+--------------------+</span><br><span class="line">|[<span class="type">MANAGER</span>, <span class="type">SALESMA</span>...|[<span class="type">SMITH</span>, <span class="type">ALLEN</span>, <span class="type">WA</span>...|</span><br><span class="line">+--------------------+--------------------+</span><br></pre></td></tr></table></figure>
<h2 id="二、分组聚合"><a href="#二、分组聚合" class="headerlink" title="二、分组聚合"></a>二、分组聚合</h2><h3 id="2-1-简单分组"><a href="#2-1-简单分组" class="headerlink" title="2.1 简单分组"></a>2.1 简单分组</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">empDF.groupBy(<span class="string">"deptno"</span>, <span class="string">"job"</span>).count().show()</span><br><span class="line"><span class="comment">//等价 SQL</span></span><br><span class="line">spark.sql(<span class="string">"SELECT deptno, job, count(*) FROM emp GROUP BY deptno, job"</span>).show()</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">+------+---------+-----+</span><br><span class="line">|deptno|      job|count|</span><br><span class="line">+------+---------+-----+</span><br><span class="line">|    <span class="number">10</span>|<span class="type">PRESIDENT</span>|    <span class="number">1</span>|</span><br><span class="line">|    <span class="number">30</span>|    <span class="type">CLERK</span>|    <span class="number">1</span>|</span><br><span class="line">|    <span class="number">10</span>|  <span class="type">MANAGER</span>|    <span class="number">1</span>|</span><br><span class="line">|    <span class="number">30</span>|  <span class="type">MANAGER</span>|    <span class="number">1</span>|</span><br><span class="line">|    <span class="number">20</span>|    <span class="type">CLERK</span>|    <span class="number">2</span>|</span><br><span class="line">|    <span class="number">30</span>| <span class="type">SALESMAN</span>|    <span class="number">4</span>|</span><br><span class="line">|    <span class="number">20</span>|  <span class="type">ANALYST</span>|    <span class="number">2</span>|</span><br><span class="line">|    <span class="number">10</span>|    <span class="type">CLERK</span>|    <span class="number">1</span>|</span><br><span class="line">|    <span class="number">20</span>|  <span class="type">MANAGER</span>|    <span class="number">1</span>|</span><br><span class="line">+------+---------+-----+</span><br></pre></td></tr></table></figure>
<h3 id="2-2-分组聚合"><a href="#2-2-分组聚合" class="headerlink" title="2.2 分组聚合"></a>2.2 分组聚合</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">empDF.groupBy(<span class="string">"deptno"</span>).agg(count(<span class="string">"ename"</span>).alias(<span class="string">"人数"</span>), sum(<span class="string">"sal"</span>).alias(<span class="string">"总工资"</span>)).show()</span><br><span class="line"><span class="comment">// 等价语法</span></span><br><span class="line">empDF.groupBy(<span class="string">"deptno"</span>).agg(<span class="string">"ename"</span>-&gt;<span class="string">"count"</span>,<span class="string">"sal"</span>-&gt;<span class="string">"sum"</span>).show()</span><br><span class="line"><span class="comment">// 等价 SQL</span></span><br><span class="line">spark.sql(<span class="string">"SELECT deptno, count(ename) ,sum(sal) FROM emp GROUP BY deptno"</span>).show()</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">+------+----+------+</span><br><span class="line">|deptno|人数|总工资|</span><br><span class="line">+------+----+------+</span><br><span class="line">|    <span class="number">10</span>|   <span class="number">3</span>|<span class="number">8750.0</span>|</span><br><span class="line">|    <span class="number">30</span>|   <span class="number">6</span>|<span class="number">9400.0</span>|</span><br><span class="line">|    <span class="number">20</span>|   <span class="number">5</span>|<span class="number">9375.0</span>|</span><br><span class="line">+------+----+------+</span><br></pre></td></tr></table></figure>
<h2 id="三、自定义聚合函数"><a href="#三、自定义聚合函数" class="headerlink" title="三、自定义聚合函数"></a>三、自定义聚合函数</h2><p>Scala 提供了两种自定义聚合函数的方法，分别如下：</p>
<ul>
<li>有类型的自定义聚合函数，主要适用于 DataSet；</li>
<li>无类型的自定义聚合函数，主要适用于 DataFrame。</li>
</ul>
<p>以下分别使用两种方式来自定义一个求平均值的聚合函数，这里以计算员工平均工资为例。两种自定义方式分别如下：</p>
<h3 id="3-1-有类型的自定义函数"><a href="#3-1-有类型的自定义函数" class="headerlink" title="3.1 有类型的自定义函数"></a>3.1 有类型的自定义函数</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.expressions.<span class="type">Aggregator</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">Encoder</span>, <span class="type">Encoders</span>, <span class="type">SparkSession</span>, functions&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 1.定义员工类,对于可能存在 null 值的字段需要使用 Option 进行包装</span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Emp</span>(<span class="params">ename: <span class="type">String</span>, comm: scala.<span class="type">Option</span>[<span class="type">Double</span>], deptno: <span class="type">Long</span>, empno: <span class="type">Long</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">               hiredate: <span class="type">String</span>, job: <span class="type">String</span>, mgr: scala.<span class="type">Option</span>[<span class="type">Long</span>], sal: <span class="type">Double</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">//</span> 2.<span class="title">定义聚合操作的中间输出类型</span></span></span><br><span class="line"><span class="class"><span class="title">case</span> <span class="title">class</span> <span class="title">SumAndCount</span>(<span class="params">var sum: <span class="type">Double</span>, var count: <span class="type">Long</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">/*</span> 3.<span class="title">自定义聚合函数</span></span></span><br><span class="line"><span class="class"> <span class="title">*</span> <span class="title">@IN</span>  <span class="title">聚合操作的输入类型</span></span></span><br><span class="line"><span class="class"> <span class="title">*</span> <span class="title">@BUF</span> <span class="title">reduction</span> <span class="title">操作输出值的类型</span></span></span><br><span class="line"><span class="class"> <span class="title">*</span> <span class="title">@OUT</span> <span class="title">聚合操作的输出类型</span></span></span><br><span class="line"><span class="class"> <span class="title">*/</span></span></span><br><span class="line"><span class="class"><span class="title">object</span> <span class="title">MyAverage</span> <span class="keyword">extends</span> <span class="title">Aggregator</span>[<span class="type">Emp</span>, <span class="type">SumAndCount</span>, <span class="type">Double</span>] </span>&#123;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 4.用于聚合操作的的初始零值</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">zero</span></span>: <span class="type">SumAndCount</span> = <span class="type">SumAndCount</span>(<span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 5.同一分区中的 reduce 操作</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">reduce</span></span>(avg: <span class="type">SumAndCount</span>, emp: <span class="type">Emp</span>): <span class="type">SumAndCount</span> = &#123;</span><br><span class="line">        avg.sum += emp.sal</span><br><span class="line">        avg.count += <span class="number">1</span></span><br><span class="line">        avg</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 6.不同分区中的 merge 操作</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(avg1: <span class="type">SumAndCount</span>, avg2: <span class="type">SumAndCount</span>): <span class="type">SumAndCount</span> = &#123;</span><br><span class="line">        avg1.sum += avg2.sum</span><br><span class="line">        avg1.count += avg2.count</span><br><span class="line">        avg1</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 7.定义最终的输出类型</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">finish</span></span>(reduction: <span class="type">SumAndCount</span>): <span class="type">Double</span> = reduction.sum / reduction.count</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 8.中间类型的编码转换</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">bufferEncoder</span></span>: <span class="type">Encoder</span>[<span class="type">SumAndCount</span>] = <span class="type">Encoders</span>.product</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 9.输出类型的编码转换</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">outputEncoder</span></span>: <span class="type">Encoder</span>[<span class="type">Double</span>] = <span class="type">Encoders</span>.scalaDouble</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkSqlApp</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 测试方法</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder().appName(<span class="string">"Spark-SQL"</span>).master(<span class="string">"local[2]"</span>).getOrCreate()</span><br><span class="line">        <span class="keyword">import</span> spark.implicits._</span><br><span class="line">        <span class="keyword">val</span> ds = spark.read.json(<span class="string">"file/emp.json"</span>).as[<span class="type">Emp</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 10.使用内置 avg() 函数和自定义函数分别进行计算，验证自定义函数是否正确</span></span><br><span class="line">        <span class="keyword">val</span> myAvg = ds.select(<span class="type">MyAverage</span>.toColumn.name(<span class="string">"average_sal"</span>)).first()</span><br><span class="line">        <span class="keyword">val</span> avg = ds.select(functions.avg(ds.col(<span class="string">"sal"</span>))).first().get(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        println(<span class="string">"自定义 average 函数 : "</span> + myAvg)</span><br><span class="line">        println(<span class="string">"内置的 average 函数 : "</span> + avg)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>自定义聚合函数需要实现的方法比较多，这里以绘图的方式来演示其执行流程，以及每个方法的作用：</p>
<div align="center"> <img src="../pictures/spark-sql-自定义函数.png"> </div>



<p>关于 <code>zero</code>,<code>reduce</code>,<code>merge</code>,<code>finish</code> 方法的作用在上图都有说明，这里解释一下中间类型和输出类型的编码转换，这个写法比较固定，基本上就是两种情况：</p>
<ul>
<li>自定义类型 Case Class 或者元组就使用 <code>Encoders.product</code> 方法；</li>
<li>基本类型就使用其对应名称的方法，如 <code>scalaByte</code>，<code>scalaFloat</code>，<code>scalaShort</code> 等，示例如下：</li>
</ul>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">bufferEncoder</span></span>: <span class="type">Encoder</span>[<span class="type">SumAndCount</span>] = <span class="type">Encoders</span>.product</span><br><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">outputEncoder</span></span>: <span class="type">Encoder</span>[<span class="type">Double</span>] = <span class="type">Encoders</span>.scalaDouble</span><br></pre></td></tr></table></figure>
<h3 id="3-2-无类型的自定义聚合函数"><a href="#3-2-无类型的自定义聚合函数" class="headerlink" title="3.2 无类型的自定义聚合函数"></a>3.2 无类型的自定义聚合函数</h3><p>理解了有类型的自定义聚合函数后，无类型的定义方式也基本相同，代码如下：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.expressions.&#123;<span class="type">MutableAggregationBuffer</span>, <span class="type">UserDefinedAggregateFunction</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types._</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">Row</span>, <span class="type">SparkSession</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">MyAverage</span> <span class="keyword">extends</span> <span class="title">UserDefinedAggregateFunction</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 1.聚合操作输入参数的类型,字段名称可以自定义</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">inputSchema</span></span>: <span class="type">StructType</span> = <span class="type">StructType</span>(<span class="type">StructField</span>(<span class="string">"MyInputColumn"</span>, <span class="type">LongType</span>) :: <span class="type">Nil</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 2.聚合操作中间值的类型,字段名称可以自定义</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">bufferSchema</span></span>: <span class="type">StructType</span> = &#123;</span><br><span class="line">    <span class="type">StructType</span>(<span class="type">StructField</span>(<span class="string">"sum"</span>, <span class="type">LongType</span>) :: <span class="type">StructField</span>(<span class="string">"MyCount"</span>, <span class="type">LongType</span>) :: <span class="type">Nil</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 3.聚合操作输出参数的类型</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">dataType</span></span>: <span class="type">DataType</span> = <span class="type">DoubleType</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// 4.此函数是否始终在相同输入上返回相同的输出,通常为 true</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">deterministic</span></span>: <span class="type">Boolean</span> = <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// 5.定义零值</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">initialize</span></span>(buffer: <span class="type">MutableAggregationBuffer</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    buffer(<span class="number">0</span>) = <span class="number">0</span>L</span><br><span class="line">    buffer(<span class="number">1</span>) = <span class="number">0</span>L</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 6.同一分区中的 reduce 操作</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">update</span></span>(buffer: <span class="type">MutableAggregationBuffer</span>, input: <span class="type">Row</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span> (!input.isNullAt(<span class="number">0</span>)) &#123;</span><br><span class="line">      buffer(<span class="number">0</span>) = buffer.getLong(<span class="number">0</span>) + input.getLong(<span class="number">0</span>)</span><br><span class="line">      buffer(<span class="number">1</span>) = buffer.getLong(<span class="number">1</span>) + <span class="number">1</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 7.不同分区中的 merge 操作</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(buffer1: <span class="type">MutableAggregationBuffer</span>, buffer2: <span class="type">Row</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    buffer1(<span class="number">0</span>) = buffer1.getLong(<span class="number">0</span>) + buffer2.getLong(<span class="number">0</span>)</span><br><span class="line">    buffer1(<span class="number">1</span>) = buffer1.getLong(<span class="number">1</span>) + buffer2.getLong(<span class="number">1</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 8.计算最终的输出值</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">evaluate</span></span>(buffer: <span class="type">Row</span>): <span class="type">Double</span> = buffer.getLong(<span class="number">0</span>).toDouble / buffer.getLong(<span class="number">1</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkSqlApp</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 测试方法</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder().appName(<span class="string">"Spark-SQL"</span>).master(<span class="string">"local[2]"</span>).getOrCreate()</span><br><span class="line">    <span class="comment">// 9.注册自定义的聚合函数</span></span><br><span class="line">    spark.udf.register(<span class="string">"myAverage"</span>, <span class="type">MyAverage</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> df = spark.read.json(<span class="string">"file/emp.json"</span>)</span><br><span class="line">    df.createOrReplaceTempView(<span class="string">"emp"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 10.使用自定义函数和内置函数分别进行计算</span></span><br><span class="line">    <span class="keyword">val</span> myAvg = spark.sql(<span class="string">"SELECT myAverage(sal) as avg_sal FROM emp"</span>).first()</span><br><span class="line">    <span class="keyword">val</span> avg = spark.sql(<span class="string">"SELECT avg(sal) as avg_sal FROM emp"</span>).first()</span><br><span class="line"></span><br><span class="line">    println(<span class="string">"自定义 average 函数 : "</span> + myAvg)</span><br><span class="line">    println(<span class="string">"内置的 average 函数 : "</span> + avg)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol>
<li>Matei Zaharia, Bill Chambers . Spark: The Definitive Guide[M] . 2018-02 </li>
</ol>
]]></content>
      <categories>
        <category>notes</category>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title>弹性式数据集RDDs</title>
    <url>/2021/03/17/Spark_RDD/</url>
    <content><![CDATA[<h2 id="一、RDD简介"><a href="#一、RDD简介" class="headerlink" title="一、RDD简介"></a>一、RDD简介</h2><p><code>RDD</code> 全称为 Resilient Distributed Datasets，是 Spark 最基本的数据抽象，它是只读的、分区记录的集合，支持并行操作，可以由外部数据集或其他 RDD 转换而来，它具有以下特性：</p>
<ul>
<li>一个 RDD 由一个或者多个分区（Partitions）组成。对于 RDD 来说，每个分区会被一个计算任务所处理，用户可以在创建 RDD 时指定其分区个数，如果没有指定，则默认采用程序所分配到的 CPU 的核心数；</li>
<li>RDD 拥有一个用于计算分区的函数 compute；</li>
<li>RDD 会保存彼此间的依赖关系，RDD 的每次转换都会生成一个新的依赖关系，这种 RDD 之间的依赖关系就像流水线一样。在部分分区数据丢失后，可以通过这种依赖关系重新计算丢失的分区数据，而不是对 RDD 的所有分区进行重新计算；</li>
<li>Key-Value 型的 RDD 还拥有 Partitioner(分区器)，用于决定数据被存储在哪个分区中，目前 Spark 中支持 HashPartitioner(按照哈希分区) 和 RangeParationer(按照范围进行分区)；</li>
<li>一个优先位置列表 (可选)，用于存储每个分区的优先位置 (prefered location)。对于一个 HDFS 文件来说，这个列表保存的就是每个分区所在的块的位置，按照“移动数据不如移动计算“的理念，Spark 在进行任务调度的时候，会尽可能的将计算任务分配到其所要处理数据块的存储位置。</li>
</ul>
<p><code>RDD[T]</code> 抽象类的部分相关代码如下：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 由子类实现以计算给定分区</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute</span></span>(split: <span class="type">Partition</span>, context: <span class="type">TaskContext</span>): <span class="type">Iterator</span>[<span class="type">T</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">// 获取所有分区</span></span><br><span class="line"><span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">getPartitions</span></span>: <span class="type">Array</span>[<span class="type">Partition</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">// 获取所有依赖关系</span></span><br><span class="line"><span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">getDependencies</span></span>: <span class="type">Seq</span>[<span class="type">Dependency</span>[_]] = deps</span><br><span class="line"></span><br><span class="line"><span class="comment">// 获取优先位置列表</span></span><br><span class="line"><span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">getPreferredLocations</span></span>(split: <span class="type">Partition</span>): <span class="type">Seq</span>[<span class="type">String</span>] = <span class="type">Nil</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 分区器 由子类重写以指定它们的分区方式</span></span><br><span class="line"><span class="meta">@transient</span> <span class="keyword">val</span> partitioner: <span class="type">Option</span>[<span class="type">Partitioner</span>] = <span class="type">None</span></span><br></pre></td></tr></table></figure>
<h2 id="二、创建RDD"><a href="#二、创建RDD" class="headerlink" title="二、创建RDD"></a>二、创建RDD</h2><p>RDD 有两种创建方式，分别介绍如下：</p>
<h3 id="2-1-由现有集合创建"><a href="#2-1-由现有集合创建" class="headerlink" title="2.1 由现有集合创建"></a>2.1 由现有集合创建</h3><p>这里使用 <code>spark-shell</code> 进行测试，启动命令如下：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">spark-shell --master local[4]</span><br></pre></td></tr></table></figure>
<p>启动 <code>spark-shell</code> 后，程序会自动创建应用上下文，相当于执行了下面的 Scala 语句：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"Spark shell"</span>).setMaster(<span class="string">"local[4]"</span>)</span><br><span class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br></pre></td></tr></table></figure>
<p>由现有集合创建 RDD，你可以在创建时指定其分区个数，如果没有指定，则采用程序所分配到的 CPU 的核心数：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> data = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>)</span><br><span class="line"><span class="comment">// 由现有集合创建 RDD,默认分区数为程序所分配到的 CPU 的核心数</span></span><br><span class="line"><span class="keyword">val</span> dataRDD = sc.parallelize(data) </span><br><span class="line"><span class="comment">// 查看分区数</span></span><br><span class="line">dataRDD.getNumPartitions</span><br><span class="line"><span class="comment">// 明确指定分区数</span></span><br><span class="line"><span class="keyword">val</span> dataRDD = sc.parallelize(data,<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<p>执行结果如下：</p>
<div align="center"> <img src="../pictures/scala-分区数.png"> </div>

<h3 id="2-2-引用外部存储系统中的数据集"><a href="#2-2-引用外部存储系统中的数据集" class="headerlink" title="2.2 引用外部存储系统中的数据集"></a>2.2 引用外部存储系统中的数据集</h3><p>引用外部存储系统中的数据集，例如本地文件系统，HDFS，HBase 或支持 Hadoop InputFormat 的任何数据源。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> fileRDD = sc.textFile(<span class="string">"/usr/file/emp.txt"</span>)</span><br><span class="line"><span class="comment">// 获取第一行文本</span></span><br><span class="line">fileRDD.take(<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>使用外部存储系统时需要注意以下两点：</p>
<ul>
<li>如果在集群环境下从本地文件系统读取数据，则要求该文件必须在集群中所有机器上都存在，且路径相同；</li>
<li>支持目录路径，支持压缩文件，支持使用通配符。</li>
</ul>
<h3 id="2-3-textFile-amp-wholeTextFiles"><a href="#2-3-textFile-amp-wholeTextFiles" class="headerlink" title="2.3 textFile &amp; wholeTextFiles"></a>2.3 textFile &amp; wholeTextFiles</h3><p>两者都可以用来读取外部文件，但是返回格式是不同的：</p>
<ul>
<li><strong>textFile</strong>：其返回格式是 <code>RDD[String]</code> ，返回的是就是文件内容，RDD 中每一个元素对应一行数据；</li>
<li><strong>wholeTextFiles</strong>：其返回格式是 <code>RDD[(String, String)]</code>，元组中第一个参数是文件路径，第二个参数是文件内容；</li>
<li>两者都提供第二个参数来控制最小分区数；</li>
<li>从 HDFS 上读取文件时，Spark 会为每个块创建一个分区。</li>
</ul>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">textFile</span></span>(path: <span class="type">String</span>,minPartitions: <span class="type">Int</span> = defaultMinPartitions): <span class="type">RDD</span>[<span class="type">String</span>] = withScope &#123;...&#125;</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">wholeTextFiles</span></span>(path: <span class="type">String</span>,minPartitions: <span class="type">Int</span> = defaultMinPartitions): <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">String</span>)]=&#123;..&#125;</span><br></pre></td></tr></table></figure>
<h2 id="三、操作RDD"><a href="#三、操作RDD" class="headerlink" title="三、操作RDD"></a>三、操作RDD</h2><p>RDD 支持两种类型的操作：<em>transformations</em>（转换，从现有数据集创建新数据集）和 <em>actions</em>（在数据集上运行计算后将值返回到驱动程序）。RDD 中的所有转换操作都是惰性的，它们只是记住这些转换操作，但不会立即执行，只有遇到 <em>action</em> 操作后才会真正的进行计算，这类似于函数式编程中的惰性求值。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> list = <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="comment">// map 是一个 transformations 操作，而 foreach 是一个 actions 操作</span></span><br><span class="line">sc.parallelize(list).map(_ * <span class="number">10</span>).foreach(println)</span><br><span class="line"><span class="comment">// 输出： 10 20 30</span></span><br></pre></td></tr></table></figure>
<h2 id="四、缓存RDD"><a href="#四、缓存RDD" class="headerlink" title="四、缓存RDD"></a>四、缓存RDD</h2><h3 id="4-1-缓存级别"><a href="#4-1-缓存级别" class="headerlink" title="4.1 缓存级别"></a>4.1 缓存级别</h3><p>Spark 速度非常快的一个原因是 RDD 支持缓存。成功缓存后，如果之后的操作使用到了该数据集，则直接从缓存中获取。虽然缓存也有丢失的风险，但是由于 RDD 之间的依赖关系，如果某个分区的缓存数据丢失，只需要重新计算该分区即可。</p>
<p>Spark 支持多种缓存级别 ：</p>
<table>
<thead>
<tr>
<th>Storage Level<br>（存储级别）</th>
<th>Meaning（含义）</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>MEMORY_ONLY</code></td>
<td>默认的缓存级别，将 RDD 以反序列化的 Java 对象的形式存储在 JVM 中。如果内存空间不够，则部分分区数据将不再缓存。</td>
</tr>
<tr>
<td><code>MEMORY_AND_DISK</code></td>
<td>将 RDD 以反序列化的 Java 对象的形式存储 JVM 中。如果内存空间不够，将未缓存的分区数据存储到磁盘，在需要使用这些分区时从磁盘读取。</td>
</tr>
<tr>
<td><code>MEMORY_ONLY_SER</code><br></td>
<td>将 RDD 以序列化的 Java 对象的形式进行存储（每个分区为一个 byte 数组）。这种方式比反序列化对象节省存储空间，但在读取时会增加 CPU 的计算负担。仅支持 Java 和 Scala 。</td>
</tr>
<tr>
<td><code>MEMORY_AND_DISK_SER</code><br></td>
<td>类似于 <code>MEMORY_ONLY_SER</code>，但是溢出的分区数据会存储到磁盘，而不是在用到它们时重新计算。仅支持 Java 和 Scala。</td>
</tr>
<tr>
<td><code>DISK_ONLY</code></td>
<td>只在磁盘上缓存 RDD</td>
</tr>
<tr>
<td><code>MEMORY_ONLY_2</code>, <br><code>MEMORY_AND_DISK_2</code>, etc</td>
<td>与上面的对应级别功能相同，但是会为每个分区在集群中的两个节点上建立副本。</td>
</tr>
<tr>
<td><code>OFF_HEAP</code></td>
<td>与 <code>MEMORY_ONLY_SER</code> 类似，但将数据存储在堆外内存中。这需要启用堆外内存。</td>
</tr>
</tbody>
</table>
<blockquote>
<p>启动堆外内存需要配置两个参数：</p>
<ul>
<li><strong>spark.memory.offHeap.enabled</strong> ：是否开启堆外内存，默认值为 false，需要设置为 true；</li>
<li><strong>spark.memory.offHeap.size</strong> : 堆外内存空间的大小，默认值为 0，需要设置为正值。</li>
</ul>
</blockquote>
<h3 id="4-2-使用缓存"><a href="#4-2-使用缓存" class="headerlink" title="4.2 使用缓存"></a>4.2 使用缓存</h3><p>缓存数据的方法有两个：<code>persist</code> 和 <code>cache</code> 。<code>cache</code> 内部调用的也是 <code>persist</code>，它是 <code>persist</code> 的特殊化形式，等价于 <code>persist(StorageLevel.MEMORY_ONLY)</code>。示例如下：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 所有存储级别均定义在 StorageLevel 对象中</span></span><br><span class="line">fileRDD.persist(<span class="type">StorageLevel</span>.<span class="type">MEMORY_AND_DISK</span>)</span><br><span class="line">fileRDD.cache()</span><br></pre></td></tr></table></figure>
<h3 id="4-3-移除缓存"><a href="#4-3-移除缓存" class="headerlink" title="4.3 移除缓存"></a>4.3 移除缓存</h3><p>Spark 会自动监视每个节点上的缓存使用情况，并按照最近最少使用（LRU）的规则删除旧数据分区。当然，你也可以使用 <code>RDD.unpersist()</code> 方法进行手动删除。</p>
<h2 id="五、理解shuffle"><a href="#五、理解shuffle" class="headerlink" title="五、理解shuffle"></a>五、理解shuffle</h2><h3 id="5-1-shuffle介绍"><a href="#5-1-shuffle介绍" class="headerlink" title="5.1 shuffle介绍"></a>5.1 shuffle介绍</h3><p>在 Spark 中，一个任务对应一个分区，通常不会跨分区操作数据。但如果遇到 <code>reduceByKey</code> 等操作，Spark 必须从所有分区读取数据，并查找所有键的所有值，然后汇总在一起以计算每个键的最终结果 ，这称为 <code>Shuffle</code>。</p>
<div align="center"> <img width="600px" src="../pictures/spark-reducebykey.png"> </div>



<h3 id="5-2-Shuffle的影响"><a href="#5-2-Shuffle的影响" class="headerlink" title="5.2 Shuffle的影响"></a>5.2 Shuffle的影响</h3><p>Shuffle 是一项昂贵的操作，因为它通常会跨节点操作数据，这会涉及磁盘 I/O，网络 I/O，和数据序列化。某些 Shuffle 操作还会消耗大量的堆内存，因为它们使用堆内存来临时存储需要网络传输的数据。Shuffle 还会在磁盘上生成大量中间文件，从 Spark 1.3 开始，这些文件将被保留，直到相应的 RDD 不再使用并进行垃圾回收，这样做是为了避免在计算时重复创建 Shuffle 文件。如果应用程序长期保留对这些 RDD 的引用，则垃圾回收可能在很长一段时间后才会发生，这意味着长时间运行的 Spark 作业可能会占用大量磁盘空间，通常可以使用 <code>spark.local.dir</code> 参数来指定这些临时文件的存储目录。</p>
<h3 id="5-3-导致Shuffle的操作"><a href="#5-3-导致Shuffle的操作" class="headerlink" title="5.3 导致Shuffle的操作"></a>5.3 导致Shuffle的操作</h3><p>由于 Shuffle 操作对性能的影响比较大，所以需要特别注意使用，以下操作都会导致 Shuffle：</p>
<ul>
<li><strong>涉及到重新分区操作</strong>： 如 <code>repartition</code> 和 <code>coalesce</code>；</li>
<li><strong>所有涉及到 ByKey 的操作</strong>：如 <code>groupByKey</code> 和 <code>reduceByKey</code>，但 <code>countByKey</code> 除外；</li>
<li><strong>联结操作</strong>：如 <code>cogroup</code> 和 <code>join</code>。</li>
</ul>
<h2 id="五、宽依赖和窄依赖"><a href="#五、宽依赖和窄依赖" class="headerlink" title="五、宽依赖和窄依赖"></a>五、宽依赖和窄依赖</h2><p>RDD 和它的父 RDD(s) 之间的依赖关系分为两种不同的类型：</p>
<ul>
<li><strong>窄依赖 (narrow dependency)</strong>：父 RDDs 的一个分区最多被子 RDDs 一个分区所依赖；</li>
<li><strong>宽依赖 (wide dependency)</strong>：父 RDDs 的一个分区可以被子 RDDs 的多个子分区所依赖。</li>
</ul>
<p>如下图，每一个方框表示一个 RDD，带有颜色的矩形表示分区：</p>
<div align="center"> <img width="600px" src="../pictures/spark-窄依赖和宽依赖.png"> </div>



<p>区分这两种依赖是非常有用的：</p>
<ul>
<li>首先，窄依赖允许在一个集群节点上以流水线的方式（pipeline）对父分区数据进行计算，例如先执行 map 操作，然后执行 filter 操作。而宽依赖则需要计算好所有父分区的数据，然后再在节点之间进行 Shuffle，这与 MapReduce 类似。</li>
<li>窄依赖能够更有效地进行数据恢复，因为只需重新对丢失分区的父分区进行计算，且不同节点之间可以并行计算；而对于宽依赖而言，如果数据丢失，则需要对所有父分区数据进行计算并再次 Shuffle。</li>
</ul>
<h2 id="六、DAG的生成"><a href="#六、DAG的生成" class="headerlink" title="六、DAG的生成"></a>六、DAG的生成</h2><p>RDD(s) 及其之间的依赖关系组成了 DAG(有向无环图)，DAG 定义了这些 RDD(s) 之间的 Lineage(血统) 关系，通过血统关系，如果一个 RDD 的部分或者全部计算结果丢失了，也可以重新进行计算。那么 Spark 是如何根据 DAG 来生成计算任务呢？主要是根据依赖关系的不同将 DAG 划分为不同的计算阶段 (Stage)：</p>
<ul>
<li>对于窄依赖，由于分区的依赖关系是确定的，其转换操作可以在同一个线程执行，所以可以划分到同一个执行阶段；</li>
<li>对于宽依赖，由于 Shuffle 的存在，只能在父 RDD(s) 被 Shuffle 处理完成后，才能开始接下来的计算，因此遇到宽依赖就需要重新划分阶段。</li>
</ul>
<div align="center"> <img width="600px" height="600px" src="../pictures/spark-DAG.png"> </div>





<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol>
<li>张安站 . Spark 技术内幕：深入解析 Spark 内核架构设计与实现原理[M] . 机械工业出版社 . 2015-09-01</li>
<li><a href="https://spark.apache.org/docs/latest/rdd-programming-guide.html#rdd-programming-guide" target="_blank" rel="noopener">RDD Programming Guide</a></li>
<li><a href="http://shiyanjun.cn/archives/744.html" target="_blank" rel="noopener">RDD：基于内存的集群计算容错抽象</a></li>
</ol>
]]></content>
      <categories>
        <category>notes</category>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark Streaming与流处理</title>
    <url>/2021/03/17/Spark_Streaming%E4%B8%8E%E6%B5%81%E5%A4%84%E7%90%86/</url>
    <content><![CDATA[<h2 id="一、流处理"><a href="#一、流处理" class="headerlink" title="一、流处理"></a>一、流处理</h2><h3 id="1-1-静态数据处理"><a href="#1-1-静态数据处理" class="headerlink" title="1.1 静态数据处理"></a>1.1 静态数据处理</h3><p>在流处理之前，数据通常存储在数据库，文件系统或其他形式的存储系统中。应用程序根据需要查询数据或计算数据。这就是传统的静态数据处理架构。Hadoop 采用 HDFS 进行数据存储，采用 MapReduce 进行数据查询或分析，这就是典型的静态数据处理架构。</p>
<div align="center"> <img src="../pictures/01_data_at_rest_infrastructure.png"> </div>



<h3 id="1-2-流处理"><a href="#1-2-流处理" class="headerlink" title="1.2 流处理"></a>1.2 流处理</h3><p>而流处理则是直接对运动中的数据的处理，在接收数据时直接计算数据。</p>
<p>大多数数据都是连续的流：传感器事件，网站上的用户活动，金融交易等等 ，所有这些数据都是随着时间的推移而创建的。</p>
<p>接收和发送数据流并执行应用程序或分析逻辑的系统称为<strong>流处理器</strong>。流处理器的基本职责是确保数据有效流动，同时具备可扩展性和容错能力，Storm 和 Flink 就是其代表性的实现。</p>
<div align="center"> <img src="../pictures/02_stream_processing_infrastructure.png"> </div>



<p>流处理带来了静态数据处理所不具备的众多优点：</p>
<ul>
<li><strong>应用程序立即对数据做出反应</strong>：降低了数据的滞后性，使得数据更具有时效性，更能反映对未来的预期；</li>
<li><strong>流处理可以处理更大的数据量</strong>：直接处理数据流，并且只保留数据中有意义的子集，并将其传送到下一个处理单元，逐级过滤数据，降低需要处理的数据量，从而能够承受更大的数据量；</li>
<li><strong>流处理更贴近现实的数据模型</strong>：在实际的环境中，一切数据都是持续变化的，要想能够通过过去的数据推断未来的趋势，必须保证数据的不断输入和模型的不断修正，典型的就是金融市场、股票市场，流处理能更好的应对这些数据的连续性的特征和及时性的需求；</li>
<li><strong>流处理分散和分离基础设施</strong>：流式处理减少了对大型数据库的需求。相反，每个流处理程序通过流处理框架维护了自己的数据和状态，这使得流处理程序更适合微服务架构。</li>
</ul>
<h2 id="二、Spark-Streaming"><a href="#二、Spark-Streaming" class="headerlink" title="二、Spark Streaming"></a>二、Spark Streaming</h2><h3 id="2-1-简介"><a href="#2-1-简介" class="headerlink" title="2.1 简介"></a>2.1 简介</h3><p>Spark Streaming 是 Spark 的一个子模块，用于快速构建可扩展，高吞吐量，高容错的流处理程序。具有以下特点：</p>
<ul>
<li>通过高级 API 构建应用程序，简单易用；</li>
<li>支持多种语言，如 Java，Scala 和 Python；</li>
<li>良好的容错性，Spark Streaming 支持快速从失败中恢复丢失的操作状态；</li>
<li>能够和 Spark 其他模块无缝集成，将流处理与批处理完美结合；</li>
<li>Spark Streaming 可以从 HDFS，Flume，Kafka，Twitter 和 ZeroMQ 读取数据，也支持自定义数据源。</li>
</ul>
<div align="center"> <img width="600px" src="../pictures/spark-streaming-arch.png"> </div>

<h3 id="2-2-DStream"><a href="#2-2-DStream" class="headerlink" title="2.2 DStream"></a>2.2 DStream</h3><p>Spark Streaming 提供称为离散流 (DStream) 的高级抽象，用于表示连续的数据流。 DStream 可以从来自 Kafka，Flume 和 Kinesis 等数据源的输入数据流创建，也可以由其他 DStream 转化而来。<strong>在内部，DStream 表示为一系列 RDD</strong>。</p>
<div align="center"> <img width="600px" src="../pictures/spark-streaming-flow.png"> </div>



<h3 id="2-3-Spark-amp-Storm-amp-Flink"><a href="#2-3-Spark-amp-Storm-amp-Flink" class="headerlink" title="2.3 Spark &amp; Storm &amp; Flink"></a>2.3 Spark &amp; Storm &amp; Flink</h3><p>storm 和 Flink 都是真正意义上的流计算框架，但 Spark Streaming 只是将数据流进行极小粒度的拆分，拆分为多个批处理，使得其能够得到接近于流处理的效果，但其本质上还是批处理（或微批处理）。</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol>
<li><a href="https://spark.apache.org/docs/latest/streaming-programming-guide.html" target="_blank" rel="noopener">Spark Streaming Programming Guide</a></li>
<li><a href="https://www.ververica.com/what-is-stream-processing" target="_blank" rel="noopener">What is stream processing?</a></li>
</ol>
]]></content>
      <categories>
        <category>notes</category>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark Streaming 基本操作</title>
    <url>/2021/03/17/Spark_Streaming%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/</url>
    <content><![CDATA[<h2 id="一、案例引入"><a href="#一、案例引入" class="headerlink" title="一、案例引入"></a>一、案例引入</h2><p>这里先引入一个基本的案例来演示流的创建：获取指定端口上的数据并进行词频统计。项目依赖和代码实现如下：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-streaming_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.4.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">NetworkWordCount</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/*指定时间间隔为 5s*/</span></span><br><span class="line">    <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"NetworkWordCount"</span>).setMaster(<span class="string">"local[2]"</span>)</span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">/*创建文本输入流,并进行词频统计*/</span></span><br><span class="line">    <span class="keyword">val</span> lines = ssc.socketTextStream(<span class="string">"hadoop001"</span>, <span class="number">9999</span>)</span><br><span class="line">    lines.flatMap(_.split(<span class="string">" "</span>)).map(x =&gt; (x, <span class="number">1</span>)).reduceByKey(_ + _).print()</span><br><span class="line"></span><br><span class="line">    <span class="comment">/*启动服务*/</span></span><br><span class="line">    ssc.start()</span><br><span class="line">    <span class="comment">/*等待服务结束*/</span></span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>使用本地模式启动 Spark 程序，然后使用 <code>nc -lk 9999</code> 打开端口并输入测试数据：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@hadoop001 ~]#  nc -lk 9999</span><br><span class="line">hello world hello spark hive hive hadoop</span><br><span class="line">storm storm flink azkaban</span><br></pre></td></tr></table></figure>
<p>此时控制台输出如下，可以看到已经接收到数据并按行进行了词频统计。</p>
<div align="center"> <img src="../pictures/spark-streaming-word-count-v1.png"> </div><br><br><br><br>下面针对示例代码进行讲解：<br><br>### 3.1 StreamingContext<br><br>Spark Streaming 编程的入口类是 StreamingContext，在创建时候需要指明 <code>sparkConf</code> 和 <code>batchDuration</code>(批次时间)，Spark 流处理本质是将流数据拆分为一个个批次，然后进行微批处理，<code>batchDuration</code> 就是批次拆分的时间间隔。这个时间可以根据业务需求和服务器性能进行指定，如果业务要求低延迟并且服务器性能也允许，则这个时间可以指定得很短。<br><br>这里需要注意的是：示例代码使用的是本地模式，配置为 <code>local[2]</code>，这里不能配置为 <code>local[1]</code>。这是因为对于流数据的处理，Spark 必须有一个独立的 Executor 来接收数据，然后再由其他的 Executors 来处理，所以为了保证数据能够被处理，至少要有 2 个 Executors。这里我们的程序只有一个数据流，在并行读取多个数据流的时候，也需要保证有足够的 Executors 来接收和处理数据。<br><br>### 3.2 数据源<br><br>在示例代码中使用的是 <code>socketTextStream</code> 来创建基于 Socket 的数据流，实际上 Spark 还支持多种数据源，分为以下两类：<br><br>+ <strong>基本数据源</strong>：包括文件系统、Socket 连接等；<br>+ <strong>高级数据源</strong>：包括 Kafka，Flume，Kinesis 等。<br><br>在基本数据源中，Spark 支持监听 HDFS 上指定目录，当有新文件加入时，会获取其文件内容作为输入流。创建方式如下：<br><br><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 对于文本文件，指明监听目录即可</span></span><br><span class="line">streamingContext.textFileStream(dataDirectory)</span><br><span class="line"><span class="comment">// 对于其他文件，需要指明目录，以及键的类型、值的类型、和输入格式</span></span><br><span class="line">streamingContext.fileStream[<span class="type">KeyClass</span>, <span class="type">ValueClass</span>, <span class="type">InputFormatClass</span>](dataDirectory)</span><br></pre></td></tr></table></figure><br><br>被监听的目录可以是具体目录，如 <code>hdfs://host:8040/logs/</code>；也可以使用通配符，如 <code>hdfs://host:8040/logs/2017/*</code>。<br><br>&gt; 关于高级数据源的整合单独整理至：<a href="https://github.com/heibaiying/BigData-Notes/blob/master/notes/Spark_Streaming整合Flume.md" target="_blank" rel="noopener">Spark Streaming 整合 Flume</a> 和 <a href="https://github.com/heibaiying/BigData-Notes/blob/master/notes/Spark_Streaming整合Kafka.md" target="_blank" rel="noopener">Spark Streaming 整合 Kafka</a><br><br>### 3.3 服务的启动与停止<br><br>在示例代码中，使用 <code>streamingContext.start()</code> 代表启动服务，此时还要使用 <code>streamingContext.awaitTermination()</code> 使服务处于等待和可用的状态，直到发生异常或者手动使用 <code>streamingContext.stop()</code> 进行终止。<br><br><br><br>## 二、Transformation<br><br>### 2.1 DStream与RDDs<br><br>DStream 是 Spark Streaming 提供的基本抽象。它表示连续的数据流。在内部，DStream 由一系列连续的 RDD 表示。所以从本质上而言，应用于 DStream 的任何操作都会转换为底层 RDD 上的操作。例如，在示例代码中 flatMap 算子的操作实际上是作用在每个 RDDs 上 (如下图)。因为这个原因，所以 DStream 能够支持 RDD 大部分的<em>transformation</em>算子。<br><br><div align="center"> <img src="../pictures/spark-streaming-dstream-ops.png"> </div>

<h3 id="2-2-updateStateByKey"><a href="#2-2-updateStateByKey" class="headerlink" title="2.2 updateStateByKey"></a>2.2 updateStateByKey</h3><p>除了能够支持 RDD 的算子外，DStream 还有部分独有的<em>transformation</em>算子，这当中比较常用的是 <code>updateStateByKey</code>。文章开头的词频统计程序，只能统计每一次输入文本中单词出现的数量，想要统计所有历史输入中单词出现的数量，可以使用 <code>updateStateByKey</code> 算子。代码如下：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">NetworkWordCountV2</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">     * 本地测试时最好指定 hadoop 用户名,否则会默认使用本地电脑的用户名,</span></span><br><span class="line"><span class="comment">     * 此时在 HDFS 上创建目录时可能会抛出权限不足的异常</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="type">System</span>.setProperty(<span class="string">"HADOOP_USER_NAME"</span>, <span class="string">"root"</span>)</span><br><span class="line">      </span><br><span class="line">    <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"NetworkWordCountV2"</span>).setMaster(<span class="string">"local[2]"</span>)</span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line">    <span class="comment">/*必须要设置检查点*/</span></span><br><span class="line">    ssc.checkpoint(<span class="string">"hdfs://hadoop001:8020/spark-streaming"</span>)</span><br><span class="line">    <span class="keyword">val</span> lines = ssc.socketTextStream(<span class="string">"hadoop001"</span>, <span class="number">9999</span>)</span><br><span class="line">    lines.flatMap(_.split(<span class="string">" "</span>)).map(x =&gt; (x, <span class="number">1</span>))</span><br><span class="line">      .updateStateByKey[<span class="type">Int</span>](updateFunction _)   <span class="comment">//updateStateByKey 算子</span></span><br><span class="line">      .print()</span><br><span class="line"></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * 累计求和</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * @param currentValues 当前的数据</span></span><br><span class="line"><span class="comment">    * @param preValues     之前的数据</span></span><br><span class="line"><span class="comment">    * @return 相加后的数据</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">updateFunction</span></span>(currentValues: <span class="type">Seq</span>[<span class="type">Int</span>], preValues: <span class="type">Option</span>[<span class="type">Int</span>]): <span class="type">Option</span>[<span class="type">Int</span>] = &#123;</span><br><span class="line">    <span class="keyword">val</span> current = currentValues.sum</span><br><span class="line">    <span class="keyword">val</span> pre = preValues.getOrElse(<span class="number">0</span>)</span><br><span class="line">    <span class="type">Some</span>(current + pre)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>使用 <code>updateStateByKey</code> 算子，你必须使用 <code>ssc.checkpoint()</code> 设置检查点，这样当使用 <code>updateStateByKey</code> 算子时，它会去检查点中取出上一次保存的信息，并使用自定义的 <code>updateFunction</code> 函数将上一次的数据和本次数据进行相加，然后返回。</p>
<h3 id="2-3-启动测试"><a href="#2-3-启动测试" class="headerlink" title="2.3 启动测试"></a>2.3 启动测试</h3><p>在监听端口输入如下测试数据：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@hadoop001 ~]#  nc -lk 9999</span><br><span class="line">hello world hello spark hive hive hadoop</span><br><span class="line">storm storm flink azkaban</span><br><span class="line">hello world hello spark hive hive hadoop</span><br><span class="line">storm storm flink azkaban</span><br></pre></td></tr></table></figure>
<p>此时控制台输出如下，所有输入都被进行了词频累计：</p>
<p><div align="center"> <img src="../pictures/spark-streaming-word-count-v2.png"> </div><br>同时在输出日志中还可以看到检查点操作的相关信息：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 保存检查点信息</span></span><br><span class="line">19/05/27 16:21:05 INFO CheckpointWriter: Saving checkpoint for time 1558945265000 ms </span><br><span class="line">to file 'hdfs://hadoop001:8020/spark-streaming/checkpoint-1558945265000'</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 删除已经无用的检查点信息</span></span><br><span class="line">19/05/27 16:21:30 INFO CheckpointWriter: </span><br><span class="line">Deleting hdfs://hadoop001:8020/spark-streaming/checkpoint-1558945265000</span><br></pre></td></tr></table></figure>
<h2 id="三、输出操作"><a href="#三、输出操作" class="headerlink" title="三、输出操作"></a>三、输出操作</h2><h3 id="3-1-输出API"><a href="#3-1-输出API" class="headerlink" title="3.1 输出API"></a>3.1 输出API</h3><p>Spark Streaming 支持以下输出操作：</p>
<table>
<thead>
<tr>
<th style="text-align:left">Output Operation</th>
<th style="text-align:left">Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><strong>print</strong>()</td>
<td style="text-align:left">在运行流应用程序的 driver 节点上打印 DStream 中每个批次的前十个元素。用于开发调试。</td>
</tr>
<tr>
<td style="text-align:left"><strong>saveAsTextFiles</strong>(<em>prefix</em>, [<em>suffix</em>])</td>
<td style="text-align:left">将 DStream 的内容保存为文本文件。每个批处理间隔的文件名基于前缀和后缀生成：“prefix-TIME_IN_MS [.suffix]”。</td>
</tr>
<tr>
<td style="text-align:left"><strong>saveAsObjectFiles</strong>(<em>prefix</em>, [<em>suffix</em>])</td>
<td style="text-align:left">将 DStream 的内容序列化为 Java 对象，并保存到 SequenceFiles。每个批处理间隔的文件名基于前缀和后缀生成：“prefix-TIME_IN_MS [.suffix]”。</td>
</tr>
<tr>
<td style="text-align:left"><strong>saveAsHadoopFiles</strong>(<em>prefix</em>, [<em>suffix</em>])</td>
<td style="text-align:left">将 DStream 的内容保存为 Hadoop 文件。每个批处理间隔的文件名基于前缀和后缀生成：“prefix-TIME_IN_MS [.suffix]”。</td>
</tr>
<tr>
<td style="text-align:left"><strong>foreachRDD</strong>(<em>func</em>)</td>
<td style="text-align:left">最通用的输出方式，它将函数 func 应用于从流生成的每个 RDD。此函数应将每个 RDD 中的数据推送到外部系统，例如将 RDD 保存到文件，或通过网络将其写入数据库。</td>
</tr>
</tbody>
</table>
<p>前面的四个 API 都是直接调用即可，下面主要讲解通用的输出方式 <code>foreachRDD(func)</code>，通过该 API 你可以将数据保存到任何你需要的数据源。</p>
<h3 id="3-1-foreachRDD"><a href="#3-1-foreachRDD" class="headerlink" title="3.1 foreachRDD"></a>3.1 foreachRDD</h3><p>这里我们使用 Redis 作为客户端，对文章开头示例程序进行改变，把每一次词频统计的结果写入到 Redis，并利用 Redis 的 <code>HINCRBY</code> 命令来进行词频统计。这里需要导入 Jedis 依赖：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>redis.clients<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>jedis<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.9.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>具体实现代码如下:</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.dstream.<span class="type">DStream</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"><span class="keyword">import</span> redis.clients.jedis.<span class="type">Jedis</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">NetworkWordCountToRedis</span> </span>&#123;</span><br><span class="line">  </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"NetworkWordCountToRedis"</span>).setMaster(<span class="string">"local[2]"</span>)</span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">/*创建文本输入流,并进行词频统计*/</span></span><br><span class="line">    <span class="keyword">val</span> lines = ssc.socketTextStream(<span class="string">"hadoop001"</span>, <span class="number">9999</span>)</span><br><span class="line">    <span class="keyword">val</span> pairs: <span class="type">DStream</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = lines.flatMap(_.split(<span class="string">" "</span>)).map(x =&gt; (x, <span class="number">1</span>)).reduceByKey(_ + _)</span><br><span class="line">     <span class="comment">/*保存数据到 Redis*/</span></span><br><span class="line">    pairs.foreachRDD &#123; rdd =&gt;</span><br><span class="line">      rdd.foreachPartition &#123; partitionOfRecords =&gt;</span><br><span class="line">        <span class="keyword">var</span> jedis: <span class="type">Jedis</span> = <span class="literal">null</span></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">          jedis = <span class="type">JedisPoolUtil</span>.getConnection</span><br><span class="line">          partitionOfRecords.foreach(record =&gt; jedis.hincrBy(<span class="string">"wordCount"</span>, record._1, record._2))</span><br><span class="line">        &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">          <span class="keyword">case</span> ex: <span class="type">Exception</span> =&gt;</span><br><span class="line">            ex.printStackTrace()</span><br><span class="line">        &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">          <span class="keyword">if</span> (jedis != <span class="literal">null</span>) jedis.close()</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>其中 <code>JedisPoolUtil</code> 的代码如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> redis.clients.jedis.Jedis;</span><br><span class="line"><span class="keyword">import</span> redis.clients.jedis.JedisPool;</span><br><span class="line"><span class="keyword">import</span> redis.clients.jedis.JedisPoolConfig;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">JedisPoolUtil</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/* 声明为 volatile 防止指令重排序 */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">volatile</span> JedisPool jedisPool = <span class="keyword">null</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String HOST = <span class="string">"localhost"</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> PORT = <span class="number">6379</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/* 双重检查锁实现懒汉式单例 */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> Jedis <span class="title">getConnection</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (jedisPool == <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">synchronized</span> (JedisPoolUtil<span class="class">.<span class="keyword">class</span>) </span>&#123;</span><br><span class="line">                <span class="keyword">if</span> (jedisPool == <span class="keyword">null</span>) &#123;</span><br><span class="line">                    JedisPoolConfig config = <span class="keyword">new</span> JedisPoolConfig();</span><br><span class="line">                    config.setMaxTotal(<span class="number">30</span>);</span><br><span class="line">                    config.setMaxIdle(<span class="number">10</span>);</span><br><span class="line">                    jedisPool = <span class="keyword">new</span> JedisPool(config, HOST, PORT);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> jedisPool.getResource();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="3-3-代码说明"><a href="#3-3-代码说明" class="headerlink" title="3.3 代码说明"></a>3.3 代码说明</h3><p>这里将上面保存到 Redis 的代码单独抽取出来，并去除异常判断的部分。精简后的代码如下：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">pairs.foreachRDD &#123; rdd =&gt;</span><br><span class="line">  rdd.foreachPartition &#123; partitionOfRecords =&gt;</span><br><span class="line">    <span class="keyword">val</span> jedis = <span class="type">JedisPoolUtil</span>.getConnection</span><br><span class="line">    partitionOfRecords.foreach(record =&gt; jedis.hincrBy(<span class="string">"wordCount"</span>, record._1, record._2))</span><br><span class="line">    jedis.close()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里可以看到一共使用了三次循环，分别是循环 RDD，循环分区，循环每条记录，上面我们的代码是在循环分区的时候获取连接，也就是为每一个分区获取一个连接。但是这里大家可能会有疑问：为什么不在循环 RDD 的时候，为每一个 RDD 获取一个连接，这样所需要的连接数会更少。实际上这是不可行的，如果按照这种情况进行改写，如下：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">pairs.foreachRDD &#123; rdd =&gt;</span><br><span class="line">    <span class="keyword">val</span> jedis = <span class="type">JedisPoolUtil</span>.getConnection</span><br><span class="line">    rdd.foreachPartition &#123; partitionOfRecords =&gt;</span><br><span class="line">        partitionOfRecords.foreach(record =&gt; jedis.hincrBy(<span class="string">"wordCount"</span>, record._1, record._2))</span><br><span class="line">    &#125;</span><br><span class="line">    jedis.close()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>此时在执行时候就会抛出 <code>Caused by: java.io.NotSerializableException: redis.clients.jedis.Jedis</code>，这是因为在实际计算时，Spark 会将对 RDD 操作分解为多个 Task，Task 运行在具体的 Worker Node 上。在执行之前，Spark 会对任务进行闭包，之后闭包被序列化并发送给每个 Executor，而 <code>Jedis</code> 显然是不能被序列化的，所以会抛出异常。</p>
<p>第二个需要注意的是 ConnectionPool 最好是一个静态，惰性初始化连接池 。这是因为 Spark 的转换操作本身就是惰性的，且没有数据流时不会触发写出操作，所以出于性能考虑，连接池应该是惰性的，因此上面 <code>JedisPool</code> 在初始化时采用了懒汉式单例进行惰性初始化。</p>
<h3 id="3-4-启动测试"><a href="#3-4-启动测试" class="headerlink" title="3.4 启动测试"></a>3.4 启动测试</h3><p>在监听端口输入如下测试数据：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@hadoop001 ~]#  nc -lk 9999</span><br><span class="line">hello world hello spark hive hive hadoop</span><br><span class="line">storm storm flink azkaban</span><br><span class="line">hello world hello spark hive hive hadoop</span><br><span class="line">storm storm flink azkaban</span><br></pre></td></tr></table></figure>
<p>使用 Redis Manager 查看写入结果 (如下图),可以看到与使用 <code>updateStateByKey</code> 算子得到的计算结果相同。</p>
<p><div align="center"> <img src="../pictures/spark-streaming-word-count-v3.png"> </div><br><br></p>
<blockquote>
<p>本片文章所有源码见本仓库：<a href="https://github.com/heibaiying/BigData-Notes/tree/master/code/spark/spark-streaming-basis" target="_blank" rel="noopener">spark-streaming-basis</a></p>
</blockquote>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>Spark 官方文档：<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html" target="_blank" rel="noopener">http://spark.apache.org/docs/latest/streaming-programming-guide.html</a></p>
]]></content>
      <categories>
        <category>notes</category>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark Streaming 整合 Kafka</title>
    <url>/2021/03/17/Spark_Streaming%E6%95%B4%E5%90%88Kafka/</url>
    <content><![CDATA[<h2 id="一、版本说明"><a href="#一、版本说明" class="headerlink" title="一、版本说明"></a>一、版本说明</h2><p>Spark 针对 Kafka 的不同版本，提供了两套整合方案：<code>spark-streaming-kafka-0-8</code> 和 <code>spark-streaming-kafka-0-10</code>，其主要区别如下：</p>
<table>
<thead>
<tr>
<th style="text-align:left"></th>
<th style="text-align:left"><a href="https://spark.apache.org/docs/latest/streaming-kafka-0-8-integration.html" target="_blank" rel="noopener">spark-streaming-kafka-0-8</a></th>
<th style="text-align:left"><a href="https://spark.apache.org/docs/latest/streaming-kafka-0-10-integration.html" target="_blank" rel="noopener">spark-streaming-kafka-0-10</a></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">Kafka 版本</td>
<td style="text-align:left">0.8.2.1 or higher</td>
<td style="text-align:left">0.10.0 or higher</td>
</tr>
<tr>
<td style="text-align:left">AP 状态</td>
<td style="text-align:left">Deprecated<br>从 Spark 2.3.0 版本开始，Kafka 0.8 支持已被弃用</td>
<td style="text-align:left">Stable(稳定版)</td>
</tr>
<tr>
<td style="text-align:left">语言支持</td>
<td style="text-align:left">Scala, Java, Python</td>
<td style="text-align:left">Scala, Java</td>
</tr>
<tr>
<td style="text-align:left">Receiver DStream</td>
<td style="text-align:left">Yes</td>
<td style="text-align:left">No</td>
</tr>
<tr>
<td style="text-align:left">Direct DStream</td>
<td style="text-align:left">Yes</td>
<td style="text-align:left">Yes</td>
</tr>
<tr>
<td style="text-align:left">SSL / TLS Support</td>
<td style="text-align:left">No</td>
<td style="text-align:left">Yes</td>
</tr>
<tr>
<td style="text-align:left">Offset Commit API(偏移量提交)</td>
<td style="text-align:left">No</td>
<td style="text-align:left">Yes</td>
</tr>
<tr>
<td style="text-align:left">Dynamic Topic Subscription<br>(动态主题订阅)</td>
<td style="text-align:left">No</td>
<td style="text-align:left">Yes</td>
</tr>
</tbody>
</table>
<p>本文使用的 Kafka 版本为 <code>kafka_2.12-2.2.0</code>，故采用第二种方式进行整合。</p>
<h2 id="二、项目依赖"><a href="#二、项目依赖" class="headerlink" title="二、项目依赖"></a>二、项目依赖</h2><p>项目采用 Maven 进行构建，主要依赖如下：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">properties</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">scala.version</span>&gt;</span>2.12<span class="tag">&lt;/<span class="name">scala.version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">properties</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- Spark Streaming--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-streaming_$&#123;scala.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;spark.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- Spark Streaming 整合 Kafka 依赖--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-streaming-kafka-0-10_$&#123;scala.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.4.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>完整源码见本仓库：<a href="https://github.com/heibaiying/BigData-Notes/tree/master/code/spark/spark-streaming-kafka" target="_blank" rel="noopener">spark-streaming-kafka</a></p>
</blockquote>
<h2 id="三、整合Kafka"><a href="#三、整合Kafka" class="headerlink" title="三、整合Kafka"></a>三、整合Kafka</h2><p>通过调用 <code>KafkaUtils</code> 对象的 <code>createDirectStream</code> 方法来创建输入流，完整代码如下：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.<span class="type">StringDeserializer</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka010.<span class="type">ConsumerStrategies</span>.<span class="type">Subscribe</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka010.<span class="type">LocationStrategies</span>.<span class="type">PreferConsistent</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka010._</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * spark streaming 整合 kafka</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">KafkaDirectStream</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"KafkaDirectStream"</span>).setMaster(<span class="string">"local[2]"</span>)</span><br><span class="line">    <span class="keyword">val</span> streamingContext = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> kafkaParams = <span class="type">Map</span>[<span class="type">String</span>, <span class="type">Object</span>](</span><br><span class="line">      <span class="comment">/*</span></span><br><span class="line"><span class="comment">       * 指定 broker 的地址清单，清单里不需要包含所有的 broker 地址，生产者会从给定的 broker 里查找其他 broker 的信息。</span></span><br><span class="line"><span class="comment">       * 不过建议至少提供两个 broker 的信息作为容错。</span></span><br><span class="line"><span class="comment">       */</span></span><br><span class="line">      <span class="string">"bootstrap.servers"</span> -&gt; <span class="string">"hadoop001:9092"</span>,</span><br><span class="line">      <span class="comment">/*键的序列化器*/</span></span><br><span class="line">      <span class="string">"key.deserializer"</span> -&gt; classOf[<span class="type">StringDeserializer</span>],</span><br><span class="line">      <span class="comment">/*值的序列化器*/</span></span><br><span class="line">      <span class="string">"value.deserializer"</span> -&gt; classOf[<span class="type">StringDeserializer</span>],</span><br><span class="line">      <span class="comment">/*消费者所在分组的 ID*/</span></span><br><span class="line">      <span class="string">"group.id"</span> -&gt; <span class="string">"spark-streaming-group"</span>,</span><br><span class="line">      <span class="comment">/*</span></span><br><span class="line"><span class="comment">       * 该属性指定了消费者在读取一个没有偏移量的分区或者偏移量无效的情况下该作何处理:</span></span><br><span class="line"><span class="comment">       * latest: 在偏移量无效的情况下，消费者将从最新的记录开始读取数据（在消费者启动之后生成的记录）</span></span><br><span class="line"><span class="comment">       * earliest: 在偏移量无效的情况下，消费者将从起始位置读取分区的记录</span></span><br><span class="line"><span class="comment">       */</span></span><br><span class="line">      <span class="string">"auto.offset.reset"</span> -&gt; <span class="string">"latest"</span>,</span><br><span class="line">      <span class="comment">/*是否自动提交*/</span></span><br><span class="line">      <span class="string">"enable.auto.commit"</span> -&gt; (<span class="literal">true</span>: java.lang.<span class="type">Boolean</span>)</span><br><span class="line">    )</span><br><span class="line">    </span><br><span class="line">    <span class="comment">/*可以同时订阅多个主题*/</span></span><br><span class="line">    <span class="keyword">val</span> topics = <span class="type">Array</span>(<span class="string">"spark-streaming-topic"</span>)</span><br><span class="line">    <span class="keyword">val</span> stream = <span class="type">KafkaUtils</span>.createDirectStream[<span class="type">String</span>, <span class="type">String</span>](</span><br><span class="line">      streamingContext,</span><br><span class="line">      <span class="comment">/*位置策略*/</span></span><br><span class="line">      <span class="type">PreferConsistent</span>,</span><br><span class="line">      <span class="comment">/*订阅主题*/</span></span><br><span class="line">      <span class="type">Subscribe</span>[<span class="type">String</span>, <span class="type">String</span>](topics, kafkaParams)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">/*打印输入流*/</span></span><br><span class="line">    stream.map(record =&gt; (record.key, record.value)).print()</span><br><span class="line"></span><br><span class="line">    streamingContext.start()</span><br><span class="line">    streamingContext.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="3-1-ConsumerRecord"><a href="#3-1-ConsumerRecord" class="headerlink" title="3.1 ConsumerRecord"></a>3.1 ConsumerRecord</h3><p>这里获得的输入流中每一个 Record 实际上是 <code>ConsumerRecord&lt;K, V&gt;</code> 的实例，其包含了 Record 的所有可用信息，源码如下：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">public <span class="class"><span class="keyword">class</span> <span class="title">ConsumerRecord&lt;K</span>, <span class="title">V&gt;</span> </span>&#123;</span><br><span class="line">    </span><br><span class="line">    public static <span class="keyword">final</span> long <span class="type">NO_TIMESTAMP</span> = <span class="type">RecordBatch</span>.<span class="type">NO_TIMESTAMP</span>;</span><br><span class="line">    public static <span class="keyword">final</span> int <span class="type">NULL_SIZE</span> = <span class="number">-1</span>;</span><br><span class="line">    public static <span class="keyword">final</span> int <span class="type">NULL_CHECKSUM</span> = <span class="number">-1</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">/*主题名称*/</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="type">String</span> topic;</span><br><span class="line">    <span class="comment">/*分区编号*/</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> int partition;</span><br><span class="line">    <span class="comment">/*偏移量*/</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> long offset;</span><br><span class="line">    <span class="comment">/*时间戳*/</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> long timestamp;</span><br><span class="line">    <span class="comment">/*时间戳代表的含义*/</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="type">TimestampType</span> timestampType;</span><br><span class="line">    <span class="comment">/*键序列化器*/</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> int serializedKeySize;</span><br><span class="line">    <span class="comment">/*值序列化器*/</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> int serializedValueSize;</span><br><span class="line">    <span class="comment">/*值序列化器*/</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="type">Headers</span> headers;</span><br><span class="line">    <span class="comment">/*键*/</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="type">K</span> key;</span><br><span class="line">    <span class="comment">/*值*/</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="type">V</span> value;</span><br><span class="line">    .....   </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="3-2-生产者属性"><a href="#3-2-生产者属性" class="headerlink" title="3.2 生产者属性"></a>3.2 生产者属性</h3><p>在示例代码中 <code>kafkaParams</code> 封装了 Kafka 消费者的属性，这些属性和 Spark Streaming 无关，是 Kafka 原生 API 中就有定义的。其中服务器地址、键序列化器和值序列化器是必选的，其他配置是可选的。其余可选的配置项如下：</p>
<h4 id="1-fetch-min-byte"><a href="#1-fetch-min-byte" class="headerlink" title="1. fetch.min.byte"></a>1. fetch.min.byte</h4><p>消费者从服务器获取记录的最小字节数。如果可用的数据量小于设置值，broker 会等待有足够的可用数据时才会把它返回给消费者。</p>
<h4 id="2-fetch-max-wait-ms"><a href="#2-fetch-max-wait-ms" class="headerlink" title="2. fetch.max.wait.ms"></a>2. fetch.max.wait.ms</h4><p>broker 返回给消费者数据的等待时间。</p>
<h4 id="3-max-partition-fetch-bytes"><a href="#3-max-partition-fetch-bytes" class="headerlink" title="3. max.partition.fetch.bytes"></a>3. max.partition.fetch.bytes</h4><p>分区返回给消费者的最大字节数。</p>
<h4 id="4-session-timeout-ms"><a href="#4-session-timeout-ms" class="headerlink" title="4. session.timeout.ms"></a>4. session.timeout.ms</h4><p>消费者在被认为死亡之前可以与服务器断开连接的时间。</p>
<h4 id="5-auto-offset-reset"><a href="#5-auto-offset-reset" class="headerlink" title="5. auto.offset.reset"></a>5. auto.offset.reset</h4><p>该属性指定了消费者在读取一个没有偏移量的分区或者偏移量无效的情况下该作何处理：</p>
<ul>
<li>latest(默认值) ：在偏移量无效的情况下，消费者将从其启动之后生成的最新的记录开始读取数据；</li>
<li>earliest ：在偏移量无效的情况下，消费者将从起始位置读取分区的记录。</li>
</ul>
<h4 id="6-enable-auto-commit"><a href="#6-enable-auto-commit" class="headerlink" title="6. enable.auto.commit"></a>6. enable.auto.commit</h4><p>是否自动提交偏移量，默认值是 true,为了避免出现重复数据和数据丢失，可以把它设置为 false。</p>
<h4 id="7-client-id"><a href="#7-client-id" class="headerlink" title="7. client.id"></a>7. client.id</h4><p>客户端 id，服务器用来识别消息的来源。</p>
<h4 id="8-max-poll-records"><a href="#8-max-poll-records" class="headerlink" title="8. max.poll.records"></a>8. max.poll.records</h4><p>单次调用 <code>poll()</code> 方法能够返回的记录数量。</p>
<h4 id="9-receive-buffer-bytes-和-send-buffer-byte"><a href="#9-receive-buffer-bytes-和-send-buffer-byte" class="headerlink" title="9. receive.buffer.bytes 和 send.buffer.byte"></a>9. receive.buffer.bytes 和 send.buffer.byte</h4><p>这两个参数分别指定 TCP socket 接收和发送数据包缓冲区的大小，-1 代表使用操作系统的默认值。</p>
<h3 id="3-3-位置策略"><a href="#3-3-位置策略" class="headerlink" title="3.3 位置策略"></a>3.3 位置策略</h3><p>Spark Streaming 中提供了如下三种位置策略，用于指定 Kafka 主题分区与 Spark 执行程序 Executors 之间的分配关系：</p>
<ul>
<li><p><strong>PreferConsistent</strong> : 它将在所有的 Executors 上均匀分配分区；</p>
</li>
<li><p><strong>PreferBrokers</strong> : 当 Spark 的 Executor 与 Kafka Broker 在同一机器上时可以选择该选项，它优先将该 Broker 上的首领分区分配给该机器上的 Executor；</p>
</li>
<li><strong>PreferFixed</strong> : 可以指定主题分区与特定主机的映射关系，显示地将分区分配到特定的主机，其构造器如下：</li>
</ul>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Experimental</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">PreferFixed</span></span>(hostMap: collection.<span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">String</span>]): <span class="type">LocationStrategy</span> =</span><br><span class="line">  <span class="keyword">new</span> <span class="type">PreferFixed</span>(<span class="keyword">new</span> ju.<span class="type">HashMap</span>[<span class="type">TopicPartition</span>, <span class="type">String</span>](hostMap.asJava))</span><br><span class="line"></span><br><span class="line"><span class="meta">@Experimental</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">PreferFixed</span></span>(hostMap: ju.<span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">String</span>]): <span class="type">LocationStrategy</span> =</span><br><span class="line">  <span class="keyword">new</span> <span class="type">PreferFixed</span>(hostMap)</span><br></pre></td></tr></table></figure>
<h3 id="3-4-订阅方式"><a href="#3-4-订阅方式" class="headerlink" title="3.4 订阅方式"></a>3.4 订阅方式</h3><p>Spark Streaming 提供了两种主题订阅方式，分别为 <code>Subscribe</code> 和 <code>SubscribePattern</code>。后者可以使用正则匹配订阅主题的名称。其构造器分别如下：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * @param 需要订阅的主题的集合</span></span><br><span class="line"><span class="comment">  * @param Kafka 消费者参数</span></span><br><span class="line"><span class="comment">  * @param offsets(可选): 在初始启动时开始的偏移量。如果没有，则将使用保存的偏移量或 auto.offset.reset 属性的值</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Subscribe</span></span>[<span class="type">K</span>, <span class="type">V</span>](</span><br><span class="line">    topics: ju.<span class="type">Collection</span>[jl.<span class="type">String</span>],</span><br><span class="line">    kafkaParams: ju.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">Object</span>],</span><br><span class="line">    offsets: ju.<span class="type">Map</span>[<span class="type">TopicPartition</span>, jl.<span class="type">Long</span>]): <span class="type">ConsumerStrategy</span>[<span class="type">K</span>, <span class="type">V</span>] = &#123; ... &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * @param 需要订阅的正则</span></span><br><span class="line"><span class="comment">  * @param Kafka 消费者参数</span></span><br><span class="line"><span class="comment">  * @param offsets(可选): 在初始启动时开始的偏移量。如果没有，则将使用保存的偏移量或 auto.offset.reset 属性的值</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">SubscribePattern</span></span>[<span class="type">K</span>, <span class="type">V</span>](</span><br><span class="line">    pattern: ju.regex.<span class="type">Pattern</span>,</span><br><span class="line">    kafkaParams: collection.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">Object</span>],</span><br><span class="line">    offsets: collection.<span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">Long</span>]): <span class="type">ConsumerStrategy</span>[<span class="type">K</span>, <span class="type">V</span>] = &#123; ... &#125;</span><br></pre></td></tr></table></figure>
<p>在示例代码中，我们实际上并没有指定第三个参数 <code>offsets</code>，所以程序默认采用的是配置的 <code>auto.offset.reset</code> 属性的值 latest，即在偏移量无效的情况下，消费者将从其启动之后生成的最新的记录开始读取数据。</p>
<h3 id="3-5-提交偏移量"><a href="#3-5-提交偏移量" class="headerlink" title="3.5 提交偏移量"></a>3.5 提交偏移量</h3><p>在示例代码中，我们将 <code>enable.auto.commit</code> 设置为 true，代表自动提交。在某些情况下，你可能需要更高的可靠性，如在业务完全处理完成后再提交偏移量，这时候可以使用手动提交。想要进行手动提交，需要调用 Kafka 原生的 API :</p>
<ul>
<li><code>commitSync</code>:  用于异步提交；</li>
<li><code>commitAsync</code>：用于同步提交。</li>
</ul>
<p>具体提交方式可以参见：<a href="https://github.com/heibaiying/BigData-Notes/blob/master/notes/Kafka 消费者详解.md" target="_blank" rel="noopener">Kafka 消费者详解</a></p>
<h2 id="四、启动测试"><a href="#四、启动测试" class="headerlink" title="四、启动测试"></a>四、启动测试</h2><h3 id="4-1-创建主题"><a href="#4-1-创建主题" class="headerlink" title="4.1 创建主题"></a>4.1 创建主题</h3><h4 id="1-启动Kakfa"><a href="#1-启动Kakfa" class="headerlink" title="1. 启动Kakfa"></a>1. 启动Kakfa</h4><p>Kafka 的运行依赖于 zookeeper，需要预先启动，可以启动 Kafka 内置的 zookeeper，也可以启动自己安装的：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> zookeeper启动命令</span></span><br><span class="line">bin/zkServer.sh start</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 内置zookeeper启动命令</span></span><br><span class="line">bin/zookeeper-server-start.sh config/zookeeper.properties</span><br></pre></td></tr></table></figure>
<p>启动单节点 kafka 用于测试：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> bin/kafka-server-start.sh config/server.properties</span></span><br></pre></td></tr></table></figure>
<h4 id="2-创建topic"><a href="#2-创建topic" class="headerlink" title="2. 创建topic"></a>2. 创建topic</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 创建用于测试主题</span></span><br><span class="line">bin/kafka-topics.sh --create \</span><br><span class="line">                    --bootstrap-server hadoop001:9092 \</span><br><span class="line">                    --replication-factor 1 \</span><br><span class="line">                    --partitions 1  \</span><br><span class="line">                    --topic spark-streaming-topic</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看所有主题</span></span><br><span class="line"> bin/kafka-topics.sh --list --bootstrap-server hadoop001:9092</span><br></pre></td></tr></table></figure>
<h4 id="3-创建生产者"><a href="#3-创建生产者" class="headerlink" title="3. 创建生产者"></a>3. 创建生产者</h4><p>这里创建一个 Kafka 生产者，用于发送测试数据：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">bin/kafka-console-producer.sh --broker-list hadoop001:9092 --topic spark-streaming-topic</span><br></pre></td></tr></table></figure>
<h3 id="4-2-本地模式测试"><a href="#4-2-本地模式测试" class="headerlink" title="4.2 本地模式测试"></a>4.2 本地模式测试</h3><p>这里我直接使用本地模式启动 Spark Streaming 程序。启动后使用生产者发送数据，从控制台查看结果。</p>
<p>从控制台输出中可以看到数据流已经被成功接收，由于采用 <code>kafka-console-producer.sh</code> 发送的数据默认是没有 key 的，所以 key 值为 null。同时从输出中也可以看到在程序中指定的 <code>groupId</code> 和程序自动分配的 <code>clientId</code>。</p>
<div align="center"> <img src="../pictures/spark-straming-kafka-console.png"> </div>





<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol>
<li><a href="https://spark.apache.org/docs/latest/streaming-kafka-0-10-integration.html" target="_blank" rel="noopener">https://spark.apache.org/docs/latest/streaming-kafka-0-10-integration.html</a></li>
</ol>
]]></content>
      <categories>
        <category>notes</category>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark Streaming 整合 Flume</title>
    <url>/2021/03/17/Spark_Streaming%E6%95%B4%E5%90%88Flume/</url>
    <content><![CDATA[<h2 id="一、简介"><a href="#一、简介" class="headerlink" title="一、简介"></a>一、简介</h2><p>Apache Flume 是一个分布式，高可用的数据收集系统，可以从不同的数据源收集数据，经过聚合后发送到分布式计算框架或者存储系统中。Spark Straming 提供了以下两种方式用于 Flume 的整合。</p>
<h2 id="二、推送式方法"><a href="#二、推送式方法" class="headerlink" title="二、推送式方法"></a>二、推送式方法</h2><p>在推送式方法 (Flume-style Push-based Approach) 中，Spark Streaming 程序需要对某台服务器的某个端口进行监听，Flume 通过 <code>avro Sink</code> 将数据源源不断推送到该端口。这里以监听日志文件为例，具体整合方式如下：</p>
<h3 id="2-1-配置日志收集Flume"><a href="#2-1-配置日志收集Flume" class="headerlink" title="2.1 配置日志收集Flume"></a>2.1 配置日志收集Flume</h3><p>新建配置 <code>netcat-memory-avro.properties</code>，使用 <code>tail</code> 命令监听文件内容变化，然后将新的文件内容通过 <code>avro sink</code> 发送到 hadoop001 这台服务器的 8888 端口：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#指定agent的sources,sinks,channels</span><br><span class="line">a1.sources = s1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line">#配置sources属性</span><br><span class="line">a1.sources.s1.type = exec</span><br><span class="line">a1.sources.s1.command = tail -F /tmp/log.txt</span><br><span class="line">a1.sources.s1.shell = /bin/bash -c</span><br><span class="line">a1.sources.s1.channels = c1</span><br><span class="line"></span><br><span class="line">#配置sink</span><br><span class="line">a1.sinks.k1.type = avro</span><br><span class="line">a1.sinks.k1.hostname = hadoop001</span><br><span class="line">a1.sinks.k1.port = 8888</span><br><span class="line">a1.sinks.k1.batch-size = 1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br><span class="line"></span><br><span class="line">#配置channel类型</span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br></pre></td></tr></table></figure>
<h3 id="2-2-项目依赖"><a href="#2-2-项目依赖" class="headerlink" title="2.2 项目依赖"></a>2.2 项目依赖</h3><p>项目采用 Maven 工程进行构建，主要依赖为 <code>spark-streaming</code> 和 <code>spark-streaming-flume</code>。</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">properties</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">scala.version</span>&gt;</span>2.11<span class="tag">&lt;/<span class="name">scala.version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">spark.version</span>&gt;</span>2.4.0<span class="tag">&lt;/<span class="name">spark.version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">properties</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- Spark Streaming--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-streaming_$&#123;scala.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;spark.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- Spark Streaming 整合 Flume 依赖--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-streaming-flume_$&#123;scala.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.4.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h3 id="2-3-Spark-Streaming接收日志数据"><a href="#2-3-Spark-Streaming接收日志数据" class="headerlink" title="2.3 Spark Streaming接收日志数据"></a>2.3 Spark Streaming接收日志数据</h3><p>调用 FlumeUtils 工具类的 <code>createStream</code> 方法，对 hadoop001 的 8888 端口进行监听，获取到流数据并进行打印：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.flume.<span class="type">FlumeUtils</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">PushBasedWordCount</span> </span>&#123;</span><br><span class="line">    </span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line">    <span class="comment">// 1.获取输入流</span></span><br><span class="line">    <span class="keyword">val</span> flumeStream = <span class="type">FlumeUtils</span>.createStream(ssc, <span class="string">"hadoop001"</span>, <span class="number">8888</span>)</span><br><span class="line">    <span class="comment">// 2.打印输入流的数据</span></span><br><span class="line">    flumeStream.map(line =&gt; <span class="keyword">new</span> <span class="type">String</span>(line.event.getBody.array()).trim).print()</span><br><span class="line"></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="2-4-项目打包"><a href="#2-4-项目打包" class="headerlink" title="2.4 项目打包"></a>2.4 项目打包</h3><p>因为 Spark 安装目录下是不含有 <code>spark-streaming-flume</code> 依赖包的，所以在提交到集群运行时候必须提供该依赖包，你可以在提交命令中使用 <code>--jar</code> 指定上传到服务器的该依赖包，或者使用 <code>--packages org.apache.spark:spark-streaming-flume_2.12:2.4.3</code> 指定依赖包的完整名称，这样程序在启动时会先去中央仓库进行下载。</p>
<p>这里我采用的是第三种方式：使用 <code>maven-shade-plugin</code> 插件进行 <code>ALL IN ONE</code> 打包，把所有依赖的 Jar 一并打入最终包中。需要注意的是 <code>spark-streaming</code> 包在 Spark 安装目录的 <code>jars</code> 目录中已经提供，所以不需要打入。插件配置如下：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">build</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-compiler-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">source</span>&gt;</span>8<span class="tag">&lt;/<span class="name">source</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">target</span>&gt;</span>8<span class="tag">&lt;/<span class="name">target</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--使用 shade 进行打包--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-shade-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">createDependencyReducedPom</span>&gt;</span>true<span class="tag">&lt;/<span class="name">createDependencyReducedPom</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">filters</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">filter</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">artifact</span>&gt;</span>*:*<span class="tag">&lt;/<span class="name">artifact</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">excludes</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>META-INF/*.SF<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>META-INF/*.sf<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>META-INF/*.DSA<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>META-INF/*.dsa<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>META-INF/*.RSA<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>META-INF/*.rsa<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>META-INF/*.EC<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>META-INF/*.ec<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>META-INF/MSFTSIG.SF<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>META-INF/MSFTSIG.RSA<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">excludes</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">filter</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">filters</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactSet</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">excludes</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>org.apache.spark:spark-streaming_$&#123;scala.version&#125;<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>org.scala-lang:scala-library<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>org.apache.commons:commons-lang3<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">excludes</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">artifactSet</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">phase</span>&gt;</span>package<span class="tag">&lt;/<span class="name">phase</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">goal</span>&gt;</span>shade<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">transformers</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">transformer</span> </span></span><br><span class="line"><span class="tag">                              <span class="attr">implementation</span>=<span class="string">"org.apache.maven.plugins.shade.resource.ServicesResourceTransformer"</span>/&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">transformer</span> </span></span><br><span class="line"><span class="tag">                              <span class="attr">implementation</span>=<span class="string">"org.apache.maven.plugins.shade.resource.ManifestResourceTransformer"</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;/<span class="name">transformer</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">transformers</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--打包.scala 文件需要配置此插件--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.scala-tools<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-scala-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.15.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">id</span>&gt;</span>scala-compile<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">goal</span>&gt;</span>compile<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">includes</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">include</span>&gt;</span>**/*.scala<span class="tag">&lt;/<span class="name">include</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">includes</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">id</span>&gt;</span>scala-test-compile<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">goal</span>&gt;</span>testCompile<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">build</span>&gt;</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>本项目完整源码见：<a href="https://github.com/heibaiying/BigData-Notes/tree/master/code/spark/spark-streaming-flume" target="_blank" rel="noopener">spark-streaming-flume</a></p>
</blockquote>
<p>使用 <code>mvn clean package</code> 命令打包后会生产以下两个 Jar 包，提交 <code>非 original</code> 开头的 Jar 即可。</p>
<div align="center"> <img src="../pictures/spark-streaming-flume-jar.png"> </div>

<h3 id="2-5-启动服务和提交作业"><a href="#2-5-启动服务和提交作业" class="headerlink" title="2.5 启动服务和提交作业"></a>2.5 启动服务和提交作业</h3><p> 启动 Flume 服务：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">flume-ng agent \</span><br><span class="line">--conf conf \</span><br><span class="line">--conf-file /usr/app/apache-flume-1.6.0-cdh5.15.2-bin/examples/netcat-memory-avro.properties \</span><br><span class="line">--name a1 -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure>
<p>提交 Spark Streaming 作业：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">spark-submit \</span><br><span class="line">--class com.heibaiying.flume.PushBasedWordCount \</span><br><span class="line">--master local[4] \</span><br><span class="line">/usr/appjar/spark-streaming-flume-1.0.jar</span><br></pre></td></tr></table></figure>
<h3 id="2-6-测试"><a href="#2-6-测试" class="headerlink" title="2.6 测试"></a>2.6 测试</h3><p>这里使用 <code>echo</code> 命令模拟日志产生的场景，往日志文件中追加数据，然后查看程序的输出：</p>
<div align="center"> <img src="../pictures/spark-flume-input.png"> </div>

<p>Spark Streaming 程序成功接收到数据并打印输出：</p>
<div align="center"> <img src="../pictures/spark-flume-console.png"> </div>

<h3 id="2-7-注意事项"><a href="#2-7-注意事项" class="headerlink" title="2.7 注意事项"></a>2.7 注意事项</h3><h4 id="1-启动顺序"><a href="#1-启动顺序" class="headerlink" title="1. 启动顺序"></a>1. 启动顺序</h4><p>这里需要注意的，不论你先启动 Spark 程序还是 Flume 程序，由于两者的启动都需要一定的时间，此时先启动的程序会短暂地抛出端口拒绝连接的异常，此时不需要进行任何操作，等待两个程序都启动完成即可。</p>
<div align="center"> <img src="../pictures/flume-retry.png"> </div>

<h4 id="2-版本一致"><a href="#2-版本一致" class="headerlink" title="2. 版本一致"></a>2. 版本一致</h4><p>最好保证用于本地开发和编译的 Scala 版本和 Spark 的 Scala 版本一致，至少保证大版本一致，如都是 <code>2.11</code>。</p>
<p><br></p>
<h2 id="三、拉取式方法"><a href="#三、拉取式方法" class="headerlink" title="三、拉取式方法"></a>三、拉取式方法</h2><p>拉取式方法 (Pull-based Approach using a Custom Sink) 是将数据推送到 <code>SparkSink</code> 接收器中，此时数据会保持缓冲状态，Spark Streaming 定时从接收器中拉取数据。这种方式是基于事务的，即只有在 Spark Streaming 接收和复制数据完成后，才会删除缓存的数据。与第一种方式相比，具有更强的可靠性和容错保证。整合步骤如下：</p>
<h3 id="3-1-配置日志收集Flume"><a href="#3-1-配置日志收集Flume" class="headerlink" title="3.1  配置日志收集Flume"></a>3.1  配置日志收集Flume</h3><p>新建 Flume 配置文件 <code>netcat-memory-sparkSink.properties</code>，配置和上面基本一致，只是把 <code>a1.sinks.k1.type</code> 的属性修改为 <code>org.apache.spark.streaming.flume.sink.SparkSink</code>，即采用 Spark 接收器。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#指定agent的sources,sinks,channels</span><br><span class="line">a1.sources = s1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line">#配置sources属性</span><br><span class="line">a1.sources.s1.type = exec</span><br><span class="line">a1.sources.s1.command = tail -F /tmp/log.txt</span><br><span class="line">a1.sources.s1.shell = /bin/bash -c</span><br><span class="line">a1.sources.s1.channels = c1</span><br><span class="line"></span><br><span class="line">#配置sink</span><br><span class="line">a1.sinks.k1.type = org.apache.spark.streaming.flume.sink.SparkSink</span><br><span class="line">a1.sinks.k1.hostname = hadoop001</span><br><span class="line">a1.sinks.k1.port = 8888</span><br><span class="line">a1.sinks.k1.batch-size = 1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br><span class="line"></span><br><span class="line">#配置channel类型</span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br></pre></td></tr></table></figure>
<h3 id="2-2-新增依赖"><a href="#2-2-新增依赖" class="headerlink" title="2.2 新增依赖"></a>2.2 新增依赖</h3><p>使用拉取式方法需要额外添加以下两个依赖：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.scala-lang<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>scala-library<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.12.8<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.commons<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>commons-lang3<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.5<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>注意：添加这两个依赖只是为了本地测试，Spark 的安装目录下已经提供了这两个依赖，所以在最终打包时需要进行排除。</p>
<h3 id="2-3-Spark-Streaming接收日志数据-1"><a href="#2-3-Spark-Streaming接收日志数据-1" class="headerlink" title="2.3 Spark Streaming接收日志数据"></a>2.3 Spark Streaming接收日志数据</h3><p>这里和上面推送式方法的代码基本相同，只是将调用方法改为 <code>createPollingStream</code>。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.flume.<span class="type">FlumeUtils</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">PullBasedWordCount</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line">    <span class="comment">// 1.获取输入流</span></span><br><span class="line">    <span class="keyword">val</span> flumeStream = <span class="type">FlumeUtils</span>.createPollingStream(ssc, <span class="string">"hadoop001"</span>, <span class="number">8888</span>)</span><br><span class="line">    <span class="comment">// 2.打印输入流中的数据</span></span><br><span class="line">    flumeStream.map(line =&gt; <span class="keyword">new</span> <span class="type">String</span>(line.event.getBody.array()).trim).print()</span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="2-4-启动测试"><a href="#2-4-启动测试" class="headerlink" title="2.4 启动测试"></a>2.4 启动测试</h3><p>启动和提交作业流程与上面相同，这里给出执行脚本，过程不再赘述。</p>
<p>启动 Flume 进行日志收集：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">flume-ng agent \</span><br><span class="line">--conf conf \</span><br><span class="line">--conf-file /usr/app/apache-flume-1.6.0-cdh5.15.2-bin/examples/netcat-memory-sparkSink.properties \</span><br><span class="line">--name a1 -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure>
<p>提交 Spark Streaming 作业：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">spark-submit \</span><br><span class="line">--class com.heibaiying.flume.PullBasedWordCount \</span><br><span class="line">--master local[4] \</span><br><span class="line">/usr/appjar/spark-streaming-flume-1.0.jar</span><br></pre></td></tr></table></figure>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li><a href="https://spark.apache.org/docs/latest/streaming-flume-integration.html" target="_blank" rel="noopener">streaming-flume-integration</a></li>
<li>关于大数据应用常用的打包方式可以参见：<a href="https://github.com/heibaiying/BigData-Notes/blob/master/notes/大数据应用常用打包方式.md" target="_blank" rel="noopener">大数据应用常用打包方式</a></li>
</ul>
]]></content>
      <categories>
        <category>notes</category>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title>Structured API基本使用</title>
    <url>/2021/03/17/Spark_Structured_API%E7%9A%84%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<h2 id="一、创建DataFrame和Dataset"><a href="#一、创建DataFrame和Dataset" class="headerlink" title="一、创建DataFrame和Dataset"></a>一、创建DataFrame和Dataset</h2><h3 id="1-1-创建DataFrame"><a href="#1-1-创建DataFrame" class="headerlink" title="1.1 创建DataFrame"></a>1.1 创建DataFrame</h3><p>Spark 中所有功能的入口点是 <code>SparkSession</code>，可以使用 <code>SparkSession.builder()</code> 创建。创建后应用程序就可以从现有 RDD，Hive 表或 Spark 数据源创建 DataFrame。示例如下：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder().appName(<span class="string">"Spark-SQL"</span>).master(<span class="string">"local[2]"</span>).getOrCreate()</span><br><span class="line"><span class="keyword">val</span> df = spark.read.json(<span class="string">"/usr/file/json/emp.json"</span>)</span><br><span class="line">df.show()</span><br><span class="line"></span><br><span class="line"><span class="comment">// 建议在进行 spark SQL 编程前导入下面的隐式转换，因为 DataFrames 和 dataSets 中很多操作都依赖了隐式转换</span></span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br></pre></td></tr></table></figure>
<p>可以使用 <code>spark-shell</code> 进行测试，需要注意的是 <code>spark-shell</code> 启动后会自动创建一个名为 <code>spark</code> 的 <code>SparkSession</code>，在命令行中可以直接引用即可：</p>
<div align="center"> <img src="../pictures/spark-sql-shell.png"> </div>

<p><br></p>
<h3 id="1-2-创建Dataset"><a href="#1-2-创建Dataset" class="headerlink" title="1.2 创建Dataset"></a>1.2 创建Dataset</h3><p>Spark 支持由内部数据集和外部数据集来创建 DataSet，其创建方式分别如下：</p>
<h4 id="1-由外部数据集创建"><a href="#1-由外部数据集创建" class="headerlink" title="1. 由外部数据集创建"></a>1. 由外部数据集创建</h4><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 1.需要导入隐式转换</span></span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line"><span class="comment">// 2.创建 case class,等价于 Java Bean</span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Emp</span>(<span class="params">ename: <span class="type">String</span>, comm: <span class="type">Double</span>, deptno: <span class="type">Long</span>, empno: <span class="type">Long</span>, </span></span></span><br><span class="line"><span class="class"><span class="params">               hiredate: <span class="type">String</span>, job: <span class="type">String</span>, mgr: <span class="type">Long</span>, sal: <span class="type">Double</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">//</span> 3.<span class="title">由外部数据集创建</span> <span class="title">Datasets</span></span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">ds</span> </span>= spark.read.json(<span class="string">"/usr/file/emp.json"</span>).as[<span class="type">Emp</span>]</span><br><span class="line">ds.show()</span><br></pre></td></tr></table></figure>
<h4 id="2-由内部数据集创建"><a href="#2-由内部数据集创建" class="headerlink" title="2. 由内部数据集创建"></a>2. 由内部数据集创建</h4><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 1.需要导入隐式转换</span></span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line"><span class="comment">// 2.创建 case class,等价于 Java Bean</span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Emp</span>(<span class="params">ename: <span class="type">String</span>, comm: <span class="type">Double</span>, deptno: <span class="type">Long</span>, empno: <span class="type">Long</span>, </span></span></span><br><span class="line"><span class="class"><span class="params">               hiredate: <span class="type">String</span>, job: <span class="type">String</span>, mgr: <span class="type">Long</span>, sal: <span class="type">Double</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">//</span> 3.<span class="title">由内部数据集创建</span> <span class="title">Datasets</span></span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">caseClassDS</span> </span>= <span class="type">Seq</span>(<span class="type">Emp</span>(<span class="string">"ALLEN"</span>, <span class="number">300.0</span>, <span class="number">30</span>, <span class="number">7499</span>, <span class="string">"1981-02-20 00:00:00"</span>, <span class="string">"SALESMAN"</span>, <span class="number">7698</span>, <span class="number">1600.0</span>),</span><br><span class="line">                      <span class="type">Emp</span>(<span class="string">"JONES"</span>, <span class="number">300.0</span>, <span class="number">30</span>, <span class="number">7499</span>, <span class="string">"1981-02-20 00:00:00"</span>, <span class="string">"SALESMAN"</span>, <span class="number">7698</span>, <span class="number">1600.0</span>))</span><br><span class="line">                    .toDS()</span><br><span class="line">caseClassDS.show()</span><br></pre></td></tr></table></figure>
<p><br></p>
<h3 id="1-3-由RDD创建DataFrame"><a href="#1-3-由RDD创建DataFrame" class="headerlink" title="1.3 由RDD创建DataFrame"></a>1.3 由RDD创建DataFrame</h3><p>Spark 支持两种方式把 RDD 转换为 DataFrame，分别是使用反射推断和指定 Schema 转换：</p>
<h4 id="1-使用反射推断"><a href="#1-使用反射推断" class="headerlink" title="1. 使用反射推断"></a>1. 使用反射推断</h4><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 1.导入隐式转换</span></span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line"><span class="comment">// 2.创建部门类</span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Dept</span>(<span class="params">deptno: <span class="type">Long</span>, dname: <span class="type">String</span>, loc: <span class="type">String</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">//</span> 3.<span class="title">创建</span> <span class="title">RDD</span> <span class="title">并转换为</span> <span class="title">dataSet</span></span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">rddToDS</span> </span>= spark.sparkContext</span><br><span class="line">  .textFile(<span class="string">"/usr/file/dept.txt"</span>)</span><br><span class="line">  .map(_.split(<span class="string">"\t"</span>))</span><br><span class="line">  .map(line =&gt; <span class="type">Dept</span>(line(<span class="number">0</span>).trim.toLong, line(<span class="number">1</span>), line(<span class="number">2</span>)))</span><br><span class="line">  .toDS()  <span class="comment">// 如果调用 toDF() 则转换为 dataFrame</span></span><br></pre></td></tr></table></figure>
<h4 id="2-以编程方式指定Schema"><a href="#2-以编程方式指定Schema" class="headerlink" title="2. 以编程方式指定Schema"></a>2. 以编程方式指定Schema</h4><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">Row</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types._</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 1.定义每个列的列类型</span></span><br><span class="line"><span class="keyword">val</span> fields = <span class="type">Array</span>(<span class="type">StructField</span>(<span class="string">"deptno"</span>, <span class="type">LongType</span>, nullable = <span class="literal">true</span>),</span><br><span class="line">                   <span class="type">StructField</span>(<span class="string">"dname"</span>, <span class="type">StringType</span>, nullable = <span class="literal">true</span>),</span><br><span class="line">                   <span class="type">StructField</span>(<span class="string">"loc"</span>, <span class="type">StringType</span>, nullable = <span class="literal">true</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">// 2.创建 schema</span></span><br><span class="line"><span class="keyword">val</span> schema = <span class="type">StructType</span>(fields)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 3.创建 RDD</span></span><br><span class="line"><span class="keyword">val</span> deptRDD = spark.sparkContext.textFile(<span class="string">"/usr/file/dept.txt"</span>)</span><br><span class="line"><span class="keyword">val</span> rowRDD = deptRDD.map(_.split(<span class="string">"\t"</span>)).map(line =&gt; <span class="type">Row</span>(line(<span class="number">0</span>).toLong, line(<span class="number">1</span>), line(<span class="number">2</span>)))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 4.将 RDD 转换为 dataFrame</span></span><br><span class="line"><span class="keyword">val</span> deptDF = spark.createDataFrame(rowRDD, schema)</span><br><span class="line">deptDF.show()</span><br></pre></td></tr></table></figure>
<p><br></p>
<h3 id="1-4-DataFrames与Datasets互相转换"><a href="#1-4-DataFrames与Datasets互相转换" class="headerlink" title="1.4  DataFrames与Datasets互相转换"></a>1.4  DataFrames与Datasets互相转换</h3><p>Spark 提供了非常简单的转换方法用于 DataFrame 与 Dataset 间的互相转换，示例如下：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> DataFrames转Datasets</span></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> df.as[Emp]</span></span><br><span class="line">res1: org.apache.spark.sql.Dataset[Emp] = [COMM: double, DEPTNO: bigint ... 6 more fields]</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Datasets转DataFrames</span></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> ds.toDF()</span></span><br><span class="line">res2: org.apache.spark.sql.DataFrame = [COMM: double, DEPTNO: bigint ... 6 more fields]</span><br></pre></td></tr></table></figure>
<p><br></p>
<h2 id="二、Columns列操作"><a href="#二、Columns列操作" class="headerlink" title="二、Columns列操作"></a>二、Columns列操作</h2><h3 id="2-1-引用列"><a href="#2-1-引用列" class="headerlink" title="2.1 引用列"></a>2.1 引用列</h3><p>Spark 支持多种方法来构造和引用列，最简单的是使用 <code>col()</code> 或 <code>column()</code> 函数。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">col(<span class="string">"colName"</span>)</span><br><span class="line">column(<span class="string">"colName"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 对于 Scala 语言而言，还可以使用$"myColumn"和'myColumn 这两种语法糖进行引用。</span></span><br><span class="line">df.select($<span class="string">"ename"</span>, $<span class="string">"job"</span>).show()</span><br><span class="line">df.select(<span class="symbol">'ename</span>, <span class="symbol">'job</span>).show()</span><br></pre></td></tr></table></figure>
<h3 id="2-2-新增列"><a href="#2-2-新增列" class="headerlink" title="2.2 新增列"></a>2.2 新增列</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 基于已有列值新增列</span></span><br><span class="line">df.withColumn(<span class="string">"upSal"</span>,$<span class="string">"sal"</span>+<span class="number">1000</span>)</span><br><span class="line"><span class="comment">// 基于固定值新增列</span></span><br><span class="line">df.withColumn(<span class="string">"intCol"</span>,lit(<span class="number">1000</span>))</span><br></pre></td></tr></table></figure>
<h3 id="2-3-删除列"><a href="#2-3-删除列" class="headerlink" title="2.3 删除列"></a>2.3 删除列</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 支持删除多个列</span></span><br><span class="line">df.drop(<span class="string">"comm"</span>,<span class="string">"job"</span>).show()</span><br></pre></td></tr></table></figure>
<h3 id="2-4-重命名列"><a href="#2-4-重命名列" class="headerlink" title="2.4 重命名列"></a>2.4 重命名列</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">df.withColumnRenamed(<span class="string">"comm"</span>, <span class="string">"common"</span>).show()</span><br></pre></td></tr></table></figure>
<p>需要说明的是新增，删除，重命名列都会产生新的 DataFrame，原来的 DataFrame 不会被改变。</p>
<p><br></p>
<h2 id="三、使用Structured-API进行基本查询"><a href="#三、使用Structured-API进行基本查询" class="headerlink" title="三、使用Structured API进行基本查询"></a>三、使用Structured API进行基本查询</h2><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 1.查询员工姓名及工作</span></span><br><span class="line">df.select($<span class="string">"ename"</span>, $<span class="string">"job"</span>).show()</span><br><span class="line"></span><br><span class="line"><span class="comment">// 2.filter 查询工资大于 2000 的员工信息</span></span><br><span class="line">df.filter($<span class="string">"sal"</span> &gt; <span class="number">2000</span>).show()</span><br><span class="line"></span><br><span class="line"><span class="comment">// 3.orderBy 按照部门编号降序，工资升序进行查询</span></span><br><span class="line">df.orderBy(desc(<span class="string">"deptno"</span>), asc(<span class="string">"sal"</span>)).show()</span><br><span class="line"></span><br><span class="line"><span class="comment">// 4.limit 查询工资最高的 3 名员工的信息</span></span><br><span class="line">df.orderBy(desc(<span class="string">"sal"</span>)).limit(<span class="number">3</span>).show()</span><br><span class="line"></span><br><span class="line"><span class="comment">// 5.distinct 查询所有部门编号</span></span><br><span class="line">df.select(<span class="string">"deptno"</span>).distinct().show()</span><br><span class="line"></span><br><span class="line"><span class="comment">// 6.groupBy 分组统计部门人数</span></span><br><span class="line">df.groupBy(<span class="string">"deptno"</span>).count().show()</span><br></pre></td></tr></table></figure>
<p><br></p>
<h2 id="四、使用Spark-SQL进行基本查询"><a href="#四、使用Spark-SQL进行基本查询" class="headerlink" title="四、使用Spark SQL进行基本查询"></a>四、使用Spark SQL进行基本查询</h2><h3 id="4-1-Spark-SQL基本使用"><a href="#4-1-Spark-SQL基本使用" class="headerlink" title="4.1 Spark  SQL基本使用"></a>4.1 Spark  SQL基本使用</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 1.首先需要将 DataFrame 注册为临时视图</span></span><br><span class="line">df.createOrReplaceTempView(<span class="string">"emp"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 2.查询员工姓名及工作</span></span><br><span class="line">spark.sql(<span class="string">"SELECT ename,job FROM emp"</span>).show()</span><br><span class="line"></span><br><span class="line"><span class="comment">// 3.查询工资大于 2000 的员工信息</span></span><br><span class="line">spark.sql(<span class="string">"SELECT * FROM emp where sal &gt; 2000"</span>).show()</span><br><span class="line"></span><br><span class="line"><span class="comment">// 4.orderBy 按照部门编号降序，工资升序进行查询</span></span><br><span class="line">spark.sql(<span class="string">"SELECT * FROM emp ORDER BY deptno DESC,sal ASC"</span>).show()</span><br><span class="line"></span><br><span class="line"><span class="comment">// 5.limit  查询工资最高的 3 名员工的信息</span></span><br><span class="line">spark.sql(<span class="string">"SELECT * FROM emp ORDER BY sal DESC LIMIT 3"</span>).show()</span><br><span class="line"></span><br><span class="line"><span class="comment">// 6.distinct 查询所有部门编号</span></span><br><span class="line">spark.sql(<span class="string">"SELECT DISTINCT(deptno) FROM emp"</span>).show()</span><br><span class="line"></span><br><span class="line"><span class="comment">// 7.分组统计部门人数</span></span><br><span class="line">spark.sql(<span class="string">"SELECT deptno,count(ename) FROM emp group by deptno"</span>).show()</span><br></pre></td></tr></table></figure>
<h3 id="4-2-全局临时视图"><a href="#4-2-全局临时视图" class="headerlink" title="4.2 全局临时视图"></a>4.2 全局临时视图</h3><p>上面使用 <code>createOrReplaceTempView</code> 创建的是会话临时视图，它的生命周期仅限于会话范围，会随会话的结束而结束。</p>
<p>你也可以使用 <code>createGlobalTempView</code> 创建全局临时视图，全局临时视图可以在所有会话之间共享，并直到整个 Spark 应用程序终止后才会消失。全局临时视图被定义在内置的 <code>global_temp</code> 数据库下，需要使用限定名称进行引用，如 <code>SELECT * FROM global_temp.view1</code>。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 注册为全局临时视图</span></span><br><span class="line">df.createGlobalTempView(<span class="string">"gemp"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 使用限定名称进行引用</span></span><br><span class="line">spark.sql(<span class="string">"SELECT ename,job FROM global_temp.gemp"</span>).show()</span><br></pre></td></tr></table></figure>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://spark.apache.org/docs/latest/sql-getting-started.html" target="_blank" rel="noopener">Spark SQL, DataFrames and Datasets Guide &gt; Getting Started</a></p>
]]></content>
      <categories>
        <category>notes</category>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark简介</title>
    <url>/2021/03/17/Spark%E7%AE%80%E4%BB%8B/</url>
    <content><![CDATA[<h2 id="一、简介"><a href="#一、简介" class="headerlink" title="一、简介"></a>一、简介</h2><p>Spark 于 2009 年诞生于加州大学伯克利分校 AMPLab，2013 年被捐赠给 Apache 软件基金会，2014 年 2 月成为 Apache 的顶级项目。相对于 MapReduce 的批处理计算，Spark 可以带来上百倍的性能提升，因此它成为继 MapReduce 之后，最为广泛使用的分布式计算框架。</p>
<h2 id="二、特点"><a href="#二、特点" class="headerlink" title="二、特点"></a>二、特点</h2><p>Apache Spark 具有以下特点：</p>
<ul>
<li>使用先进的 DAG 调度程序，查询优化器和物理执行引擎，以实现性能上的保证；</li>
<li>多语言支持，目前支持的有 Java，Scala，Python 和 R；</li>
<li>提供了 80 多个高级 API，可以轻松地构建应用程序；</li>
<li>支持批处理，流处理和复杂的业务分析；</li>
<li>丰富的类库支持：包括 SQL，MLlib，GraphX 和 Spark Streaming 等库，并且可以将它们无缝地进行组合；  </li>
<li>丰富的部署模式：支持本地模式和自带的集群模式，也支持在 Hadoop，Mesos，Kubernetes 上运行；</li>
<li>多数据源支持：支持访问 HDFS，Alluxio，Cassandra，HBase，Hive 以及数百个其他数据源中的数据。</li>
</ul>
<div align="center"> <img width="600px" src="../pictures/future-of-spark.png"> </div>

<h2 id="三、集群架构"><a href="#三、集群架构" class="headerlink" title="三、集群架构"></a>三、集群架构</h2><table>
<thead>
<tr>
<th>Term（术语）</th>
<th>Meaning（含义）</th>
</tr>
</thead>
<tbody>
<tr>
<td>Application</td>
<td>Spark 应用程序，由集群上的一个 Driver 节点和多个 Executor 节点组成。</td>
</tr>
<tr>
<td>Driver program</td>
<td>主运用程序，该进程运行应用的 main() 方法并且创建  SparkContext</td>
</tr>
<tr>
<td>Cluster manager</td>
<td>集群资源管理器（例如，Standlone Manager，Mesos，YARN）</td>
</tr>
<tr>
<td>Worker node</td>
<td>执行计算任务的工作节点</td>
</tr>
<tr>
<td>Executor</td>
<td>位于工作节点上的应用进程，负责执行计算任务并且将输出数据保存到内存或者磁盘中</td>
</tr>
<tr>
<td>Task</td>
<td>被发送到 Executor 中的工作单元</td>
</tr>
</tbody>
</table>
<div align="center"> <img src="../pictures/spark-集群模式.png"> </div>

<p><strong>执行过程</strong>：</p>
<ol>
<li>用户程序创建 SparkContext 后，它会连接到集群资源管理器，集群资源管理器会为用户程序分配计算资源，并启动 Executor；</li>
<li>Dirver 将计算程序划分为不同的执行阶段和多个 Task，之后将 Task 发送给 Executor；</li>
<li>Executor 负责执行 Task，并将执行状态汇报给 Driver，同时也会将当前节点资源的使用情况汇报给集群资源管理器。</li>
</ol>
<h2 id="四、核心组件"><a href="#四、核心组件" class="headerlink" title="四、核心组件"></a>四、核心组件</h2><p>Spark 基于 Spark Core 扩展了四个核心组件，分别用于满足不同领域的计算需求。</p>
<div align="center"> <img width="600px" src="../pictures/spark-stack.png"> </div>

<h3 id="3-1-Spark-SQL"><a href="#3-1-Spark-SQL" class="headerlink" title="3.1 Spark  SQL"></a>3.1 Spark  SQL</h3><p>Spark SQL 主要用于结构化数据的处理。其具有以下特点：</p>
<ul>
<li>能够将 SQL 查询与 Spark 程序无缝混合，允许您使用 SQL 或 DataFrame API 对结构化数据进行查询；</li>
<li>支持多种数据源，包括 Hive，Avro，Parquet，ORC，JSON 和 JDBC；</li>
<li>支持 HiveQL 语法以及用户自定义函数 (UDF)，允许你访问现有的 Hive 仓库；</li>
<li>支持标准的 JDBC 和 ODBC 连接；</li>
<li>支持优化器，列式存储和代码生成等特性，以提高查询效率。</li>
</ul>
<h3 id="3-2-Spark-Streaming"><a href="#3-2-Spark-Streaming" class="headerlink" title="3.2 Spark Streaming"></a>3.2 Spark Streaming</h3><p>Spark Streaming 主要用于快速构建可扩展，高吞吐量，高容错的流处理程序。支持从 HDFS，Flume，Kafka，Twitter 和 ZeroMQ 读取数据，并进行处理。</p>
<div align="center"> <img width="600px" src="../pictures/spark-streaming-arch.png"> </div>

<p> Spark Streaming 的本质是微批处理，它将数据流进行极小粒度的拆分，拆分为多个批处理，从而达到接近于流处理的效果。</p>
<div align="center"> <img width="600px" src="../pictures/spark-streaming-flow.png"> </div>



<h3 id="3-3-MLlib"><a href="#3-3-MLlib" class="headerlink" title="3.3 MLlib"></a>3.3 MLlib</h3><p>MLlib 是 Spark 的机器学习库。其设计目标是使得机器学习变得简单且可扩展。它提供了以下工具：</p>
<ul>
<li><strong>常见的机器学习算法</strong>：如分类，回归，聚类和协同过滤；</li>
<li><strong>特征化</strong>：特征提取，转换，降维和选择；</li>
<li><strong>管道</strong>：用于构建，评估和调整 ML 管道的工具；</li>
<li><strong>持久性</strong>：保存和加载算法，模型，管道数据；</li>
<li><strong>实用工具</strong>：线性代数，统计，数据处理等。</li>
</ul>
<h3 id="3-4-Graphx"><a href="#3-4-Graphx" class="headerlink" title="3.4 Graphx"></a>3.4 Graphx</h3><p>GraphX 是 Spark 中用于图形计算和图形并行计算的新组件。在高层次上，GraphX 通过引入一个新的图形抽象来扩展 RDD(一种具有附加到每个顶点和边缘的属性的定向多重图形)。为了支持图计算，GraphX 提供了一组基本运算符（如： subgraph，joinVertices 和 aggregateMessages）以及优化后的 Pregel API。此外，GraphX 还包括越来越多的图形算法和构建器，以简化图形分析任务。</p>
<h2 id><a href="#" class="headerlink" title=" "></a> </h2>]]></content>
      <categories>
        <category>notes</category>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title>Transformation 和 Action 常用算子</title>
    <url>/2021/03/17/Spark_Transformation%E5%92%8CAction%E7%AE%97%E5%AD%90/</url>
    <content><![CDATA[<h2 id="一、Transformation"><a href="#一、Transformation" class="headerlink" title="一、Transformation"></a>一、Transformation</h2><p>spark 常用的 Transformation 算子如下表：</p>
<table>
<thead>
<tr>
<th>Transformation 算子</th>
<th>Meaning（含义）</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>map</strong>(<em>func</em>)</td>
<td>对原 RDD 中每个元素运用 <em>func</em> 函数，并生成新的 RDD</td>
</tr>
<tr>
<td><strong>filter</strong>(<em>func</em>)</td>
<td>对原 RDD 中每个元素使用<em>func</em> 函数进行过滤，并生成新的 RDD</td>
</tr>
<tr>
<td><strong>flatMap</strong>(<em>func</em>)</td>
<td>与 map 类似，但是每一个输入的 item 被映射成 0 个或多个输出的 items（ <em>func</em> 返回类型需要为 Seq ）。</td>
</tr>
<tr>
<td><strong>mapPartitions</strong>(<em>func</em>)</td>
<td>与 map 类似，但函数单独在 RDD 的每个分区上运行， <em>func</em>函数的类型为  Iterator\<t> =&gt; Iterator\<u> ，其中 T 是 RDD 的类型，即 RDD[T]</u></t></td>
</tr>
<tr>
<td><strong>mapPartitionsWithIndex</strong>(<em>func</em>)</td>
<td>与 mapPartitions 类似，但 <em>func</em> 类型为 (Int, Iterator\<t>) =&gt; Iterator\<u> ，其中第一个参数为分区索引</u></t></td>
</tr>
<tr>
<td><strong>sample</strong>(<em>withReplacement</em>, <em>fraction</em>, <em>seed</em>)</td>
<td>数据采样，有三个可选参数：设置是否放回（withReplacement）、采样的百分比（<em>fraction</em>）、随机数生成器的种子（seed）；</td>
</tr>
<tr>
<td><strong>union</strong>(<em>otherDataset</em>)</td>
<td>合并两个 RDD</td>
</tr>
<tr>
<td><strong>intersection</strong>(<em>otherDataset</em>)</td>
<td>求两个 RDD 的交集</td>
</tr>
<tr>
<td><strong>distinct</strong>([<em>numTasks</em>]))</td>
<td>去重</td>
</tr>
<tr>
<td><strong>groupByKey</strong>([<em>numTasks</em>])</td>
<td>按照 key 值进行分区，即在一个 (K, V) 对的 dataset 上调用时，返回一个 (K, Iterable\<v>) <br><strong>Note:</strong> 如果分组是为了在每一个 key 上执行聚合操作（例如，sum 或 average)，此时使用 <code>reduceByKey</code> 或 <code>aggregateByKey</code> 性能会更好<br><strong>Note:</strong> 默认情况下，并行度取决于父 RDD 的分区数。可以传入 <code>numTasks</code> 参数进行修改。</v></td>
</tr>
<tr>
<td><strong>reduceByKey</strong>(<em>func</em>, [<em>numTasks</em>])</td>
<td>按照 key 值进行分组，并对分组后的数据执行归约操作。</td>
</tr>
<tr>
<td><strong>aggregateByKey</strong>(<em>zeroValue</em>,<em>numPartitions</em>)(<em>seqOp</em>, <em>combOp</em>, [<em>numTasks</em>])</td>
<td>当调用（K，V）对的数据集时，返回（K，U）对的数据集，其中使用给定的组合函数和 zeroValue 聚合每个键的值。与 groupByKey 类似，reduce 任务的数量可通过第二个参数进行配置。</td>
</tr>
<tr>
<td><strong>sortByKey</strong>([<em>ascending</em>], [<em>numTasks</em>])</td>
<td>按照 key 进行排序，其中的 key 需要实现 Ordered 特质，即可比较</td>
</tr>
<tr>
<td><strong>join</strong>(<em>otherDataset</em>, [<em>numTasks</em>])</td>
<td>在一个 (K, V) 和 (K, W) 类型的 dataset 上调用时，返回一个 (K, (V, W)) pairs 的 dataset，等价于内连接操作。如果想要执行外连接，可以使用 <code>leftOuterJoin</code>, <code>rightOuterJoin</code> 和 <code>fullOuterJoin</code> 等算子。</td>
</tr>
<tr>
<td><strong>cogroup</strong>(<em>otherDataset</em>, [<em>numTasks</em>])</td>
<td>在一个 (K, V) 对的 dataset 上调用时，返回一个 (K, (Iterable\<v>, Iterable\<w>)) tuples 的 dataset。</w></v></td>
</tr>
<tr>
<td><strong>cartesian</strong>(<em>otherDataset</em>)</td>
<td>在一个 T 和 U 类型的 dataset 上调用时，返回一个 (T, U) 类型的 dataset（即笛卡尔积）。</td>
</tr>
<tr>
<td><strong>coalesce</strong>(<em>numPartitions</em>)</td>
<td>将 RDD 中的分区数减少为 numPartitions。</td>
</tr>
<tr>
<td><strong>repartition</strong>(<em>numPartitions</em>)</td>
<td>随机重新调整 RDD 中的数据以创建更多或更少的分区，并在它们之间进行平衡。</td>
</tr>
<tr>
<td><strong>repartitionAndSortWithinPartitions</strong>(<em>partitioner</em>)</td>
<td>根据给定的 partitioner（分区器）对 RDD 进行重新分区，并对分区中的数据按照 key 值进行排序。这比调用 <code>repartition</code> 然后再 sorting（排序）效率更高，因为它可以将排序过程推送到 shuffle 操作所在的机器。</td>
</tr>
</tbody>
</table>
<p>下面分别给出这些算子的基本使用示例：</p>
<h3 id="1-1-map"><a href="#1-1-map" class="headerlink" title="1.1 map"></a>1.1 map</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> list = <span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">sc.parallelize(list).map(_ * <span class="number">10</span>).foreach(println)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出结果： 10 20 30 （这里为了节省篇幅去掉了换行,后文亦同）</span></span><br></pre></td></tr></table></figure>
<h3 id="1-2-filter"><a href="#1-2-filter" class="headerlink" title="1.2 filter"></a>1.2 filter</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> list = <span class="type">List</span>(<span class="number">3</span>, <span class="number">6</span>, <span class="number">9</span>, <span class="number">10</span>, <span class="number">12</span>, <span class="number">21</span>)</span><br><span class="line">sc.parallelize(list).filter(_ &gt;= <span class="number">10</span>).foreach(println)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出： 10 12 21</span></span><br></pre></td></tr></table></figure>
<h3 id="1-3-flatMap"><a href="#1-3-flatMap" class="headerlink" title="1.3 flatMap"></a>1.3 flatMap</h3><p><code>flatMap(func)</code> 与 <code>map</code> 类似，但每一个输入的 item 会被映射成 0 个或多个输出的 items（ <em>func</em> 返回类型需要为 <code>Seq</code>）。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> list = <span class="type">List</span>(<span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>), <span class="type">List</span>(<span class="number">3</span>), <span class="type">List</span>(), <span class="type">List</span>(<span class="number">4</span>, <span class="number">5</span>))</span><br><span class="line">sc.parallelize(list).flatMap(_.toList).map(_ * <span class="number">10</span>).foreach(println)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出结果 ： 10 20 30 40 50</span></span><br></pre></td></tr></table></figure>
<p>flatMap 这个算子在日志分析中使用概率非常高，这里进行一下演示：拆分输入的每行数据为单个单词，并赋值为 1，代表出现一次，之后按照单词分组并统计其出现总次数，代码如下：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> lines = <span class="type">List</span>(<span class="string">"spark flume spark"</span>,</span><br><span class="line">                 <span class="string">"hadoop flume hive"</span>)</span><br><span class="line">sc.parallelize(lines).flatMap(line =&gt; line.split(<span class="string">" "</span>)).</span><br><span class="line">map(word=&gt;(word,<span class="number">1</span>)).reduceByKey(_+_).foreach(println)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出：</span></span><br><span class="line">(spark,<span class="number">2</span>)</span><br><span class="line">(hive,<span class="number">1</span>)</span><br><span class="line">(hadoop,<span class="number">1</span>)</span><br><span class="line">(flume,<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<h3 id="1-4-mapPartitions"><a href="#1-4-mapPartitions" class="headerlink" title="1.4 mapPartitions"></a>1.4 mapPartitions</h3><p>与 map 类似，但函数单独在 RDD 的每个分区上运行， <em>func</em>函数的类型为 <code>Iterator&lt;T&gt; =&gt; Iterator&lt;U&gt;</code> (其中 T 是 RDD 的类型)，即输入和输出都必须是可迭代类型。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> list = <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>)</span><br><span class="line">sc.parallelize(list, <span class="number">3</span>).mapPartitions(iterator =&gt; &#123;</span><br><span class="line">  <span class="keyword">val</span> buffer = <span class="keyword">new</span> <span class="type">ListBuffer</span>[<span class="type">Int</span>]</span><br><span class="line">  <span class="keyword">while</span> (iterator.hasNext) &#123;</span><br><span class="line">    buffer.append(iterator.next() * <span class="number">100</span>)</span><br><span class="line">  &#125;</span><br><span class="line">  buffer.toIterator</span><br><span class="line">&#125;).foreach(println)</span><br><span class="line"><span class="comment">//输出结果</span></span><br><span class="line"><span class="number">100</span> <span class="number">200</span> <span class="number">300</span> <span class="number">400</span> <span class="number">500</span> <span class="number">600</span></span><br></pre></td></tr></table></figure>
<h3 id="1-5-mapPartitionsWithIndex"><a href="#1-5-mapPartitionsWithIndex" class="headerlink" title="1.5 mapPartitionsWithIndex"></a>1.5 mapPartitionsWithIndex</h3><p>  与 mapPartitions 类似，但 <em>func</em> 类型为 <code>(Int, Iterator&lt;T&gt;) =&gt; Iterator&lt;U&gt;</code> ，其中第一个参数为分区索引。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> list = <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>)</span><br><span class="line">sc.parallelize(list, <span class="number">3</span>).mapPartitionsWithIndex((index, iterator) =&gt; &#123;</span><br><span class="line">  <span class="keyword">val</span> buffer = <span class="keyword">new</span> <span class="type">ListBuffer</span>[<span class="type">String</span>]</span><br><span class="line">  <span class="keyword">while</span> (iterator.hasNext) &#123;</span><br><span class="line">    buffer.append(index + <span class="string">"分区:"</span> + iterator.next() * <span class="number">100</span>)</span><br><span class="line">  &#125;</span><br><span class="line">  buffer.toIterator</span><br><span class="line">&#125;).foreach(println)</span><br><span class="line"><span class="comment">//输出</span></span><br><span class="line"><span class="number">0</span> 分区:<span class="number">100</span></span><br><span class="line"><span class="number">0</span> 分区:<span class="number">200</span></span><br><span class="line"><span class="number">1</span> 分区:<span class="number">300</span></span><br><span class="line"><span class="number">1</span> 分区:<span class="number">400</span></span><br><span class="line"><span class="number">2</span> 分区:<span class="number">500</span></span><br><span class="line"><span class="number">2</span> 分区:<span class="number">600</span></span><br></pre></td></tr></table></figure>
<h3 id="1-6-sample"><a href="#1-6-sample" class="headerlink" title="1.6 sample"></a>1.6 sample</h3><p>  数据采样。有三个可选参数：设置是否放回 (withReplacement)、采样的百分比 (fraction)、随机数生成器的种子 (seed) ：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> list = <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>)</span><br><span class="line">sc.parallelize(list).sample(withReplacement = <span class="literal">false</span>, fraction = <span class="number">0.5</span>).foreach(println)</span><br></pre></td></tr></table></figure>
<h3 id="1-7-union"><a href="#1-7-union" class="headerlink" title="1.7 union"></a>1.7 union</h3><p>合并两个 RDD：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> list1 = <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="keyword">val</span> list2 = <span class="type">List</span>(<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>)</span><br><span class="line">sc.parallelize(list1).union(sc.parallelize(list2)).foreach(println)</span><br><span class="line"><span class="comment">// 输出: 1 2 3 4 5 6</span></span><br></pre></td></tr></table></figure>
<h3 id="1-8-intersection"><a href="#1-8-intersection" class="headerlink" title="1.8 intersection"></a>1.8 intersection</h3><p>求两个 RDD 的交集：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> list1 = <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>)</span><br><span class="line"><span class="keyword">val</span> list2 = <span class="type">List</span>(<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>)</span><br><span class="line">sc.parallelize(list1).intersection(sc.parallelize(list2)).foreach(println)</span><br><span class="line"><span class="comment">// 输出:  4 5</span></span><br></pre></td></tr></table></figure>
<h3 id="1-9-distinct"><a href="#1-9-distinct" class="headerlink" title="1.9 distinct"></a>1.9 distinct</h3><p>去重：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> list = <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line">sc.parallelize(list).distinct().foreach(println)</span><br><span class="line"><span class="comment">// 输出: 4 1 2</span></span><br></pre></td></tr></table></figure>
<h3 id="1-10-groupByKey"><a href="#1-10-groupByKey" class="headerlink" title="1.10 groupByKey"></a>1.10 groupByKey</h3><p>按照键进行分组：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> list = <span class="type">List</span>((<span class="string">"hadoop"</span>, <span class="number">2</span>), (<span class="string">"spark"</span>, <span class="number">3</span>), (<span class="string">"spark"</span>, <span class="number">5</span>), (<span class="string">"storm"</span>, <span class="number">6</span>), (<span class="string">"hadoop"</span>, <span class="number">2</span>))</span><br><span class="line">sc.parallelize(list).groupByKey().map(x =&gt; (x._1, x._2.toList)).foreach(println)</span><br><span class="line"></span><br><span class="line"><span class="comment">//输出：</span></span><br><span class="line">(spark,<span class="type">List</span>(<span class="number">3</span>, <span class="number">5</span>))</span><br><span class="line">(hadoop,<span class="type">List</span>(<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">(storm,<span class="type">List</span>(<span class="number">6</span>))</span><br></pre></td></tr></table></figure>
<h3 id="1-11-reduceByKey"><a href="#1-11-reduceByKey" class="headerlink" title="1.11 reduceByKey"></a>1.11 reduceByKey</h3><p>按照键进行归约操作：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> list = <span class="type">List</span>((<span class="string">"hadoop"</span>, <span class="number">2</span>), (<span class="string">"spark"</span>, <span class="number">3</span>), (<span class="string">"spark"</span>, <span class="number">5</span>), (<span class="string">"storm"</span>, <span class="number">6</span>), (<span class="string">"hadoop"</span>, <span class="number">2</span>))</span><br><span class="line">sc.parallelize(list).reduceByKey(_ + _).foreach(println)</span><br><span class="line"></span><br><span class="line"><span class="comment">//输出</span></span><br><span class="line">(spark,<span class="number">8</span>)</span><br><span class="line">(hadoop,<span class="number">4</span>)</span><br><span class="line">(storm,<span class="number">6</span>)</span><br></pre></td></tr></table></figure>
<h3 id="1-12-sortBy-amp-sortByKey"><a href="#1-12-sortBy-amp-sortByKey" class="headerlink" title="1.12 sortBy &amp; sortByKey"></a>1.12 sortBy &amp; sortByKey</h3><p>按照键进行排序：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> list01 = <span class="type">List</span>((<span class="number">100</span>, <span class="string">"hadoop"</span>), (<span class="number">90</span>, <span class="string">"spark"</span>), (<span class="number">120</span>, <span class="string">"storm"</span>))</span><br><span class="line">sc.parallelize(list01).sortByKey(ascending = <span class="literal">false</span>).foreach(println)</span><br><span class="line"><span class="comment">// 输出</span></span><br><span class="line">(<span class="number">120</span>,storm)</span><br><span class="line">(<span class="number">90</span>,spark)</span><br><span class="line">(<span class="number">100</span>,hadoop)</span><br></pre></td></tr></table></figure>
<p>按照指定元素进行排序：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> list02 = <span class="type">List</span>((<span class="string">"hadoop"</span>,<span class="number">100</span>), (<span class="string">"spark"</span>,<span class="number">90</span>), (<span class="string">"storm"</span>,<span class="number">120</span>))</span><br><span class="line">sc.parallelize(list02).sortBy(x=&gt;x._2,ascending=<span class="literal">false</span>).foreach(println)</span><br><span class="line"><span class="comment">// 输出</span></span><br><span class="line">(storm,<span class="number">120</span>)</span><br><span class="line">(hadoop,<span class="number">100</span>)</span><br><span class="line">(spark,<span class="number">90</span>)</span><br></pre></td></tr></table></figure>
<h3 id="1-13-join"><a href="#1-13-join" class="headerlink" title="1.13 join"></a>1.13 join</h3><p>在一个 (K, V) 和 (K, W) 类型的 Dataset 上调用时，返回一个 (K, (V, W)) 的 Dataset，等价于内连接操作。如果想要执行外连接，可以使用 <code>leftOuterJoin</code>, <code>rightOuterJoin</code> 和 <code>fullOuterJoin</code> 等算子。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> list01 = <span class="type">List</span>((<span class="number">1</span>, <span class="string">"student01"</span>), (<span class="number">2</span>, <span class="string">"student02"</span>), (<span class="number">3</span>, <span class="string">"student03"</span>))</span><br><span class="line"><span class="keyword">val</span> list02 = <span class="type">List</span>((<span class="number">1</span>, <span class="string">"teacher01"</span>), (<span class="number">2</span>, <span class="string">"teacher02"</span>), (<span class="number">3</span>, <span class="string">"teacher03"</span>))</span><br><span class="line">sc.parallelize(list01).join(sc.parallelize(list02)).foreach(println)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出</span></span><br><span class="line">(<span class="number">1</span>,(student01,teacher01))</span><br><span class="line">(<span class="number">3</span>,(student03,teacher03))</span><br><span class="line">(<span class="number">2</span>,(student02,teacher02))</span><br></pre></td></tr></table></figure>
<h3 id="1-14-cogroup"><a href="#1-14-cogroup" class="headerlink" title="1.14 cogroup"></a>1.14 cogroup</h3><p>在一个 (K, V) 对的 Dataset 上调用时，返回多个类型为 (K, (Iterable\<v>, Iterable\<w>)) 的元组所组成的 Dataset。</w></v></p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> list01 = <span class="type">List</span>((<span class="number">1</span>, <span class="string">"a"</span>),(<span class="number">1</span>, <span class="string">"a"</span>), (<span class="number">2</span>, <span class="string">"b"</span>), (<span class="number">3</span>, <span class="string">"e"</span>))</span><br><span class="line"><span class="keyword">val</span> list02 = <span class="type">List</span>((<span class="number">1</span>, <span class="string">"A"</span>), (<span class="number">2</span>, <span class="string">"B"</span>), (<span class="number">3</span>, <span class="string">"E"</span>))</span><br><span class="line"><span class="keyword">val</span> list03 = <span class="type">List</span>((<span class="number">1</span>, <span class="string">"[ab]"</span>), (<span class="number">2</span>, <span class="string">"[bB]"</span>), (<span class="number">3</span>, <span class="string">"eE"</span>),(<span class="number">3</span>, <span class="string">"eE"</span>))</span><br><span class="line">sc.parallelize(list01).cogroup(sc.parallelize(list02),sc.parallelize(list03)).foreach(println)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出： 同一个 RDD 中的元素先按照 key 进行分组，然后再对不同 RDD 中的元素按照 key 进行分组</span></span><br><span class="line">(<span class="number">1</span>,(<span class="type">CompactBuffer</span>(a, a),<span class="type">CompactBuffer</span>(<span class="type">A</span>),<span class="type">CompactBuffer</span>([ab])))</span><br><span class="line">(<span class="number">3</span>,(<span class="type">CompactBuffer</span>(e),<span class="type">CompactBuffer</span>(<span class="type">E</span>),<span class="type">CompactBuffer</span>(eE, eE)))</span><br><span class="line">(<span class="number">2</span>,(<span class="type">CompactBuffer</span>(b),<span class="type">CompactBuffer</span>(<span class="type">B</span>),<span class="type">CompactBuffer</span>([bB])))</span><br></pre></td></tr></table></figure>
<h3 id="1-15-cartesian"><a href="#1-15-cartesian" class="headerlink" title="1.15 cartesian"></a>1.15 cartesian</h3><p>计算笛卡尔积：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> list1 = <span class="type">List</span>(<span class="string">"A"</span>, <span class="string">"B"</span>, <span class="string">"C"</span>)</span><br><span class="line"><span class="keyword">val</span> list2 = <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">sc.parallelize(list1).cartesian(sc.parallelize(list2)).foreach(println)</span><br><span class="line"></span><br><span class="line"><span class="comment">//输出笛卡尔积</span></span><br><span class="line">(<span class="type">A</span>,<span class="number">1</span>)</span><br><span class="line">(<span class="type">A</span>,<span class="number">2</span>)</span><br><span class="line">(<span class="type">A</span>,<span class="number">3</span>)</span><br><span class="line">(<span class="type">B</span>,<span class="number">1</span>)</span><br><span class="line">(<span class="type">B</span>,<span class="number">2</span>)</span><br><span class="line">(<span class="type">B</span>,<span class="number">3</span>)</span><br><span class="line">(<span class="type">C</span>,<span class="number">1</span>)</span><br><span class="line">(<span class="type">C</span>,<span class="number">2</span>)</span><br><span class="line">(<span class="type">C</span>,<span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<h3 id="1-16-aggregateByKey"><a href="#1-16-aggregateByKey" class="headerlink" title="1.16 aggregateByKey"></a>1.16 aggregateByKey</h3><p>当调用（K，V）对的数据集时，返回（K，U）对的数据集，其中使用给定的组合函数和 zeroValue 聚合每个键的值。与 <code>groupByKey</code> 类似，reduce 任务的数量可通过第二个参数 <code>numPartitions</code> 进行配置。示例如下：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 为了清晰，以下所有参数均使用具名传参</span></span><br><span class="line"><span class="keyword">val</span> list = <span class="type">List</span>((<span class="string">"hadoop"</span>, <span class="number">3</span>), (<span class="string">"hadoop"</span>, <span class="number">2</span>), (<span class="string">"spark"</span>, <span class="number">4</span>), (<span class="string">"spark"</span>, <span class="number">3</span>), (<span class="string">"storm"</span>, <span class="number">6</span>), (<span class="string">"storm"</span>, <span class="number">8</span>))</span><br><span class="line">sc.parallelize(list,numSlices = <span class="number">2</span>).aggregateByKey(zeroValue = <span class="number">0</span>,numPartitions = <span class="number">3</span>)(</span><br><span class="line">      seqOp = math.max(_, _),</span><br><span class="line">      combOp = _ + _</span><br><span class="line">    ).collect.foreach(println)</span><br><span class="line"><span class="comment">//输出结果：</span></span><br><span class="line">(hadoop,<span class="number">3</span>)</span><br><span class="line">(storm,<span class="number">8</span>)</span><br><span class="line">(spark,<span class="number">7</span>)</span><br></pre></td></tr></table></figure>
<p>这里使用了 <code>numSlices = 2</code> 指定 aggregateByKey 父操作 parallelize 的分区数量为 2，其执行流程如下：</p>
<div align="center"> <img src="../pictures/spark-aggregateByKey.png"> </div>

<p>基于同样的执行流程，如果 <code>numSlices = 1</code>，则意味着只有输入一个分区，则其最后一步 combOp 相当于是无效的，执行结果为：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(hadoop,3)</span><br><span class="line">(storm,8)</span><br><span class="line">(spark,4)</span><br></pre></td></tr></table></figure>
<p>同样的，如果每个单词对一个分区，即 <code>numSlices = 6</code>，此时相当于求和操作，执行结果为：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(hadoop,5)</span><br><span class="line">(storm,14)</span><br><span class="line">(spark,7)</span><br></pre></td></tr></table></figure>
<p><code>aggregateByKey(zeroValue = 0,numPartitions = 3)</code> 的第二个参数 <code>numPartitions</code> 决定的是输出 RDD 的分区数量，想要验证这个问题，可以对上面代码进行改写，使用 <code>getNumPartitions</code> 方法获取分区数量：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">sc.parallelize(list,numSlices = <span class="number">6</span>).aggregateByKey(zeroValue = <span class="number">0</span>,numPartitions = <span class="number">3</span>)(</span><br><span class="line">  seqOp = math.max(_, _),</span><br><span class="line">  combOp = _ + _</span><br><span class="line">).getNumPartitions</span><br></pre></td></tr></table></figure>
<div align="center"> <img src="../pictures/spark-getpartnum.png"> </div>

<h2 id="二、Action"><a href="#二、Action" class="headerlink" title="二、Action"></a>二、Action</h2><p>Spark 常用的 Action 算子如下：</p>
<table>
<thead>
<tr>
<th>Action（动作）</th>
<th>Meaning（含义）</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>reduce</strong>(<em>func</em>)</td>
<td>使用函数<em>func</em>执行归约操作</td>
</tr>
<tr>
<td><strong>collect</strong>()</td>
<td>以一个 array 数组的形式返回 dataset 的所有元素，适用于小结果集。</td>
</tr>
<tr>
<td><strong>count</strong>()</td>
<td>返回 dataset 中元素的个数。</td>
</tr>
<tr>
<td><strong>first</strong>()</td>
<td>返回 dataset 中的第一个元素，等价于 take(1)。</td>
</tr>
<tr>
<td><strong>take</strong>(<em>n</em>)</td>
<td>将数据集中的前 <em>n</em> 个元素作为一个 array 数组返回。</td>
</tr>
<tr>
<td><strong>takeSample</strong>(<em>withReplacement</em>, <em>num</em>, [<em>seed</em>])</td>
<td>对一个 dataset 进行随机抽样</td>
</tr>
<tr>
<td><strong>takeOrdered</strong>(<em>n</em>, <em>[ordering]</em>)</td>
<td>按自然顺序（natural order）或自定义比较器（custom comparator）排序后返回前 <em>n</em> 个元素。只适用于小结果集，因为所有数据都会被加载到驱动程序的内存中进行排序。</td>
</tr>
<tr>
<td><strong>saveAsTextFile</strong>(<em>path</em>)</td>
<td>将 dataset 中的元素以文本文件的形式写入本地文件系统、HDFS 或其它 Hadoop 支持的文件系统中。Spark 将对每个元素调用 toString 方法，将元素转换为文本文件中的一行记录。</td>
</tr>
<tr>
<td><strong>saveAsSequenceFile</strong>(<em>path</em>)</td>
<td>将 dataset 中的元素以 Hadoop SequenceFile 的形式写入到本地文件系统、HDFS 或其它 Hadoop 支持的文件系统中。该操作要求 RDD 中的元素需要实现 Hadoop 的 Writable 接口。对于 Scala 语言而言，它可以将 Spark 中的基本数据类型自动隐式转换为对应 Writable 类型。(目前仅支持 Java and Scala)</td>
</tr>
<tr>
<td><strong>saveAsObjectFile</strong>(<em>path</em>)</td>
<td>使用 Java 序列化后存储，可以使用 <code>SparkContext.objectFile()</code> 进行加载。(目前仅支持 Java and Scala)</td>
</tr>
<tr>
<td><strong>countByKey</strong>()</td>
<td>计算每个键出现的次数。</td>
</tr>
<tr>
<td><strong>foreach</strong>(<em>func</em>)</td>
<td>遍历 RDD 中每个元素，并对其执行<em>fun</em>函数</td>
</tr>
</tbody>
</table>
<h3 id="2-1-reduce"><a href="#2-1-reduce" class="headerlink" title="2.1 reduce"></a>2.1 reduce</h3><p>使用函数<em>func</em>执行归约操作：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"> <span class="keyword">val</span> list = <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>)</span><br><span class="line">sc.parallelize(list).reduce((x, y) =&gt; x + y)</span><br><span class="line">sc.parallelize(list).reduce(_ + _)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出 15</span></span><br></pre></td></tr></table></figure>
<h3 id="2-2-takeOrdered"><a href="#2-2-takeOrdered" class="headerlink" title="2.2 takeOrdered"></a>2.2 takeOrdered</h3><p>按自然顺序（natural order）或自定义比较器（custom comparator）排序后返回前 <em>n</em> 个元素。需要注意的是 <code>takeOrdered</code> 使用隐式参数进行隐式转换，以下为其源码。所以在使用自定义排序时，需要继承 <code>Ordering[T]</code> 实现自定义比较器，然后将其作为隐式参数引入。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">takeOrdered</span></span>(num: <span class="type">Int</span>)(<span class="keyword">implicit</span> ord: <span class="type">Ordering</span>[<span class="type">T</span>]): <span class="type">Array</span>[<span class="type">T</span>] = withScope &#123;</span><br><span class="line">  .........</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>自定义规则排序：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 继承 Ordering[T],实现自定义比较器，按照 value 值的长度进行排序</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CustomOrdering</span> <span class="keyword">extends</span> <span class="title">Ordering</span>[(<span class="type">Int</span>, <span class="type">String</span>)] </span>&#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">compare</span></span>(x: (<span class="type">Int</span>, <span class="type">String</span>), y: (<span class="type">Int</span>, <span class="type">String</span>)): <span class="type">Int</span></span><br><span class="line">    = <span class="keyword">if</span> (x._2.length &gt; y._2.length) <span class="number">1</span> <span class="keyword">else</span> <span class="number">-1</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> list = <span class="type">List</span>((<span class="number">1</span>, <span class="string">"hadoop"</span>), (<span class="number">1</span>, <span class="string">"storm"</span>), (<span class="number">1</span>, <span class="string">"azkaban"</span>), (<span class="number">1</span>, <span class="string">"hive"</span>))</span><br><span class="line"><span class="comment">//  引入隐式默认值</span></span><br><span class="line"><span class="keyword">implicit</span> <span class="keyword">val</span> implicitOrdering = <span class="keyword">new</span> <span class="type">CustomOrdering</span></span><br><span class="line">sc.parallelize(list).takeOrdered(<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出： Array((1,hive), (1,storm), (1,hadoop), (1,azkaban)</span></span><br></pre></td></tr></table></figure>
<h3 id="2-3-countByKey"><a href="#2-3-countByKey" class="headerlink" title="2.3 countByKey"></a>2.3 countByKey</h3><p>计算每个键出现的次数：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> list = <span class="type">List</span>((<span class="string">"hadoop"</span>, <span class="number">10</span>), (<span class="string">"hadoop"</span>, <span class="number">10</span>), (<span class="string">"storm"</span>, <span class="number">3</span>), (<span class="string">"storm"</span>, <span class="number">3</span>), (<span class="string">"azkaban"</span>, <span class="number">1</span>))</span><br><span class="line">sc.parallelize(list).countByKey()</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出： Map(hadoop -&gt; 2, storm -&gt; 2, azkaban -&gt; 1)</span></span><br></pre></td></tr></table></figure>
<h3 id="2-4-saveAsTextFile"><a href="#2-4-saveAsTextFile" class="headerlink" title="2.4 saveAsTextFile"></a>2.4 saveAsTextFile</h3><p>将 dataset 中的元素以文本文件的形式写入本地文件系统、HDFS 或其它 Hadoop 支持的文件系统中。Spark 将对每个元素调用 toString 方法，将元素转换为文本文件中的一行记录。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> list = <span class="type">List</span>((<span class="string">"hadoop"</span>, <span class="number">10</span>), (<span class="string">"hadoop"</span>, <span class="number">10</span>), (<span class="string">"storm"</span>, <span class="number">3</span>), (<span class="string">"storm"</span>, <span class="number">3</span>), (<span class="string">"azkaban"</span>, <span class="number">1</span>))</span><br><span class="line">sc.parallelize(list).saveAsTextFile(<span class="string">"/usr/file/temp"</span>)</span><br></pre></td></tr></table></figure>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="http://spark.apache.org/docs/latest/rdd-programming-guide.html#rdd-programming-guide" target="_blank" rel="noopener">RDD Programming Guide</a></p>
]]></content>
      <categories>
        <category>notes</category>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark部署模式与作业提交</title>
    <url>/2021/03/17/Spark%E9%83%A8%E7%BD%B2%E6%A8%A1%E5%BC%8F%E4%B8%8E%E4%BD%9C%E4%B8%9A%E6%8F%90%E4%BA%A4/</url>
    <content><![CDATA[<h2 id="一、作业提交"><a href="#一、作业提交" class="headerlink" title="一、作业提交"></a>一、作业提交</h2><h3 id="1-1-spark-submit"><a href="#1-1-spark-submit" class="headerlink" title="1.1  spark-submit"></a>1.1  spark-submit</h3><p>Spark 所有模式均使用 <code>spark-submit</code> 命令提交作业，其格式如下：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">./bin/spark-submit \</span><br><span class="line">  --class &lt;main-class&gt; \        # 应用程序主入口类</span><br><span class="line">  --master &lt;master-url&gt; \       # 集群的 Master Url</span><br><span class="line">  --deploy-mode &lt;deploy-mode&gt; \ # 部署模式</span><br><span class="line">  --conf &lt;key&gt;=&lt;value&gt; \        # 可选配置       </span><br><span class="line">  ... # other options    </span><br><span class="line">  &lt;application-jar&gt; \           # Jar 包路径 </span><br><span class="line">  [application-arguments]       #传递给主入口类的参数</span><br></pre></td></tr></table></figure>
<p>需要注意的是：在集群环境下，<code>application-jar</code> 必须能被集群中所有节点都能访问，可以是 HDFS 上的路径；也可以是本地文件系统路径，如果是本地文件系统路径，则要求集群中每一个机器节点上的相同路径都存在该 Jar 包。</p>
<h3 id="1-2-deploy-mode"><a href="#1-2-deploy-mode" class="headerlink" title="1.2 deploy-mode"></a>1.2 deploy-mode</h3><p>deploy-mode 有 <code>cluster</code> 和 <code>client</code> 两个可选参数，默认为 <code>client</code>。这里以 Spark On Yarn 模式对两者进行说明 ：</p>
<ul>
<li>在 cluster 模式下，Spark Drvier 在应用程序的 Master 进程内运行，该进程由群集上的 YARN 管理，提交作业的客户端可以在启动应用程序后关闭；</li>
<li>在 client 模式下，Spark Drvier 在提交作业的客户端进程中运行，Master 进程仅用于从 YARN 请求资源。</li>
</ul>
<h3 id="1-3-master-url"><a href="#1-3-master-url" class="headerlink" title="1.3 master-url"></a>1.3 master-url</h3><p>master-url 的所有可选参数如下表所示：</p>
<table>
<thead>
<tr>
<th>Master URL</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>local</code></td>
<td>使用一个线程本地运行 Spark</td>
</tr>
<tr>
<td><code>local[K]</code></td>
<td>使用 K 个 worker 线程本地运行 Spark</td>
</tr>
<tr>
<td><code>local[K,F]</code></td>
<td>使用 K 个 worker 线程本地运行 , 第二个参数为 Task 的失败重试次数</td>
</tr>
<tr>
<td><code>local[*]</code></td>
<td>使用与 CPU 核心数一样的线程数在本地运行 Spark</td>
</tr>
<tr>
<td><code>local[*,F]</code></td>
<td>使用与 CPU 核心数一样的线程数在本地运行 Spark<br>第二个参数为 Task 的失败重试次数</td>
</tr>
<tr>
<td><code>spark://HOST:PORT</code></td>
<td>连接至指定的 standalone 集群的 master 节点。端口号默认是 7077。</td>
</tr>
<tr>
<td><code>spark://HOST1:PORT1,HOST2:PORT2</code></td>
<td>如果 standalone 集群采用 Zookeeper 实现高可用，则必须包含由 zookeeper 设置的所有 master 主机地址。</td>
</tr>
<tr>
<td><code>mesos://HOST:PORT</code></td>
<td>连接至给定的 Mesos 集群。端口默认是 5050。对于使用了 ZooKeeper 的 Mesos cluster 来说，使用 <code>mesos://zk://...</code> 来指定地址，使用 <code>--deploy-mode cluster</code> 模式来提交。</td>
</tr>
<tr>
<td><code>yarn</code></td>
<td>连接至一个 YARN 集群，集群由配置的 <code>HADOOP_CONF_DIR</code> 或者 <code>YARN_CONF_DIR</code> 来决定。使用 <code>--deploy-mode</code> 参数来配置 <code>client</code> 或 <code>cluster</code> 模式。</td>
</tr>
</tbody>
</table>
<p>下面主要介绍三种常用部署模式及对应的作业提交方式。</p>
<h2 id="二、Local模式"><a href="#二、Local模式" class="headerlink" title="二、Local模式"></a>二、Local模式</h2><p>Local 模式下提交作业最为简单，不需要进行任何配置，提交命令如下：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 本地模式提交应用</span></span><br><span class="line">spark-submit \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \</span><br><span class="line">--master local[2] \</span><br><span class="line">/usr/app/spark-2.4.0-bin-hadoop2.6/examples/jars/spark-examples_2.11-2.4.0.jar \</span><br><span class="line">100   # 传给 SparkPi 的参数</span><br></pre></td></tr></table></figure>
<p><code>spark-examples_2.11-2.4.0.jar</code> 是 Spark 提供的测试用例包，<code>SparkPi</code> 用于计算 Pi 值，执行结果如下：</p>
<div align="center"> <img src="../pictures/spark-pi.png"> </div>



<h2 id="三、Standalone模式"><a href="#三、Standalone模式" class="headerlink" title="三、Standalone模式"></a>三、Standalone模式</h2><p>Standalone 是 Spark 提供的一种内置的集群模式，采用内置的资源管理器进行管理。下面按照如图所示演示 1 个 Mater 和 2 个 Worker 节点的集群配置，这里使用两台主机进行演示：</p>
<ul>
<li>hadoop001： 由于只有两台主机，所以 hadoop001 既是 Master 节点，也是 Worker 节点;</li>
<li>hadoop002 ： Worker 节点。</li>
</ul>
<div align="center"> <img src="../pictures/spark-集群模式.png"> </div>

<h3 id="3-1-环境配置"><a href="#3-1-环境配置" class="headerlink" title="3.1 环境配置"></a>3.1 环境配置</h3><p>首先需要保证 Spark 已经解压在两台主机的相同路径上。然后进入 hadoop001 的 <code>${SPARK_HOME}/conf/</code> 目录下，拷贝配置样本并进行相关配置：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> cp spark-env.sh.template spark-env.sh</span></span><br></pre></td></tr></table></figure>
<p>在 <code>spark-env.sh</code> 中配置 JDK 的目录，完成后将该配置使用 scp 命令分发到 hadoop002 上：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> JDK安装位置</span></span><br><span class="line">JAVA_HOME=/usr/java/jdk1.8.0_201</span><br></pre></td></tr></table></figure>
<h3 id="3-2-集群配置"><a href="#3-2-集群配置" class="headerlink" title="3.2 集群配置"></a>3.2 集群配置</h3><p>在 <code>${SPARK_HOME}/conf/</code> 目录下，拷贝集群配置样本并进行相关配置：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># cp slaves.template slaves</span><br></pre></td></tr></table></figure>
<p>指定所有 Worker 节点的主机名：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> A Spark Worker will be started on each of the machines listed below.</span></span><br><span class="line">hadoop001</span><br><span class="line">hadoop002</span><br></pre></td></tr></table></figure>
<p>这里需要注意以下三点：</p>
<ul>
<li>主机名与 IP 地址的映射必须在 <code>/etc/hosts</code> 文件中已经配置，否则就直接使用 IP 地址；</li>
<li>每个主机名必须独占一行；</li>
<li>Spark 的 Master 主机是通过 SSH 访问所有的 Worker 节点，所以需要预先配置免密登录。</li>
</ul>
<h3 id="3-3-启动"><a href="#3-3-启动" class="headerlink" title="3.3 启动"></a>3.3 启动</h3><p>使用 <code>start-all.sh</code> 代表启动 Master 和所有 Worker 服务。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">./sbin/start-master.sh</span><br></pre></td></tr></table></figure>
<p>访问 8080 端口，查看 Spark 的 Web-UI 界面,，此时应该显示有两个有效的工作节点：</p>
<div align="center"> <img src="../pictures/spark-Standalone-web-ui.png"> </div>

<h3 id="3-4-提交作业"><a href="#3-4-提交作业" class="headerlink" title="3.4 提交作业"></a>3.4 提交作业</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 以client模式提交到standalone集群 </span></span><br><span class="line">spark-submit \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \</span><br><span class="line">--master spark://hadoop001:7077 \</span><br><span class="line">--executor-memory 2G \</span><br><span class="line">--total-executor-cores 10 \</span><br><span class="line">/usr/app/spark-2.4.0-bin-hadoop2.6/examples/jars/spark-examples_2.11-2.4.0.jar \</span><br><span class="line">100</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 以cluster模式提交到standalone集群 </span></span><br><span class="line">spark-submit \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \</span><br><span class="line">--master spark://207.184.161.138:7077 \</span><br><span class="line">--deploy-mode cluster \</span><br><span class="line">--supervise \  # 配置此参数代表开启监督，如果主应用程序异常退出，则自动重启 Driver</span><br><span class="line">--executor-memory 2G \</span><br><span class="line">--total-executor-cores 10 \</span><br><span class="line">/usr/app/spark-2.4.0-bin-hadoop2.6/examples/jars/spark-examples_2.11-2.4.0.jar \</span><br><span class="line">100</span><br></pre></td></tr></table></figure>
<h3 id="3-5-可选配置"><a href="#3-5-可选配置" class="headerlink" title="3.5 可选配置"></a>3.5 可选配置</h3><p>在虚拟机上提交作业时经常出现一个的问题是作业无法申请到足够的资源：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Initial job has not accepted any resources; </span><br><span class="line">check your cluster UI to ensure that workers are registered and have sufficient resources</span><br></pre></td></tr></table></figure>
<div align="center"> <img src="../pictures/spark-内存不足2.png"> </div>

<p><br></p>
<p>这时候可以查看 Web UI，我这里是内存空间不足：提交命令中要求作业的 <code>executor-memory</code> 是 2G，但是实际的工作节点的 <code>Memory</code> 只有 1G，这时候你可以修改 <code>--executor-memory</code>，也可以修改 Woker 的 <code>Memory</code>，其默认值为主机所有可用内存值减去 1G。</p>
<div align="center"> <img src="../pictures/spark-内存不足.png"> </div>   

<p><br></p>
<p>关于 Master 和 Woker 节点的所有可选配置如下，可以在 <code>spark-env.sh</code> 中进行对应的配置：    </p>
<table>
<thead>
<tr>
<th>Environment Variable（环境变量）</th>
<th>Meaning（含义）</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>SPARK_MASTER_HOST</code></td>
<td>master 节点地址</td>
</tr>
<tr>
<td><code>SPARK_MASTER_PORT</code></td>
<td>master 节点地址端口（默认：7077）</td>
</tr>
<tr>
<td><code>SPARK_MASTER_WEBUI_PORT</code></td>
<td>master 的 web UI 的端口（默认：8080）</td>
</tr>
<tr>
<td><code>SPARK_MASTER_OPTS</code></td>
<td>仅用于 master 的配置属性，格式是 “-Dx=y”（默认：none）,所有属性可以参考官方文档：<a href="https://spark.apache.org/docs/latest/spark-standalone.html#spark-standalone-mode" target="_blank" rel="noopener">spark-standalone-mode</a></td>
</tr>
<tr>
<td><code>SPARK_LOCAL_DIRS</code></td>
<td>spark 的临时存储的目录，用于暂存 map 的输出和持久化存储 RDDs。多个目录用逗号分隔</td>
</tr>
<tr>
<td><code>SPARK_WORKER_CORES</code></td>
<td>spark worker 节点可以使用 CPU Cores 的数量。（默认：全部可用）</td>
</tr>
<tr>
<td><code>SPARK_WORKER_MEMORY</code></td>
<td>spark worker 节点可以使用的内存数量（默认：全部的内存减去 1GB）；</td>
</tr>
<tr>
<td><code>SPARK_WORKER_PORT</code></td>
<td>spark worker 节点的端口（默认： random（随机））</td>
</tr>
<tr>
<td><code>SPARK_WORKER_WEBUI_PORT</code></td>
<td>worker 的 web UI 的 Port（端口）（默认：8081）</td>
</tr>
<tr>
<td><code>SPARK_WORKER_DIR</code></td>
<td>worker 运行应用程序的目录，这个目录中包含日志和暂存空间（default：SPARK_HOME/work）</td>
</tr>
<tr>
<td><code>SPARK_WORKER_OPTS</code></td>
<td>仅用于 worker 的配置属性，格式是 “-Dx=y”（默认：none）。所有属性可以参考官方文档：<a href="https://spark.apache.org/docs/latest/spark-standalone.html#spark-standalone-mode" target="_blank" rel="noopener">spark-standalone-mode</a></td>
</tr>
<tr>
<td><code>SPARK_DAEMON_MEMORY</code></td>
<td>分配给 spark master 和 worker 守护进程的内存。（默认： 1G）</td>
</tr>
<tr>
<td><code>SPARK_DAEMON_JAVA_OPTS</code></td>
<td>spark master 和 worker 守护进程的 JVM 选项，格式是 “-Dx=y”（默认：none）</td>
</tr>
<tr>
<td><code>SPARK_PUBLIC_DNS</code></td>
<td>spark master 和 worker 的公开 DNS 名称。（默认：none）</td>
</tr>
</tbody>
</table>
<h2 id="三、Spark-on-Yarn模式"><a href="#三、Spark-on-Yarn模式" class="headerlink" title="三、Spark on Yarn模式"></a>三、Spark on Yarn模式</h2><p>Spark 支持将作业提交到 Yarn 上运行，此时不需要启动 Master 节点，也不需要启动 Worker 节点。</p>
<h3 id="3-1-配置"><a href="#3-1-配置" class="headerlink" title="3.1 配置"></a>3.1 配置</h3><p>在 <code>spark-env.sh</code> 中配置 hadoop 的配置目录的位置，可以使用 <code>YARN_CONF_DIR</code> 或 <code>HADOOP_CONF_DIR</code> 进行指定：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">YARN_CONF_DIR=/usr/app/hadoop-2.6.0-cdh5.15.2/etc/hadoop</span><br><span class="line"># JDK安装位置</span><br><span class="line">JAVA_HOME=/usr/java/jdk1.8.0_201</span><br></pre></td></tr></table></figure>
<h3 id="3-2-启动"><a href="#3-2-启动" class="headerlink" title="3.2 启动"></a>3.2 启动</h3><p>必须要保证 Hadoop 已经启动，这里包括 YARN 和 HDFS 都需要启动，因为在计算过程中 Spark 会使用 HDFS 存储临时文件，如果 HDFS 没有启动，则会抛出异常。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> start-yarn.sh</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> start-dfs.sh</span></span><br></pre></td></tr></table></figure>
<h3 id="3-3-提交应用"><a href="#3-3-提交应用" class="headerlink" title="3.3 提交应用"></a>3.3 提交应用</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">  以client模式提交到yarn集群 </span></span><br><span class="line">spark-submit \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \</span><br><span class="line">--master yarn \</span><br><span class="line">--deploy-mode client \</span><br><span class="line">--executor-memory 2G \</span><br><span class="line">--num-executors 10 \</span><br><span class="line">/usr/app/spark-2.4.0-bin-hadoop2.6/examples/jars/spark-examples_2.11-2.4.0.jar \</span><br><span class="line">100</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">  以cluster模式提交到yarn集群 </span></span><br><span class="line">spark-submit \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \</span><br><span class="line">--master yarn \</span><br><span class="line">--deploy-mode cluster \</span><br><span class="line">--executor-memory 2G \</span><br><span class="line">--num-executors 10 \</span><br><span class="line">/usr/app/spark-2.4.0-bin-hadoop2.6/examples/jars/spark-examples_2.11-2.4.0.jar \</span><br><span class="line">100</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>notes</category>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark 累加器与广播变量</title>
    <url>/2021/03/17/Spark%E7%B4%AF%E5%8A%A0%E5%99%A8%E4%B8%8E%E5%B9%BF%E6%92%AD%E5%8F%98%E9%87%8F/</url>
    <content><![CDATA[<h2 id="一、简介"><a href="#一、简介" class="headerlink" title="一、简介"></a>一、简介</h2><p>在 Spark 中，提供了两种类型的共享变量：累加器 (accumulator) 与广播变量 (broadcast variable)：</p>
<ul>
<li><strong>累加器</strong>：用来对信息进行聚合，主要用于累计计数等场景；</li>
<li><strong>广播变量</strong>：主要用于在节点间高效分发大对象。</li>
</ul>
<h2 id="二、累加器"><a href="#二、累加器" class="headerlink" title="二、累加器"></a>二、累加器</h2><p>这里先看一个具体的场景，对于正常的累计求和，如果在集群模式中使用下面的代码进行计算，会发现执行结果并非预期：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">var</span> counter = <span class="number">0</span></span><br><span class="line"><span class="keyword">val</span> data = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>)</span><br><span class="line">sc.parallelize(data).foreach(x =&gt; counter += x)</span><br><span class="line"> println(counter)</span><br></pre></td></tr></table></figure>
<p>counter 最后的结果是 0，导致这个问题的主要原因是闭包。</p>
<div align="center"> <img src="../pictures/spark-累加器1.png"> </div>



<h3 id="2-1-理解闭包"><a href="#2-1-理解闭包" class="headerlink" title="2.1 理解闭包"></a>2.1 理解闭包</h3><p><strong>1. Scala 中闭包的概念</strong></p>
<p>这里先介绍一下 Scala 中关于闭包的概念：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">var more = 10</span><br><span class="line">val addMore = (x: Int) =&gt; x + more</span><br></pre></td></tr></table></figure>
<p>如上函数 <code>addMore</code> 中有两个变量 x 和 more:</p>
<ul>
<li><strong>x</strong> : 是一个绑定变量 (bound variable)，因为其是该函数的入参，在函数的上下文中有明确的定义；</li>
<li><strong>more</strong> : 是一个自由变量 (free variable)，因为函数字面量本生并没有给 more 赋予任何含义。</li>
</ul>
<p>按照定义：在创建函数时，如果需要捕获自由变量，那么包含指向被捕获变量的引用的函数就被称为闭包函数。</p>
<p><strong>2. Spark 中的闭包</strong></p>
<p>在实际计算时，Spark 会将对 RDD 操作分解为 Task，Task 运行在 Worker Node 上。在执行之前，Spark 会对任务进行闭包，如果闭包内涉及到自由变量，则程序会进行拷贝，并将副本变量放在闭包中，之后闭包被序列化并发送给每个执行者。因此，当在 foreach 函数中引用 <code>counter</code> 时，它将不再是 Driver 节点上的 <code>counter</code>，而是闭包中的副本 <code>counter</code>，默认情况下，副本 <code>counter</code> 更新后的值不会回传到 Driver，所以 <code>counter</code> 的最终值仍然为零。</p>
<p>需要注意的是：在 Local 模式下，有可能执行 <code>foreach</code> 的 Worker Node 与 Diver 处在相同的 JVM，并引用相同的原始 <code>counter</code>，这时候更新可能是正确的，但是在集群模式下一定不正确。所以在遇到此类问题时应优先使用累加器。</p>
<p>累加器的原理实际上很简单：就是将每个副本变量的最终值传回 Driver，由 Driver 聚合后得到最终值，并更新原始变量。</p>
<div align="center"> <img src="../pictures/spark-集群模式.png"> </div>

<h3 id="2-2-使用累加器"><a href="#2-2-使用累加器" class="headerlink" title="2.2 使用累加器"></a>2.2 使用累加器</h3><p><code>SparkContext</code> 中定义了所有创建累加器的方法，需要注意的是：被中横线划掉的累加器方法在 Spark 2.0.0 之后被标识为废弃。</p>
<div align="center"> <img src="../pictures/spark-累加器方法.png"> </div>

<p>使用示例和执行结果分别如下：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> data = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>)</span><br><span class="line"><span class="comment">// 定义累加器</span></span><br><span class="line"><span class="keyword">val</span> accum = sc.longAccumulator(<span class="string">"My Accumulator"</span>)</span><br><span class="line">sc.parallelize(data).foreach(x =&gt; accum.add(x))</span><br><span class="line"><span class="comment">// 获取累加器的值</span></span><br><span class="line">accum.value</span><br></pre></td></tr></table></figure>
<div align="center"> <img src="../pictures/spark-累加器2.png"> </div>



<h2 id="三、广播变量"><a href="#三、广播变量" class="headerlink" title="三、广播变量"></a>三、广播变量</h2><p>在上面介绍中闭包的过程中我们说道每个 Task 任务的闭包都会持有自由变量的副本，如果变量很大且 Task 任务很多的情况下，这必然会对网络 IO 造成压力，为了解决这个情况，Spark 提供了广播变量。</p>
<p>广播变量的做法很简单：就是不把副本变量分发到每个 Task 中，而是将其分发到每个 Executor，Executor 中的所有 Task 共享一个副本变量。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 把一个数组定义为一个广播变量</span></span><br><span class="line"><span class="keyword">val</span> broadcastVar = sc.broadcast(<span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>))</span><br><span class="line"><span class="comment">// 之后用到该数组时应优先使用广播变量，而不是原值</span></span><br><span class="line">sc.parallelize(broadcastVar.value).map(_ * <span class="number">10</span>).collect()</span><br></pre></td></tr></table></figure>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="http://spark.apache.org/docs/latest/rdd-programming-guide.html#rdd-programming-guide" target="_blank" rel="noopener">RDD Programming Guide</a></p>
]]></content>
      <categories>
        <category>notes</category>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title>Spring/Spring Boot 整合 Mybatis + Phoenix</title>
    <url>/2021/03/17/Spring+Mybtais+Phoenix%E6%95%B4%E5%90%88/</url>
    <content><![CDATA[<h2 id="一、前言"><a href="#一、前言" class="headerlink" title="一、前言"></a>一、前言</h2><p>使用 Spring+Mybatis 操作 Phoenix 和操作其他的关系型数据库（如 Mysql，Oracle）在配置上是基本相同的，下面会分别给出 Spring/Spring Boot 整合步骤，完整代码见本仓库：</p>
<ul>
<li><a href="https://github.com/heibaiying/BigData-Notes/tree/master/code/Phoenix/spring-mybatis-phoenix" target="_blank" rel="noopener">Spring + Mybatis + Phoenix</a></li>
<li><a href="https://github.com/heibaiying/BigData-Notes/tree/master/code/Phoenix/spring-boot-mybatis-phoenix" target="_blank" rel="noopener">SpringBoot + Mybatis + Phoenix</a></li>
</ul>
<h2 id="二、Spring-Mybatis-Phoenix"><a href="#二、Spring-Mybatis-Phoenix" class="headerlink" title="二、Spring + Mybatis + Phoenix"></a>二、Spring + Mybatis + Phoenix</h2><h3 id="2-1-项目结构"><a href="#2-1-项目结构" class="headerlink" title="2.1 项目结构"></a>2.1 项目结构</h3><div align="center"> <img src="../pictures/spring-mybatis-phoenix.png"> </div>

<h3 id="2-2-主要依赖"><a href="#2-2-主要依赖" class="headerlink" title="2.2 主要依赖"></a>2.2 主要依赖</h3><p>除了 Spring 相关依赖外，还需要导入 <code>phoenix-core</code> 和对应的 Mybatis 依赖包</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!--mybatis 依赖包--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.mybatis<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>mybatis-spring<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.3.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.mybatis<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>mybatis<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.4.6<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--phoenix core--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.phoenix<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>phoenix-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>4.14.0-cdh5.14.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h3 id="2-3-数据库配置文件"><a href="#2-3-数据库配置文件" class="headerlink" title="2.3  数据库配置文件"></a>2.3  数据库配置文件</h3><p>在数据库配置文件 <code>jdbc.properties</code>  中配置数据库驱动和 zookeeper 地址</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 数据库驱动</span><br><span class="line">phoenix.driverClassName=org.apache.phoenix.jdbc.PhoenixDriver</span><br><span class="line"># zookeeper地址</span><br><span class="line">phoenix.url=jdbc:phoenix:192.168.0.105:2181</span><br></pre></td></tr></table></figure>
<h3 id="2-4-配置数据源和会话工厂"><a href="#2-4-配置数据源和会话工厂" class="headerlink" title="2.4  配置数据源和会话工厂"></a>2.4  配置数据源和会话工厂</h3><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version="1.0" encoding="UTF-8"?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">beans</span> <span class="attr">xmlns</span>=<span class="string">"http://www.springframework.org/schema/beans"</span></span></span><br><span class="line"><span class="tag">       <span class="attr">xmlns:xsi</span>=<span class="string">"http://www.w3.org/2001/XMLSchema-instance"</span></span></span><br><span class="line"><span class="tag">       <span class="attr">xmlns:context</span>=<span class="string">"http://www.springframework.org/schema/context"</span> <span class="attr">xmlns:tx</span>=<span class="string">"http://www.springframework.org/schema/tx"</span></span></span><br><span class="line"><span class="tag">       <span class="attr">xsi:schemaLocation</span>=<span class="string">"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd</span></span></span><br><span class="line"><span class="tag"><span class="string">        http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-4.1.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx.xsd"</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 开启注解包扫描--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">context:component-scan</span> <span class="attr">base-package</span>=<span class="string">"com.heibaiying.*"</span>/&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!--指定配置文件的位置--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">context:property-placeholder</span> <span class="attr">location</span>=<span class="string">"classpath:jdbc.properties"</span>/&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!--配置数据源--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">bean</span> <span class="attr">id</span>=<span class="string">"dataSource"</span> <span class="attr">class</span>=<span class="string">"org.springframework.jdbc.datasource.DriverManagerDataSource"</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--Phoenix 配置--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"driverClassName"</span> <span class="attr">value</span>=<span class="string">"$&#123;phoenix.driverClassName&#125;"</span>/&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"url"</span> <span class="attr">value</span>=<span class="string">"$&#123;phoenix.url&#125;"</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">bean</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!--配置 mybatis 会话工厂 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">bean</span> <span class="attr">id</span>=<span class="string">"sqlSessionFactory"</span> <span class="attr">class</span>=<span class="string">"org.mybatis.spring.SqlSessionFactoryBean"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"dataSource"</span> <span class="attr">ref</span>=<span class="string">"dataSource"</span>/&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--指定 mapper 文件所在的位置--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"mapperLocations"</span> <span class="attr">value</span>=<span class="string">"classpath*:/mappers/**/*.xml"</span>/&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"configLocation"</span> <span class="attr">value</span>=<span class="string">"classpath:mybatisConfig.xml"</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">bean</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!--扫描注册接口 --&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--作用:从接口的基础包开始递归搜索，并将它们注册为 MapperFactoryBean(只有至少一种方法的接口才会被注册;, 具体类将被忽略)--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">bean</span> <span class="attr">class</span>=<span class="string">"org.mybatis.spring.mapper.MapperScannerConfigurer"</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--指定会话工厂 --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"sqlSessionFactoryBeanName"</span> <span class="attr">value</span>=<span class="string">"sqlSessionFactory"</span>/&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- 指定 mybatis 接口所在的包 --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"basePackage"</span> <span class="attr">value</span>=<span class="string">"com.heibaiying.dao"</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">bean</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">beans</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h3 id="2-5-Mybtais参数配置"><a href="#2-5-Mybtais参数配置" class="headerlink" title="2.5 Mybtais参数配置"></a>2.5 Mybtais参数配置</h3><p>新建 mybtais 配置文件，按照需求配置额外参数， 更多 settings 配置项可以参考<a href="http://www.mybatis.org/mybatis-3/zh/configuration.html" target="_blank" rel="noopener">官方文档</a></p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version="1.0" encoding="UTF-8" ?&gt;</span></span><br><span class="line"><span class="meta">&lt;!DOCTYPE <span class="meta-keyword">configuration</span></span></span><br><span class="line"><span class="meta">        <span class="meta-keyword">PUBLIC</span> <span class="meta-string">"-//mybatis.org//DTD Config 3.0//EN"</span></span></span><br><span class="line"><span class="meta">        <span class="meta-string">"http://mybatis.org/dtd/mybatis-3-config.dtd"</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- mybatis 配置文件 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">settings</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- 开启驼峰命名 --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">setting</span> <span class="attr">name</span>=<span class="string">"mapUnderscoreToCamelCase"</span> <span class="attr">value</span>=<span class="string">"true"</span>/&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- 打印查询 sql --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">setting</span> <span class="attr">name</span>=<span class="string">"logImpl"</span> <span class="attr">value</span>=<span class="string">"STDOUT_LOGGING"</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">settings</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h3 id="2-6-查询接口"><a href="#2-6-查询接口" class="headerlink" title="2.6 查询接口"></a>2.6 查询接口</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">PopulationDao</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function">List&lt;USPopulation&gt; <span class="title">queryAll</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">save</span><span class="params">(USPopulation USPopulation)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="function">USPopulation <span class="title">queryByStateAndCity</span><span class="params">(@Param(<span class="string">"state"</span>)</span> String state, @<span class="title">Param</span><span class="params">(<span class="string">"city"</span>)</span> String city)</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">deleteByStateAndCity</span><span class="params">(@Param(<span class="string">"state"</span>)</span> String state, @<span class="title">Param</span><span class="params">(<span class="string">"city"</span>)</span> String city)</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="meta">&lt;!DOCTYPE <span class="meta-keyword">mapper</span></span></span><br><span class="line"><span class="meta">        <span class="meta-keyword">PUBLIC</span> <span class="meta-string">"-//mybatis.org//DTD Mapper 3.0//EN"</span></span></span><br><span class="line"><span class="meta">        <span class="meta-string">"http://mybatis.org/dtd/mybatis-3-mapper.dtd"</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">mapper</span> <span class="attr">namespace</span>=<span class="string">"com.heibaiying.dao.PopulationDao"</span>&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">select</span> <span class="attr">id</span>=<span class="string">"queryAll"</span> <span class="attr">resultType</span>=<span class="string">"com.heibaiying.bean.USPopulation"</span>&gt;</span></span><br><span class="line">        SELECT * FROM us_population</span><br><span class="line">    <span class="tag">&lt;/<span class="name">select</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">insert</span> <span class="attr">id</span>=<span class="string">"save"</span>&gt;</span></span><br><span class="line">        UPSERT INTO us_population VALUES( #&#123;state&#125;, #&#123;city&#125;, #&#123;population&#125; )</span><br><span class="line">    <span class="tag">&lt;/<span class="name">insert</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">select</span> <span class="attr">id</span>=<span class="string">"queryByStateAndCity"</span> <span class="attr">resultType</span>=<span class="string">"com.heibaiying.bean.USPopulation"</span>&gt;</span></span><br><span class="line">        SELECT * FROM us_population WHERE state=#&#123;state&#125; AND city = #&#123;city&#125;</span><br><span class="line">    <span class="tag">&lt;/<span class="name">select</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">delete</span> <span class="attr">id</span>=<span class="string">"deleteByStateAndCity"</span>&gt;</span></span><br><span class="line">        DELETE FROM us_population WHERE state=#&#123;state&#125; AND city = #&#123;city&#125;</span><br><span class="line">    <span class="tag">&lt;/<span class="name">delete</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">mapper</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h3 id="2-7-单元测试"><a href="#2-7-单元测试" class="headerlink" title="2.7 单元测试"></a>2.7 单元测试</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@RunWith</span>(SpringRunner<span class="class">.<span class="keyword">class</span>)</span></span><br><span class="line"><span class="class">@<span class="title">ContextConfiguration</span>(</span>&#123;<span class="string">"classpath:springApplication.xml"</span>&#125;)</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">PopulationDaoTest</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Autowired</span></span><br><span class="line">    <span class="keyword">private</span> PopulationDao populationDao;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">queryAll</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        List&lt;USPopulation&gt; USPopulationList = populationDao.queryAll();</span><br><span class="line">        <span class="keyword">if</span> (USPopulationList != <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">for</span> (USPopulation USPopulation : USPopulationList) &#123;</span><br><span class="line">                System.out.println(USPopulation.getCity() + <span class="string">" "</span> + USPopulation.getPopulation());</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">save</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        populationDao.save(<span class="keyword">new</span> USPopulation(<span class="string">"TX"</span>, <span class="string">"Dallas"</span>, <span class="number">66666</span>));</span><br><span class="line">        USPopulation usPopulation = populationDao.queryByStateAndCity(<span class="string">"TX"</span>, <span class="string">"Dallas"</span>);</span><br><span class="line">        System.out.println(usPopulation);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">update</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        populationDao.save(<span class="keyword">new</span> USPopulation(<span class="string">"TX"</span>, <span class="string">"Dallas"</span>, <span class="number">99999</span>));</span><br><span class="line">        USPopulation usPopulation = populationDao.queryByStateAndCity(<span class="string">"TX"</span>, <span class="string">"Dallas"</span>);</span><br><span class="line">        System.out.println(usPopulation);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">delete</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        populationDao.deleteByStateAndCity(<span class="string">"TX"</span>, <span class="string">"Dallas"</span>);</span><br><span class="line">        USPopulation usPopulation = populationDao.queryByStateAndCity(<span class="string">"TX"</span>, <span class="string">"Dallas"</span>);</span><br><span class="line">        System.out.println(usPopulation);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="三、SpringBoot-Mybatis-Phoenix"><a href="#三、SpringBoot-Mybatis-Phoenix" class="headerlink" title="三、SpringBoot + Mybatis + Phoenix"></a>三、SpringBoot + Mybatis + Phoenix</h2><h3 id="3-1-项目结构"><a href="#3-1-项目结构" class="headerlink" title="3.1 项目结构"></a>3.1 项目结构</h3><div align="center"> <img src="../pictures/spring-boot-mybatis-phoenix.png"> </div>

<h3 id="3-2-主要依赖"><a href="#3-2-主要依赖" class="headerlink" title="3.2 主要依赖"></a>3.2 主要依赖</h3><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!--spring 1.5 x 以上版本对应 mybatis 1.3.x (1.3.1)</span></span><br><span class="line"><span class="comment">        关于更多 spring-boot 与 mybatis 的版本对应可以参见 &lt;a href="http://www.mybatis.org/spring-boot-starter/mybatis-spring-boot-autoconfigure/"&gt;--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.mybatis.spring.boot<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>mybatis-spring-boot-starter<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.3.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--phoenix core--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.phoenix<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>phoenix-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>4.14.0-cdh5.14.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>spring boot 与 mybatis 版本的对应关系：</p>
<table>
<thead>
<tr>
<th>MyBatis-Spring-Boot-Starter 版本</th>
<th>MyBatis-Spring 版本</th>
<th>Spring Boot 版本</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>1.3.x (1.3.1)</strong></td>
<td>1.3 or higher</td>
<td>1.5 or higher</td>
</tr>
<tr>
<td><strong>1.2.x (1.2.1)</strong></td>
<td>1.3 or higher</td>
<td>1.4 or higher</td>
</tr>
<tr>
<td><strong>1.1.x (1.1.1)</strong></td>
<td>1.3 or higher</td>
<td>1.3 or higher</td>
</tr>
<tr>
<td><strong>1.0.x (1.0.2)</strong></td>
<td>1.2 or higher</td>
<td>1.3 or higher</td>
</tr>
</tbody>
</table>
<h3 id="3-3-配置数据源"><a href="#3-3-配置数据源" class="headerlink" title="3.3 配置数据源"></a>3.3 配置数据源</h3><p>在 application.yml 中配置数据源，spring boot 2.x 版本默认采用 Hikari 作为数据库连接池，Hikari 是目前 java 平台性能最好的连接池，性能好于 druid。</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">spring:</span></span><br><span class="line">  <span class="attr">datasource:</span></span><br><span class="line">    <span class="comment">#zookeeper 地址</span></span><br><span class="line">    <span class="attr">url:</span> <span class="string">jdbc:phoenix:192.168.0.105:2181</span></span><br><span class="line">    <span class="attr">driver-class-name:</span> <span class="string">org.apache.phoenix.jdbc.PhoenixDriver</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 如果不想配置对数据库连接池做特殊配置的话,以下关于连接池的配置就不是必须的</span></span><br><span class="line">    <span class="comment"># spring-boot 2.X 默认采用高性能的 Hikari 作为连接池 更多配置可以参考 https://github.com/brettwooldridge/HikariCP#configuration-knobs-baby</span></span><br><span class="line">    <span class="attr">type:</span> <span class="string">com.zaxxer.hikari.HikariDataSource</span></span><br><span class="line">    <span class="attr">hikari:</span></span><br><span class="line">      <span class="comment"># 池中维护的最小空闲连接数</span></span><br><span class="line">      <span class="attr">minimum-idle:</span> <span class="number">10</span></span><br><span class="line">      <span class="comment"># 池中最大连接数，包括闲置和使用中的连接</span></span><br><span class="line">      <span class="attr">maximum-pool-size:</span> <span class="number">20</span></span><br><span class="line">      <span class="comment"># 此属性控制从池返回的连接的默认自动提交行为。默认为 true</span></span><br><span class="line">      <span class="attr">auto-commit:</span> <span class="literal">true</span></span><br><span class="line">      <span class="comment"># 允许最长空闲时间</span></span><br><span class="line">      <span class="attr">idle-timeout:</span> <span class="number">30000</span></span><br><span class="line">      <span class="comment"># 此属性表示连接池的用户定义名称，主要显示在日志记录和 JMX 管理控制台中，以标识池和池配置。 默认值：自动生成</span></span><br><span class="line">      <span class="attr">pool-name:</span> <span class="string">custom-hikari</span></span><br><span class="line">      <span class="comment">#此属性控制池中连接的最长生命周期，值 0 表示无限生命周期，默认 1800000 即 30 分钟</span></span><br><span class="line">      <span class="attr">max-lifetime:</span> <span class="number">1800000</span></span><br><span class="line">      <span class="comment"># 数据库连接超时时间,默认 30 秒，即 30000</span></span><br><span class="line">      <span class="attr">connection-timeout:</span> <span class="number">30000</span></span><br><span class="line">      <span class="comment"># 连接测试 sql 这个地方需要根据数据库方言差异而配置 例如 oracle 就应该写成  select 1 from dual</span></span><br><span class="line">      <span class="attr">connection-test-query:</span> <span class="string">SELECT</span> <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># mybatis 相关配置</span></span><br><span class="line"><span class="attr">mybatis:</span></span><br><span class="line">  <span class="attr">configuration:</span></span><br><span class="line">    <span class="comment"># 是否打印 sql 语句 调试的时候可以开启</span></span><br><span class="line">    <span class="attr">log-impl:</span> <span class="string">org.apache.ibatis.logging.stdout.StdOutImpl</span></span><br></pre></td></tr></table></figure>
<h3 id="3-4-新建查询接口"><a href="#3-4-新建查询接口" class="headerlink" title="3.4 新建查询接口"></a>3.4 新建查询接口</h3><p>上面 Spring+Mybatis 我们使用了 XML 的方式来写 SQL，为了体现 Mybatis 支持多种方式，这里使用注解的方式来写 SQL。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Mapper</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">PopulationDao</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Select</span>(<span class="string">"SELECT * from us_population"</span>)</span><br><span class="line">    <span class="function">List&lt;USPopulation&gt; <span class="title">queryAll</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Insert</span>(<span class="string">"UPSERT INTO us_population VALUES( #&#123;state&#125;, #&#123;city&#125;, #&#123;population&#125; )"</span>)</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">save</span><span class="params">(USPopulation USPopulation)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Select</span>(<span class="string">"SELECT * FROM us_population WHERE state=#&#123;state&#125; AND city = #&#123;city&#125;"</span>)</span><br><span class="line">    <span class="function">USPopulation <span class="title">queryByStateAndCity</span><span class="params">(String state, String city)</span></span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="meta">@Delete</span>(<span class="string">"DELETE FROM us_population WHERE state=#&#123;state&#125; AND city = #&#123;city&#125;"</span>)</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">deleteByStateAndCity</span><span class="params">(String state, String city)</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="3-5-单元测试"><a href="#3-5-单元测试" class="headerlink" title="3.5 单元测试"></a>3.5 单元测试</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@RunWith</span>(SpringRunner<span class="class">.<span class="keyword">class</span>)</span></span><br><span class="line"><span class="class">@<span class="title">SpringBootTest</span></span></span><br><span class="line"><span class="class"><span class="title">public</span> <span class="title">class</span> <span class="title">PopulationTest</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Autowired</span></span><br><span class="line">    <span class="keyword">private</span> PopulationDao populationDao;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">queryAll</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        List&lt;USPopulation&gt; USPopulationList = populationDao.queryAll();</span><br><span class="line">        <span class="keyword">if</span> (USPopulationList != <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">for</span> (USPopulation USPopulation : USPopulationList) &#123;</span><br><span class="line">                System.out.println(USPopulation.getCity() + <span class="string">" "</span> + USPopulation.getPopulation());</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">save</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        populationDao.save(<span class="keyword">new</span> USPopulation(<span class="string">"TX"</span>, <span class="string">"Dallas"</span>, <span class="number">66666</span>));</span><br><span class="line">        USPopulation usPopulation = populationDao.queryByStateAndCity(<span class="string">"TX"</span>, <span class="string">"Dallas"</span>);</span><br><span class="line">        System.out.println(usPopulation);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">update</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        populationDao.save(<span class="keyword">new</span> USPopulation(<span class="string">"TX"</span>, <span class="string">"Dallas"</span>, <span class="number">99999</span>));</span><br><span class="line">        USPopulation usPopulation = populationDao.queryByStateAndCity(<span class="string">"TX"</span>, <span class="string">"Dallas"</span>);</span><br><span class="line">        System.out.println(usPopulation);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">delete</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        populationDao.deleteByStateAndCity(<span class="string">"TX"</span>, <span class="string">"Dallas"</span>);</span><br><span class="line">        USPopulation usPopulation = populationDao.queryByStateAndCity(<span class="string">"TX"</span>, <span class="string">"Dallas"</span>);</span><br><span class="line">        System.out.println(usPopulation);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="附：建表语句"><a href="#附：建表语句" class="headerlink" title="附：建表语句"></a>附：建表语句</h2><p>上面单元测试涉及到的测试表的建表语句如下：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="keyword">IF</span> <span class="keyword">NOT</span> <span class="keyword">EXISTS</span> us_population (</span><br><span class="line">      state <span class="built_in">CHAR</span>(<span class="number">2</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">      city <span class="built_in">VARCHAR</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">      population <span class="built_in">BIGINT</span></span><br><span class="line">      <span class="keyword">CONSTRAINT</span> my_pk PRIMARY <span class="keyword">KEY</span> (state, city));</span><br><span class="line">      </span><br><span class="line"><span class="comment">-- 测试数据</span></span><br><span class="line">UPSERT INTO us_population <span class="keyword">VALUES</span>(<span class="string">'NY'</span>,<span class="string">'New York'</span>,<span class="number">8143197</span>);</span><br><span class="line">UPSERT INTO us_population <span class="keyword">VALUES</span>(<span class="string">'CA'</span>,<span class="string">'Los Angeles'</span>,<span class="number">3844829</span>);</span><br><span class="line">UPSERT INTO us_population <span class="keyword">VALUES</span>(<span class="string">'IL'</span>,<span class="string">'Chicago'</span>,<span class="number">2842518</span>);</span><br><span class="line">UPSERT INTO us_population <span class="keyword">VALUES</span>(<span class="string">'TX'</span>,<span class="string">'Houston'</span>,<span class="number">2016582</span>);</span><br><span class="line">UPSERT INTO us_population <span class="keyword">VALUES</span>(<span class="string">'PA'</span>,<span class="string">'Philadelphia'</span>,<span class="number">1463281</span>);</span><br><span class="line">UPSERT INTO us_population <span class="keyword">VALUES</span>(<span class="string">'AZ'</span>,<span class="string">'Phoenix'</span>,<span class="number">1461575</span>);</span><br><span class="line">UPSERT INTO us_population <span class="keyword">VALUES</span>(<span class="string">'TX'</span>,<span class="string">'San Antonio'</span>,<span class="number">1256509</span>);</span><br><span class="line">UPSERT INTO us_population <span class="keyword">VALUES</span>(<span class="string">'CA'</span>,<span class="string">'San Diego'</span>,<span class="number">1255540</span>);</span><br><span class="line">UPSERT INTO us_population <span class="keyword">VALUES</span>(<span class="string">'CA'</span>,<span class="string">'San Jose'</span>,<span class="number">912332</span>);</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>notes</category>
        <category>Spring Boot</category>
      </categories>
      <tags>
        <tag>Spring Boot</tag>
      </tags>
  </entry>
  <entry>
    <title>Sqoop基本使用</title>
    <url>/2021/03/17/Sqoop%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<h2 id="一、Sqoop-基本命令"><a href="#一、Sqoop-基本命令" class="headerlink" title="一、Sqoop 基本命令"></a>一、Sqoop 基本命令</h2><h3 id="1-查看所有命令"><a href="#1-查看所有命令" class="headerlink" title="1. 查看所有命令"></a>1. 查看所有命令</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> sqoop <span class="built_in">help</span></span></span><br></pre></td></tr></table></figure>
<div align="center"> <img src="../pictures/sqoop-help.png"> </div>

<p><br></p>
<h3 id="2-查看某条命令的具体使用方法"><a href="#2-查看某条命令的具体使用方法" class="headerlink" title="2. 查看某条命令的具体使用方法"></a>2. 查看某条命令的具体使用方法</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> sqoop <span class="built_in">help</span> 命令名</span></span><br></pre></td></tr></table></figure>
<h2 id="二、Sqoop-与-MySQL"><a href="#二、Sqoop-与-MySQL" class="headerlink" title="二、Sqoop 与 MySQL"></a>二、Sqoop 与 MySQL</h2><h3 id="1-查询MySQL所有数据库"><a href="#1-查询MySQL所有数据库" class="headerlink" title="1. 查询MySQL所有数据库"></a>1. 查询MySQL所有数据库</h3><p>通常用于 Sqoop 与 MySQL 连通测试：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sqoop list-databases \</span><br><span class="line">--connect jdbc:mysql://hadoop001:3306/ \</span><br><span class="line">--username root \</span><br><span class="line">--password root</span><br></pre></td></tr></table></figure>
<div align="center"> <img src="../pictures/sqoop-list-databases.png"> </div>

<p><br></p>
<h3 id="2-查询指定数据库中所有数据表"><a href="#2-查询指定数据库中所有数据表" class="headerlink" title="2. 查询指定数据库中所有数据表"></a>2. 查询指定数据库中所有数据表</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sqoop list-tables \</span><br><span class="line">--connect jdbc:mysql://hadoop001:3306/mysql \</span><br><span class="line">--username root \</span><br><span class="line">--password root</span><br></pre></td></tr></table></figure>
<h2 id="三、Sqoop-与-HDFS"><a href="#三、Sqoop-与-HDFS" class="headerlink" title="三、Sqoop 与 HDFS"></a>三、Sqoop 与 HDFS</h2><h3 id="3-1-MySQL数据导入到HDFS"><a href="#3-1-MySQL数据导入到HDFS" class="headerlink" title="3.1 MySQL数据导入到HDFS"></a>3.1 MySQL数据导入到HDFS</h3><h4 id="1-导入命令"><a href="#1-导入命令" class="headerlink" title="1. 导入命令"></a>1. 导入命令</h4><p>示例：导出 MySQL 数据库中的 <code>help_keyword</code> 表到 HDFS 的 <code>/sqoop</code> 目录下，如果导入目录存在则先删除再导入，使用 3 个 <code>map tasks</code> 并行导入。</p>
<blockquote>
<p>注：help_keyword 是 MySQL 内置的一张字典表，之后的示例均使用这张表。</p>
</blockquote>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line">--connect jdbc:mysql://hadoop001:3306/mysql \     </span><br><span class="line">--username root \</span><br><span class="line">--password root \</span><br><span class="line">--table help_keyword \           # 待导入的表</span><br><span class="line">--delete-target-dir \            # 目标目录存在则先删除</span><br><span class="line">--target-dir /sqoop \            # 导入的目标目录</span><br><span class="line">--fields-terminated-by '\t'  \   # 指定导出数据的分隔符</span><br><span class="line">-m 3                             # 指定并行执行的 map tasks 数量</span><br></pre></td></tr></table></figure>
<p>日志输出如下，可以看到输入数据被平均 <code>split</code> 为三份，分别由三个 <code>map task</code> 进行处理。数据默认以表的主键列作为拆分依据，如果你的表没有主键，有以下两种方案：</p>
<ul>
<li>添加 <code>-- autoreset-to-one-mapper</code> 参数，代表只启动一个 <code>map task</code>，即不并行执行；</li>
<li>若仍希望并行执行，则可以使用 <code>--split-by &lt;column-name&gt;</code> 指明拆分数据的参考列。</li>
</ul>
<div align="center"> <img src="../pictures/sqoop-map-task.png"> </div>

<h4 id="2-导入验证"><a href="#2-导入验证" class="headerlink" title="2. 导入验证"></a>2. 导入验证</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 查看导入后的目录</span></span><br><span class="line">hadoop fs -ls  -R /sqoop</span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看导入内容</span></span><br><span class="line">hadoop fs -text  /sqoop/part-m-00000</span><br></pre></td></tr></table></figure>
<p>查看 HDFS 导入目录,可以看到表中数据被分为 3 部分进行存储，这是由指定的并行度决定的。</p>
<div align="center"> <img src="../pictures/sqoop_hdfs_ls.png"> </div>

<p><br></p>
<h3 id="3-2-HDFS数据导出到MySQL"><a href="#3-2-HDFS数据导出到MySQL" class="headerlink" title="3.2 HDFS数据导出到MySQL"></a>3.2 HDFS数据导出到MySQL</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sqoop export  \</span><br><span class="line">    --connect jdbc:mysql://hadoop001:3306/mysql \</span><br><span class="line">    --username root \</span><br><span class="line">    --password root \</span><br><span class="line">    --table help_keyword_from_hdfs \        # 导出数据存储在 MySQL 的 help_keyword_from_hdf 的表中</span><br><span class="line">    --export-dir /sqoop  \</span><br><span class="line">    --input-fields-terminated-by '\t'\</span><br><span class="line">    --m 3</span><br></pre></td></tr></table></figure>
<p>表必须预先创建，建表语句如下：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> help_keyword_from_hdfs <span class="keyword">LIKE</span> help_keyword ;</span><br></pre></td></tr></table></figure>
<h2 id="四、Sqoop-与-Hive"><a href="#四、Sqoop-与-Hive" class="headerlink" title="四、Sqoop 与 Hive"></a>四、Sqoop 与 Hive</h2><h3 id="4-1-MySQL数据导入到Hive"><a href="#4-1-MySQL数据导入到Hive" class="headerlink" title="4.1 MySQL数据导入到Hive"></a>4.1 MySQL数据导入到Hive</h3><p>Sqoop 导入数据到 Hive 是通过先将数据导入到 HDFS 上的临时目录，然后再将数据从 HDFS 上 <code>Load</code> 到 Hive 中，最后将临时目录删除。可以使用 <code>target-dir</code> 来指定临时目录。</p>
<h4 id="1-导入命令-1"><a href="#1-导入命令-1" class="headerlink" title="1. 导入命令"></a>1. 导入命令</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line">  --connect jdbc:mysql://hadoop001:3306/mysql \</span><br><span class="line">  --username root \</span><br><span class="line">  --password root \</span><br><span class="line">  --table help_keyword \        # 待导入的表     </span><br><span class="line">  --delete-target-dir \         # 如果临时目录存在删除</span><br><span class="line">  --target-dir /sqoop_hive  \   # 临时目录位置</span><br><span class="line">  --hive-database sqoop_test \  # 导入到 Hive 的 sqoop_test 数据库，数据库需要预先创建。不指定则默认为 default 库</span><br><span class="line">  --hive-import \               # 导入到 Hive</span><br><span class="line">  --hive-overwrite \            # 如果 Hive 表中有数据则覆盖，这会清除表中原有的数据，然后再写入</span><br><span class="line">  -m 3                          # 并行度</span><br></pre></td></tr></table></figure>
<p>导入到 Hive 中的 <code>sqoop_test</code> 数据库需要预先创建，不指定则默认使用 Hive 中的 <code>default</code> 库。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 查看 hive 中的所有数据库</span></span><br><span class="line"><span class="meta">hive&gt;</span><span class="bash">  SHOW DATABASES;</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 创建 sqoop_test 数据库</span></span><br><span class="line"><span class="meta">hive&gt;</span><span class="bash">  CREATE DATABASE sqoop_test;</span></span><br></pre></td></tr></table></figure>
<h4 id="2-导入验证-1"><a href="#2-导入验证-1" class="headerlink" title="2. 导入验证"></a>2. 导入验证</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 查看 sqoop_test 数据库的所有表</span></span><br><span class="line"><span class="meta"> hive&gt;</span><span class="bash">  SHOW  TABLES  IN  sqoop_test;</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看表中数据</span></span><br><span class="line"><span class="meta"> hive&gt;</span><span class="bash"> SELECT * FROM sqoop_test.help_keyword;</span></span><br></pre></td></tr></table></figure>
<div align="center"> <img src="../pictures/sqoop_hive_tables.png"> </div>

<h4 id="3-可能出现的问题"><a href="#3-可能出现的问题" class="headerlink" title="3. 可能出现的问题"></a>3. 可能出现的问题</h4><div align="center"> <img src="../pictures/sqoop_hive_error.png"> </div>

<p><br></p>
<p>如果执行报错 <code>java.io.IOException: java.lang.ClassNotFoundException: org.apache.hadoop.hive.conf.HiveConf</code>，则需将 Hive 安装目录下 <code>lib</code> 下的 <code>hive-exec-**.jar</code> 放到 sqoop 的 <code>lib</code> 。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@hadoop001 lib]# ll hive-exec-*</span><br><span class="line">-rw-r--r--. 1 1106 4001 19632031 11 月 13 21:45 hive-exec-1.1.0-cdh5.15.2.jar</span><br><span class="line">[root@hadoop001 lib]# cp hive-exec-1.1.0-cdh5.15.2.jar  $&#123;SQOOP_HOME&#125;/lib</span><br></pre></td></tr></table></figure>
<p><br></p>
<h3 id="4-2-Hive-导出数据到MySQL"><a href="#4-2-Hive-导出数据到MySQL" class="headerlink" title="4.2 Hive 导出数据到MySQL"></a>4.2 Hive 导出数据到MySQL</h3><p>由于 Hive 的数据是存储在 HDFS 上的，所以 Hive 导入数据到 MySQL，实际上就是 HDFS 导入数据到 MySQL。</p>
<h4 id="1-查看Hive表在HDFS的存储位置"><a href="#1-查看Hive表在HDFS的存储位置" class="headerlink" title="1. 查看Hive表在HDFS的存储位置"></a>1. 查看Hive表在HDFS的存储位置</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 进入对应的数据库</span></span><br><span class="line"><span class="meta">hive&gt;</span><span class="bash"> use sqoop_test;</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看表信息</span></span><br><span class="line"><span class="meta">hive&gt;</span><span class="bash"> desc formatted help_keyword;</span></span><br></pre></td></tr></table></figure>
<p><code>Location</code> 属性为其存储位置：</p>
<div align="center"> <img src="../pictures/sqoop-hive-location.png"> </div>

<p>这里可以查看一下这个目录，文件结构如下：</p>
<div align="center"> <img src="../pictures/sqoop-hive-hdfs.png"> </div>

<h4 id="3-2-执行导出命令"><a href="#3-2-执行导出命令" class="headerlink" title="3.2 执行导出命令"></a>3.2 执行导出命令</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sqoop export  \</span><br><span class="line">    --connect jdbc:mysql://hadoop001:3306/mysql \</span><br><span class="line">    --username root \</span><br><span class="line">    --password root \</span><br><span class="line">    --table help_keyword_from_hive \</span><br><span class="line">    --export-dir /user/hive/warehouse/sqoop_test.db/help_keyword  \</span><br><span class="line">    -input-fields-terminated-by '\001' \             # 需要注意的是 hive 中默认的分隔符为 \001</span><br><span class="line">    --m 3</span><br></pre></td></tr></table></figure>
<p>MySQL 中的表需要预先创建：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> help_keyword_from_hive <span class="keyword">LIKE</span> help_keyword ;</span><br></pre></td></tr></table></figure>
<h2 id="五、Sqoop-与-HBase"><a href="#五、Sqoop-与-HBase" class="headerlink" title="五、Sqoop 与 HBase"></a>五、Sqoop 与 HBase</h2><blockquote>
<p>本小节只讲解从 RDBMS 导入数据到 HBase，因为暂时没有命令能够从 HBase 直接导出数据到 RDBMS。</p>
</blockquote>
<h3 id="5-1-MySQL导入数据到HBase"><a href="#5-1-MySQL导入数据到HBase" class="headerlink" title="5.1 MySQL导入数据到HBase"></a>5.1 MySQL导入数据到HBase</h3><h4 id="1-导入数据"><a href="#1-导入数据" class="headerlink" title="1. 导入数据"></a>1. 导入数据</h4><p>将 <code>help_keyword</code> 表中数据导入到 HBase 上的 <code>help_keyword_hbase</code> 表中，使用原表的主键 <code>help_keyword_id</code> 作为 <code>RowKey</code>，原表的所有列都会在 <code>keywordInfo</code> 列族下，目前只支持全部导入到一个列族下，不支持分别指定列族。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line">    --connect jdbc:mysql://hadoop001:3306/mysql \</span><br><span class="line">    --username root \</span><br><span class="line">    --password root \</span><br><span class="line">    --table help_keyword \              # 待导入的表</span><br><span class="line">    --hbase-table help_keyword_hbase \  # hbase 表名称，表需要预先创建</span><br><span class="line">    --column-family keywordInfo \       # 所有列导入到 keywordInfo 列族下 </span><br><span class="line">    --hbase-row-key help_keyword_id     # 使用原表的 help_keyword_id 作为 RowKey</span><br></pre></td></tr></table></figure>
<p>导入的 HBase 表需要预先创建：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 查看所有表</span></span><br><span class="line"><span class="meta">hbase&gt;</span><span class="bash"> list</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 创建表</span></span><br><span class="line"><span class="meta">hbase&gt;</span><span class="bash"> create <span class="string">'help_keyword_hbase'</span>, <span class="string">'keywordInfo'</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看表信息</span></span><br><span class="line"><span class="meta">hbase&gt;</span><span class="bash"> desc <span class="string">'help_keyword_hbase'</span></span></span><br></pre></td></tr></table></figure>
<h4 id="2-导入验证-2"><a href="#2-导入验证-2" class="headerlink" title="2. 导入验证"></a>2. 导入验证</h4><p>使用 <code>scan</code> 查看表数据：</p>
<div align="center"> <img src="../pictures/sqoop_hbase.png"> </div>





<h2 id="六、全库导出"><a href="#六、全库导出" class="headerlink" title="六、全库导出"></a>六、全库导出</h2><p>Sqoop 支持通过 <code>import-all-tables</code> 命令进行全库导出到 HDFS/Hive，但需要注意有以下两个限制：</p>
<ul>
<li>所有表必须有主键；或者使用 <code>--autoreset-to-one-mapper</code>，代表只启动一个 <code>map task</code>;</li>
<li>你不能使用非默认的分割列，也不能通过 WHERE 子句添加任何限制。</li>
</ul>
<blockquote>
<p>第二点解释得比较拗口，这里列出官方原本的说明：</p>
<ul>
<li>You must not intend to use non-default splitting column, nor impose any conditions via a <code>WHERE</code> clause.</li>
</ul>
</blockquote>
<p>全库导出到 HDFS：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sqoop import-all-tables \</span><br><span class="line">    --connect jdbc:mysql://hadoop001:3306/数据库名 \</span><br><span class="line">    --username root \</span><br><span class="line">    --password root \</span><br><span class="line">    --warehouse-dir  /sqoop_all \     # 每个表会单独导出到一个目录，需要用此参数指明所有目录的父目录</span><br><span class="line">    --fields-terminated-by '\t'  \</span><br><span class="line">    -m 3</span><br></pre></td></tr></table></figure>
<p>全库导出到 Hive：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sqoop import-all-tables -Dorg.apache.sqoop.splitter.allow_text_splitter=true \</span><br><span class="line">  --connect jdbc:mysql://hadoop001:3306/数据库名 \</span><br><span class="line">  --username root \</span><br><span class="line">  --password root \</span><br><span class="line">  --hive-database sqoop_test \         # 导出到 Hive 对应的库   </span><br><span class="line">  --hive-import \</span><br><span class="line">  --hive-overwrite \</span><br><span class="line">  -m 3</span><br></pre></td></tr></table></figure>
<h2 id="七、Sqoop-数据过滤"><a href="#七、Sqoop-数据过滤" class="headerlink" title="七、Sqoop 数据过滤"></a>七、Sqoop 数据过滤</h2><h3 id="7-1-query参数"><a href="#7-1-query参数" class="headerlink" title="7.1 query参数"></a>7.1 query参数</h3><p>Sqoop 支持使用 <code>query</code> 参数定义查询 SQL，从而可以导出任何想要的结果集。使用示例如下：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line">  --connect jdbc:mysql://hadoop001:3306/mysql \</span><br><span class="line">  --username root \</span><br><span class="line">  --password root \</span><br><span class="line">  --query 'select * from help_keyword where  $CONDITIONS and  help_keyword_id &lt; 50' \  </span><br><span class="line">  --delete-target-dir \            </span><br><span class="line">  --target-dir /sqoop_hive  \ </span><br><span class="line">  --hive-database sqoop_test \           # 指定导入目标数据库 不指定则默认使用 Hive 中的 default 库</span><br><span class="line">  --hive-table filter_help_keyword \     # 指定导入目标表</span><br><span class="line">  --split-by help_keyword_id \           # 指定用于 split 的列      </span><br><span class="line">  --hive-import \                        # 导入到 Hive</span><br><span class="line">  --hive-overwrite \                     、</span><br><span class="line">  -m 3</span><br></pre></td></tr></table></figure>
<p>在使用 <code>query</code> 进行数据过滤时，需要注意以下三点：</p>
<ul>
<li>必须用 <code>--hive-table</code> 指明目标表；</li>
<li>如果并行度 <code>-m</code> 不为 1 或者没有指定 <code>--autoreset-to-one-mapper</code>，则需要用 <code>--split-by</code> 指明参考列；</li>
<li>SQL 的 <code>where</code> 字句必须包含 <code>$CONDITIONS</code>，这是固定写法，作用是动态替换。</li>
</ul>
<h3 id="7-2-增量导入"><a href="#7-2-增量导入" class="headerlink" title="7.2 增量导入"></a>7.2 增量导入</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line">    --connect jdbc:mysql://hadoop001:3306/mysql \</span><br><span class="line">    --username root \</span><br><span class="line">    --password root \</span><br><span class="line">    --table help_keyword \</span><br><span class="line">    --target-dir /sqoop_hive  \</span><br><span class="line">    --hive-database sqoop_test \         </span><br><span class="line">    --incremental  append  \             # 指明模式</span><br><span class="line">    --check-column  help_keyword_id \    # 指明用于增量导入的参考列</span><br><span class="line">    --last-value 300  \                  # 指定参考列上次导入的最大值</span><br><span class="line">    --hive-import \   </span><br><span class="line">    -m 3</span><br></pre></td></tr></table></figure>
<p><code>incremental</code> 参数有以下两个可选的选项：</p>
<ul>
<li><strong>append</strong>：要求参考列的值必须是递增的，所有大于 <code>last-value</code> 的值都会被导入；</li>
<li><strong>lastmodified</strong>：要求参考列的值必须是 <code>timestamp</code> 类型，且插入数据时候要在参考列插入当前时间戳，更新数据时也要更新参考列的时间戳，所有时间晚于 <code>last-value</code> 的数据都会被导入。</li>
</ul>
<p>通过上面的解释我们可以看出来，其实 Sqoop 的增量导入并没有太多神器的地方，就是依靠维护的参考列来判断哪些是增量数据。当然我们也可以使用上面介绍的 <code>query</code> 参数来进行手动的增量导出，这样反而更加灵活。</p>
<h2 id="八、类型支持"><a href="#八、类型支持" class="headerlink" title="八、类型支持"></a>八、类型支持</h2><p>Sqoop 默认支持数据库的大多数字段类型，但是某些特殊类型是不支持的。遇到不支持的类型，程序会抛出异常 <code>Hive does not support the SQL type for column xxx</code> 异常，此时可以通过下面两个参数进行强制类型转换：</p>
<ul>
<li><strong>–map-column-java\<mapping></mapping></strong>   ：重写 SQL 到 Java 类型的映射；</li>
<li><strong>–map-column-hive \<mapping></mapping></strong> ： 重写 Hive 到 Java 类型的映射。</li>
</ul>
<p>示例如下，将原先 <code>id</code> 字段强制转为 String 类型，<code>value</code> 字段强制转为 Integer 类型：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ sqoop import ... --map-column-java id=String,value=Integer</span><br></pre></td></tr></table></figure>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="http://sqoop.apache.org/docs/1.4.7/SqoopUserGuide.html" target="_blank" rel="noopener">Sqoop User Guide (v1.4.7)</a></p>
]]></content>
      <categories>
        <category>技术</category>
        <category>Sqoop</category>
      </categories>
      <tags>
        <tag>Sqoop</tag>
      </tags>
  </entry>
  <entry>
    <title>Sqoop 简介与安装</title>
    <url>/2021/03/17/Sqoop%E7%AE%80%E4%BB%8B%E4%B8%8E%E5%AE%89%E8%A3%85/</url>
    <content><![CDATA[<h2 id="一、Sqoop-简介"><a href="#一、Sqoop-简介" class="headerlink" title="一、Sqoop 简介"></a>一、Sqoop 简介</h2><p>Sqoop 是一个常用的数据迁移工具，主要用于在不同存储系统之间实现数据的导入与导出：</p>
<ul>
<li><p>导入数据：从 MySQL，Oracle 等关系型数据库中导入数据到 HDFS、Hive、HBase 等分布式文件存储系统中；</p>
</li>
<li><p>导出数据：从 分布式文件系统中导出数据到关系数据库中。</p>
</li>
</ul>
<p>其原理是将执行命令转化成 MapReduce 作业来实现数据的迁移，如下图：</p>
<div align="center"> <img src="../pictures/sqoop-tool.png"> </div>

<h2 id="二、安装"><a href="#二、安装" class="headerlink" title="二、安装"></a>二、安装</h2><p>版本选择：目前 Sqoop 有 Sqoop 1 和 Sqoop 2 两个版本，但是截至到目前，官方并不推荐使用 Sqoop 2，因为其与 Sqoop 1 并不兼容，且功能还没有完善，所以这里优先推荐使用 Sqoop 1。</p>
<div align="center"> <img src="../pictures/sqoop-version-selected.png"> </div>



<h3 id="2-1-下载并解压"><a href="#2-1-下载并解压" class="headerlink" title="2.1 下载并解压"></a>2.1 下载并解压</h3><p>下载所需版本的 Sqoop ，这里我下载的是 <code>CDH</code> 版本的 Sqoop 。下载地址为：<a href="http://archive.cloudera.com/cdh5/cdh/5/" target="_blank" rel="noopener">http://archive.cloudera.com/cdh5/cdh/5/</a></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 下载后进行解压</span></span><br><span class="line">tar -zxvf  sqoop-1.4.6-cdh5.15.2.tar.gz</span><br></pre></td></tr></table></figure>
<h3 id="2-2-配置环境变量"><a href="#2-2-配置环境变量" class="headerlink" title="2.2 配置环境变量"></a>2.2 配置环境变量</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> vim /etc/profile</span></span><br></pre></td></tr></table></figure>
<p>添加环境变量：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">export SQOOP_HOME=/usr/app/sqoop-1.4.6-cdh5.15.2</span><br><span class="line">export PATH=$SQOOP_HOME/bin:$PATH</span><br></pre></td></tr></table></figure>
<p>使得配置的环境变量立即生效：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">source</span> /etc/profile</span></span><br></pre></td></tr></table></figure>
<h3 id="2-3-修改配置"><a href="#2-3-修改配置" class="headerlink" title="2.3 修改配置"></a>2.3 修改配置</h3><p>进入安装目录下的 <code>conf/</code> 目录，拷贝 Sqoop 的环境配置模板 <code>sqoop-env.sh.template</code></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> cp sqoop-env-template.sh sqoop-env.sh</span></span><br></pre></td></tr></table></figure>
<p>修改 <code>sqoop-env.sh</code>，内容如下 (以下配置中 <code>HADOOP_COMMON_HOME</code> 和 <code>HADOOP_MAPRED_HOME</code> 是必选的，其他的是可选的)：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Set Hadoop-specific environment variables here.</span></span><br><span class="line"><span class="meta">#</span><span class="bash">Set path to <span class="built_in">where</span> bin/hadoop is available</span></span><br><span class="line">export HADOOP_COMMON_HOME=/usr/app/hadoop-2.6.0-cdh5.15.2</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">Set path to <span class="built_in">where</span> hadoop-*-core.jar is available</span></span><br><span class="line">export HADOOP_MAPRED_HOME=/usr/app/hadoop-2.6.0-cdh5.15.2</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="built_in">set</span> the path to <span class="built_in">where</span> bin/hbase is available</span></span><br><span class="line">export HBASE_HOME=/usr/app/hbase-1.2.0-cdh5.15.2</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">Set the path to <span class="built_in">where</span> bin/hive is available</span></span><br><span class="line">export HIVE_HOME=/usr/app/hive-1.1.0-cdh5.15.2</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">Set the path <span class="keyword">for</span> <span class="built_in">where</span> zookeper config dir is</span></span><br><span class="line">export ZOOCFGDIR=/usr/app/zookeeper-3.4.13/conf</span><br></pre></td></tr></table></figure>
<h3 id="2-4-拷贝数据库驱动"><a href="#2-4-拷贝数据库驱动" class="headerlink" title="2.4 拷贝数据库驱动"></a>2.4 拷贝数据库驱动</h3><p>将 MySQL 驱动包拷贝到 Sqoop 安装目录的 <code>lib</code> 目录下, 驱动包的下载地址为 <a href="https://dev.mysql.com/downloads/connector/j/" target="_blank" rel="noopener">https://dev.mysql.com/downloads/connector/j/</a>  。在本仓库的<a href="https://github.com/heibaiying/BigData-Notes/tree/master/resources" target="_blank" rel="noopener">resources</a> 目录下我也上传了一份，有需要的话可以自行下载。</p>
<div align="center"> <img src="../pictures/sqoop-mysql-jar.png"> </div>



<h3 id="2-5-验证"><a href="#2-5-验证" class="headerlink" title="2.5 验证"></a>2.5 验证</h3><p>由于已经将 sqoop 的 <code>bin</code> 目录配置到环境变量，直接使用以下命令验证是否配置成功：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> sqoop version</span></span><br></pre></td></tr></table></figure>
<p>出现对应的版本信息则代表配置成功：</p>
<div align="center"> <img src="../pictures/sqoop-version.png"> </div>

<p>这里出现的两个 <code>Warning</code> 警告是因为我们本身就没有用到 <code>HCatalog</code> 和 <code>Accumulo</code>，忽略即可。Sqoop 在启动时会去检查环境变量中是否有配置这些软件，如果想去除这些警告，可以修改 <code>bin/configure-sqoop</code>，注释掉不必要的检查。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Check: If we can<span class="string">'t find our dependencies, give up here.</span></span></span><br><span class="line">if [ ! -d "$&#123;HADOOP_COMMON_HOME&#125;" ]; then</span><br><span class="line">  echo "Error: $HADOOP_COMMON_HOME does not exist!"</span><br><span class="line">  echo 'Please set $HADOOP_COMMON_HOME to the root of your Hadoop installation.'</span><br><span class="line">  exit 1</span><br><span class="line">fi</span><br><span class="line">if [ ! -d "$&#123;HADOOP_MAPRED_HOME&#125;" ]; then</span><br><span class="line">  echo "Error: $HADOOP_MAPRED_HOME does not exist!"</span><br><span class="line">  echo 'Please set $HADOOP_MAPRED_HOME to the root of your Hadoop MapReduce installation.'</span><br><span class="line">  exit 1</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># Moved to be a runtime check in sqoop.</span></span></span><br><span class="line">if [ ! -d "$&#123;HBASE_HOME&#125;" ]; then</span><br><span class="line">  echo "Warning: $HBASE_HOME does not exist! HBase imports will fail."</span><br><span class="line">  echo 'Please set $HBASE_HOME to the root of your HBase installation.'</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># Moved to be a runtime check in sqoop.</span></span></span><br><span class="line">if [ ! -d "$&#123;HCAT_HOME&#125;" ]; then</span><br><span class="line">  echo "Warning: $HCAT_HOME does not exist! HCatalog jobs will fail."</span><br><span class="line">  echo 'Please set $HCAT_HOME to the root of your HCatalog installation.'</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">if [ ! -d "$&#123;ACCUMULO_HOME&#125;" ]; then</span><br><span class="line">  echo "Warning: $ACCUMULO_HOME does not exist! Accumulo imports will fail."</span><br><span class="line">  echo 'Please set $ACCUMULO_HOME to the root of your Accumulo installation.'</span><br><span class="line">fi</span><br><span class="line">if [ ! -d "$&#123;ZOOKEEPER_HOME&#125;" ]; then</span><br><span class="line">  echo "Warning: $ZOOKEEPER_HOME does not exist! Accumulo imports will fail."</span><br><span class="line">  echo 'Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation.'</span><br><span class="line">fi</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>技术</category>
        <category>Sqoop</category>
      </categories>
      <tags>
        <tag>Sqoop</tag>
      </tags>
  </entry>
  <entry>
    <title>Zookeeper ACL</title>
    <url>/2021/03/17/Zookeeper_ACL%E6%9D%83%E9%99%90%E6%8E%A7%E5%88%B6/</url>
    <content><![CDATA[<h2 id="一、前言"><a href="#一、前言" class="headerlink" title="一、前言"></a>一、前言</h2><p>为了避免存储在 Zookeeper 上的数据被其他程序或者人为误修改，Zookeeper 提供了 ACL(Access Control Lists) 进行权限控制。只有拥有对应权限的用户才可以对节点进行增删改查等操作。下文分别介绍使用原生的 Shell 命令和 Apache Curator 客户端进行权限设置。</p>
<h2 id="二、使用Shell进行权限管理"><a href="#二、使用Shell进行权限管理" class="headerlink" title="二、使用Shell进行权限管理"></a>二、使用Shell进行权限管理</h2><h3 id="2-1-设置与查看权限"><a href="#2-1-设置与查看权限" class="headerlink" title="2.1 设置与查看权限"></a>2.1 设置与查看权限</h3><p>想要给某个节点设置权限 (ACL)，有以下两个可选的命令：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 1.给已有节点赋予权限</span></span><br><span class="line">setAcl path acl</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 2.在创建节点时候指定权限</span></span><br><span class="line">create [-s] [-e] path data acl</span><br></pre></td></tr></table></figure>
<p>查看指定节点的权限命令如下：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">getAcl path</span><br></pre></td></tr></table></figure>
<h3 id="2-2-权限组成"><a href="#2-2-权限组成" class="headerlink" title="2.2 权限组成"></a>2.2 权限组成</h3><p>Zookeeper 的权限由[scheme : id :permissions]三部分组成，其中 Schemes 和 Permissions 内置的可选项分别如下：</p>
<p><strong>Permissions 可选项</strong>：</p>
<ul>
<li><strong>CREATE</strong>：允许创建子节点；</li>
<li><strong>READ</strong>：允许从节点获取数据并列出其子节点；</li>
<li><strong>WRITE</strong>：允许为节点设置数据；</li>
<li><strong>DELETE</strong>：允许删除子节点；</li>
<li><strong>ADMIN</strong>：允许为节点设置权限。  </li>
</ul>
<p><strong>Schemes 可选项</strong>：</p>
<ul>
<li><strong>world</strong>：默认模式，所有客户端都拥有指定的权限。world 下只有一个 id 选项，就是 anyone，通常组合写法为 <code>world:anyone:[permissons]</code>；</li>
<li><strong>auth</strong>：只有经过认证的用户才拥有指定的权限。通常组合写法为 <code>auth:user:password:[permissons]</code>，使用这种模式时，你需要先进行登录，之后采用 auth 模式设置权限时，<code>user</code> 和 <code>password</code> 都将使用登录的用户名和密码；</li>
<li><strong>digest</strong>：只有经过认证的用户才拥有指定的权限。通常组合写法为 <code>auth:user:BASE64(SHA1(password)):[permissons]</code>，这种形式下的密码必须通过 SHA1 和 BASE64 进行双重加密；</li>
<li><strong>ip</strong>：限制只有特定 IP 的客户端才拥有指定的权限。通常组成写法为 <code>ip:182.168.0.168:[permissions]</code>；</li>
<li><strong>super</strong>：代表超级管理员，拥有所有的权限，需要修改 Zookeeper 启动脚本进行配置。</li>
</ul>
<h3 id="2-3-添加认证信息"><a href="#2-3-添加认证信息" class="headerlink" title="2.3 添加认证信息"></a>2.3 添加认证信息</h3><p>可以使用如下所示的命令为当前 Session 添加用户认证信息，等价于登录操作。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 格式</span></span><br><span class="line">addauth scheme auth </span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">示例：添加用户名为heibai,密码为root的用户认证信息</span></span><br><span class="line">addauth digest heibai:root</span><br></pre></td></tr></table></figure>
<h3 id="2-4-权限设置示例"><a href="#2-4-权限设置示例" class="headerlink" title="2.4 权限设置示例"></a>2.4 权限设置示例</h3><h4 id="1-world模式"><a href="#1-world模式" class="headerlink" title="1. world模式"></a>1. world模式</h4><p>world 是一种默认的模式，即创建时如果不指定权限，则默认的权限就是 world。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 32] create /hadoop 123</span><br><span class="line">Created /hadoop</span><br><span class="line">[zk: localhost:2181(CONNECTED) 33] getAcl /hadoop</span><br><span class="line">'world,'anyone    #默认的权限</span><br><span class="line">: cdrwa</span><br><span class="line">[zk: localhost:2181(CONNECTED) 34] setAcl /hadoop world:anyone:cwda   # 修改节点，不允许所有客户端读</span><br><span class="line">....</span><br><span class="line">[zk: localhost:2181(CONNECTED) 35] get /hadoop</span><br><span class="line">Authentication is not valid : /hadoop     # 权限不足</span><br></pre></td></tr></table></figure>
<h4 id="2-auth模式"><a href="#2-auth模式" class="headerlink" title="2. auth模式"></a>2. auth模式</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 36] addauth digest heibai:heibai  # 登录</span><br><span class="line">[zk: localhost:2181(CONNECTED) 37] setAcl /hadoop auth::cdrwa    # 设置权限</span><br><span class="line">[zk: localhost:2181(CONNECTED) 38] getAcl /hadoop                # 获取权限</span><br><span class="line">'digest,'heibai:sCxtVJ1gPG8UW/jzFHR0A1ZKY5s=   #用户名和密码 (密码经过加密处理)，注意返回的权限类型是 digest</span><br><span class="line">: cdrwa</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">用户名和密码都是使用登录的用户名和密码，即使你在创建权限时候进行指定也是无效的</span></span><br><span class="line">[zk: localhost:2181(CONNECTED) 39] setAcl /hadoop auth:root:root:cdrwa    #指定用户名和密码为 root</span><br><span class="line">[zk: localhost:2181(CONNECTED) 40] getAcl /hadoop</span><br><span class="line">'digest,'heibai:sCxtVJ1gPG8UW/jzFHR0A1ZKY5s=  #无效，使用的用户名和密码依然还是 heibai</span><br><span class="line">: cdrwa</span><br></pre></td></tr></table></figure>
<h4 id="3-digest模式"><a href="#3-digest模式" class="headerlink" title="3. digest模式"></a>3. digest模式</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[zk:44] create /spark "spark" digest:heibai:sCxtVJ1gPG8UW/jzFHR0A1ZKY5s=:cdrwa  #指定用户名和加密后的密码</span><br><span class="line">[zk:45] getAcl /spark  #获取权限</span><br><span class="line">'digest,'heibai:sCxtVJ1gPG8UW/jzFHR0A1ZKY5s=   # 返回的权限类型是 digest</span><br><span class="line">: cdrwa</span><br></pre></td></tr></table></figure>
<p>到这里你可以发现使用 <code>auth</code> 模式设置的权限和使用 <code>digest</code> 模式设置的权限，在最终结果上，得到的权限模式都是 <code>digest</code>。某种程度上，你可以把 <code>auth</code> 模式理解成是 <code>digest</code> 模式的一种简便实现。因为在 <code>digest</code> 模式下，每次设置都需要书写用户名和加密后的密码，这是比较繁琐的，采用 <code>auth</code> 模式就可以避免这种麻烦。</p>
<h4 id="4-ip模式"><a href="#4-ip模式" class="headerlink" title="4. ip模式"></a>4. ip模式</h4><p>限定只有特定的 ip 才能访问。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 46] create  /hive "hive" ip:192.168.0.108:cdrwa  </span><br><span class="line">[zk: localhost:2181(CONNECTED) 47] get /hive</span><br><span class="line">Authentication is not valid : /hive  # 当前主机已经不能访问</span><br></pre></td></tr></table></figure>
<p>这里可以看到当前主机已经不能访问，想要能够再次访问，可以使用对应 IP 的客户端，或使用下面介绍的 <code>super</code> 模式。</p>
<h4 id="5-super模式"><a href="#5-super模式" class="headerlink" title="5. super模式"></a>5. super模式</h4><p>需要修改启动脚本 <code>zkServer.sh</code>，并在指定位置添加超级管理员账户和密码信息：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">"-Dzookeeper.DigestAuthenticationProvider.superDigest=heibai:sCxtVJ1gPG8UW/jzFHR0A1ZKY5s="</span><br></pre></td></tr></table></figure>
<div align="center"> <img src="../pictures/zookeeper-super.png"> </div>

<p>修改完成后需要使用 <code>zkServer.sh restart</code> 重启服务，此时再次访问限制 IP 的节点：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 0] get /hive  #访问受限</span><br><span class="line">Authentication is not valid : /hive</span><br><span class="line">[zk: localhost:2181(CONNECTED) 1] addauth digest heibai:heibai  # 登录 (添加认证信息)</span><br><span class="line">[zk: localhost:2181(CONNECTED) 2] get /hive  #成功访问</span><br><span class="line">hive</span><br><span class="line">cZxid = 0x158</span><br><span class="line">ctime = Sat May 25 09:11:29 CST 2019</span><br><span class="line">mZxid = 0x158</span><br><span class="line">mtime = Sat May 25 09:11:29 CST 2019</span><br><span class="line">pZxid = 0x158</span><br><span class="line">cversion = 0</span><br><span class="line">dataVersion = 0</span><br><span class="line">aclVersion = 0</span><br><span class="line">ephemeralOwner = 0x0</span><br><span class="line">dataLength = 4</span><br><span class="line">numChildren = 0</span><br></pre></td></tr></table></figure>
<h2 id="三、使用Java客户端进行权限管理"><a href="#三、使用Java客户端进行权限管理" class="headerlink" title="三、使用Java客户端进行权限管理"></a>三、使用Java客户端进行权限管理</h2><h3 id="3-1-主要依赖"><a href="#3-1-主要依赖" class="headerlink" title="3.1 主要依赖"></a>3.1 主要依赖</h3><p>这里以 Apache Curator 为例，使用前需要导入相关依赖，完整依赖如下：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--Apache Curator 相关依赖--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.curator<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>curator-framework<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>4.0.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.curator<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>curator-recipes<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>4.0.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.zookeeper<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>zookeeper<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.4.13<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--单元测试相关依赖--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>4.12<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h3 id="3-2-权限管理API"><a href="#3-2-权限管理API" class="headerlink" title="3.2 权限管理API"></a>3.2 权限管理API</h3><p> Apache Curator 权限设置的示例如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">AclOperation</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> CuratorFramework client = <span class="keyword">null</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String zkServerPath = <span class="string">"192.168.0.226:2181"</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String nodePath = <span class="string">"/hadoop/hdfs"</span>;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Before</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">prepare</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        RetryPolicy retryPolicy = <span class="keyword">new</span> RetryNTimes(<span class="number">3</span>, <span class="number">5000</span>);</span><br><span class="line">        client = CuratorFrameworkFactory.builder()</span><br><span class="line">                .authorization(<span class="string">"digest"</span>, <span class="string">"heibai:123456"</span>.getBytes()) <span class="comment">//等价于 addauth 命令</span></span><br><span class="line">                .connectString(zkServerPath)</span><br><span class="line">                .sessionTimeoutMs(<span class="number">10000</span>).retryPolicy(retryPolicy)</span><br><span class="line">                .namespace(<span class="string">"workspace"</span>).build();</span><br><span class="line">        client.start();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 新建节点并赋予权限</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">createNodesWithAcl</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        List&lt;ACL&gt; aclList = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        <span class="comment">// 对密码进行加密</span></span><br><span class="line">        String digest1 = DigestAuthenticationProvider.generateDigest(<span class="string">"heibai:123456"</span>);</span><br><span class="line">        String digest2 = DigestAuthenticationProvider.generateDigest(<span class="string">"ying:123456"</span>);</span><br><span class="line">        Id user01 = <span class="keyword">new</span> Id(<span class="string">"digest"</span>, digest1);</span><br><span class="line">        Id user02 = <span class="keyword">new</span> Id(<span class="string">"digest"</span>, digest2);</span><br><span class="line">        <span class="comment">// 指定所有权限</span></span><br><span class="line">        aclList.add(<span class="keyword">new</span> ACL(Perms.ALL, user01));</span><br><span class="line">        <span class="comment">// 如果想要指定权限的组合，中间需要使用 | ,这里的|代表的是位运算中的 按位或</span></span><br><span class="line">        aclList.add(<span class="keyword">new</span> ACL(Perms.DELETE | Perms.CREATE, user02));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 创建节点</span></span><br><span class="line">        <span class="keyword">byte</span>[] data = <span class="string">"abc"</span>.getBytes();</span><br><span class="line">        client.create().creatingParentsIfNeeded()</span><br><span class="line">                .withMode(CreateMode.PERSISTENT)</span><br><span class="line">                .withACL(aclList, <span class="keyword">true</span>)</span><br><span class="line">                .forPath(nodePath, data);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 给已有节点设置权限,注意这会删除所有原来节点上已有的权限设置</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">SetAcl</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        String digest = DigestAuthenticationProvider.generateDigest(<span class="string">"admin:admin"</span>);</span><br><span class="line">        Id user = <span class="keyword">new</span> Id(<span class="string">"digest"</span>, digest);</span><br><span class="line">        client.setACL()</span><br><span class="line">                .withACL(Collections.singletonList(<span class="keyword">new</span> ACL(Perms.READ | Perms.DELETE, user)))</span><br><span class="line">                .forPath(nodePath);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 获取权限</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">getAcl</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        List&lt;ACL&gt; aclList = client.getACL().forPath(nodePath);</span><br><span class="line">        ACL acl = aclList.get(<span class="number">0</span>);</span><br><span class="line">        System.out.println(acl.getId().getId() </span><br><span class="line">                           + <span class="string">"是否有删读权限:"</span> + (acl.getPerms() == (Perms.READ | Perms.DELETE)));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@After</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">destroy</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (client != <span class="keyword">null</span>) &#123;</span><br><span class="line">            client.close();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>完整源码见本仓库： <a href="https://github.com/heibaiying/BigData-Notes/tree/master/code/Zookeeper/curator" target="_blank" rel="noopener">https://github.com/heibaiying/BigData-Notes/tree/master/code/Zookeeper/curator</a></p>
</blockquote>
]]></content>
      <categories>
        <category>技术</category>
        <category>Zookeeper</category>
      </categories>
      <tags>
        <tag>Zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title>Zookeeper Java 客户端</title>
    <url>/2021/03/17/Zookeeper_Java%E5%AE%A2%E6%88%B7%E7%AB%AFCurator/</url>
    <content><![CDATA[<h2 id="一、基本依赖"><a href="#一、基本依赖" class="headerlink" title="一、基本依赖"></a>一、基本依赖</h2><p>Curator 是 Netflix 公司开源的一个 Zookeeper 客户端，目前由 Apache 进行维护。与 Zookeeper 原生客户端相比，Curator 的抽象层次更高，功能也更加丰富，是目前 Zookeeper 使用范围最广的 Java 客户端。本篇文章主要讲解其基本使用，项目采用 Maven 构建，以单元测试的方法进行讲解，相关依赖如下：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--Curator 相关依赖--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.curator<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>curator-framework<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>4.0.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.curator<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>curator-recipes<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>4.0.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.zookeeper<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>zookeeper<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.4.13<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--单元测试相关依赖--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>4.12<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>完整源码见本仓库： <a href="https://github.com/heibaiying/BigData-Notes/tree/master/code/Zookeeper/curator" target="_blank" rel="noopener">https://github.com/heibaiying/BigData-Notes/tree/master/code/Zookeeper/curator</a></p>
</blockquote>
<h2 id="二、客户端相关操作"><a href="#二、客户端相关操作" class="headerlink" title="二、客户端相关操作"></a>二、客户端相关操作</h2><h3 id="2-1-创建客户端实例"><a href="#2-1-创建客户端实例" class="headerlink" title="2.1 创建客户端实例"></a>2.1 创建客户端实例</h3><p>这里使用 <code>@Before</code> 在单元测试执行前创建客户端实例，并使用 <code>@After</code> 在单元测试后关闭客户端连接。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">BasicOperation</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> CuratorFramework client = <span class="keyword">null</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String zkServerPath = <span class="string">"192.168.0.226:2181"</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String nodePath = <span class="string">"/hadoop/yarn"</span>;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Before</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">prepare</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 重试策略</span></span><br><span class="line">        RetryPolicy retryPolicy = <span class="keyword">new</span> RetryNTimes(<span class="number">3</span>, <span class="number">5000</span>);</span><br><span class="line">        client = CuratorFrameworkFactory.builder()</span><br><span class="line">        .connectString(zkServerPath)</span><br><span class="line">        .sessionTimeoutMs(<span class="number">10000</span>).retryPolicy(retryPolicy)</span><br><span class="line">        .namespace(<span class="string">"workspace"</span>).build();  <span class="comment">//指定命名空间后，client 的所有路径操作都会以/workspace 开头</span></span><br><span class="line">        client.start();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@After</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">destroy</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (client != <span class="keyword">null</span>) &#123;</span><br><span class="line">            client.close();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="2-2-重试策略"><a href="#2-2-重试策略" class="headerlink" title="2.2 重试策略"></a>2.2 重试策略</h3><p>在连接 Zookeeper 时，Curator 提供了多种重试策略以满足各种需求，所有重试策略均继承自 <code>RetryPolicy</code> 接口，如下图：</p>
<div align="center"> <img src="../pictures/curator-retry-policy.png"> </div>

<p>这些重试策略类主要分为以下两类：</p>
<ul>
<li><strong>RetryForever</strong> ：代表一直重试，直到连接成功；</li>
<li><strong>SleepingRetry</strong> ： 基于一定间隔时间的重试。这里以其子类 <code>ExponentialBackoffRetry</code> 为例说明，其构造器如下：</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> baseSleepTimeMs 重试之间等待的初始时间</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> maxRetries 最大重试次数</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> maxSleepMs 每次重试间隔的最长睡眠时间（毫秒）</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line">ExponentialBackoffRetry(<span class="keyword">int</span> baseSleepTimeMs, <span class="keyword">int</span> maxRetries, <span class="keyword">int</span> maxSleepMs)</span><br></pre></td></tr></table></figure>
<h3 id="2-3-判断服务状态"><a href="#2-3-判断服务状态" class="headerlink" title="2.3 判断服务状态"></a>2.3 判断服务状态</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line">public void getStatus() &#123;</span><br><span class="line">    <span class="type">CuratorFrameworkState</span> state = client.getState();</span><br><span class="line">    <span class="type">System</span>.out.println(<span class="string">"服务是否已经启动:"</span> + (state == <span class="type">CuratorFrameworkState</span>.<span class="type">STARTED</span>));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="三、节点增删改查"><a href="#三、节点增删改查" class="headerlink" title="三、节点增删改查"></a>三、节点增删改查</h2><h3 id="3-1-创建节点"><a href="#3-1-创建节点" class="headerlink" title="3.1 创建节点"></a>3.1 创建节点</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">createNodes</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    <span class="keyword">byte</span>[] data = <span class="string">"abc"</span>.getBytes();</span><br><span class="line">    client.create().creatingParentsIfNeeded()</span><br><span class="line">            .withMode(CreateMode.PERSISTENT)      <span class="comment">//节点类型</span></span><br><span class="line">            .withACL(ZooDefs.Ids.OPEN_ACL_UNSAFE)</span><br><span class="line">            .forPath(nodePath, data);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>创建时可以指定节点类型，这里的节点类型和 Zookeeper 原生的一致，全部类型定义在枚举类 <code>CreateMode</code> 中：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">enum</span> CreateMode &#123;</span><br><span class="line">    <span class="comment">// 永久节点</span></span><br><span class="line">    PERSISTENT (<span class="number">0</span>, <span class="keyword">false</span>, <span class="keyword">false</span>),</span><br><span class="line">    <span class="comment">//永久有序节点</span></span><br><span class="line">    PERSISTENT_SEQUENTIAL (<span class="number">2</span>, <span class="keyword">false</span>, <span class="keyword">true</span>),</span><br><span class="line">    <span class="comment">// 临时节点</span></span><br><span class="line">    EPHEMERAL (<span class="number">1</span>, <span class="keyword">true</span>, <span class="keyword">false</span>),</span><br><span class="line">    <span class="comment">// 临时有序节点</span></span><br><span class="line">    EPHEMERAL_SEQUENTIAL (<span class="number">3</span>, <span class="keyword">true</span>, <span class="keyword">true</span>);</span><br><span class="line">    ....</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="2-2-获取节点信息"><a href="#2-2-获取节点信息" class="headerlink" title="2.2 获取节点信息"></a>2.2 获取节点信息</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line">public void getNode() <span class="keyword">throws</span> <span class="type">Exception</span> &#123;</span><br><span class="line">    <span class="type">Stat</span> stat = <span class="keyword">new</span> <span class="type">Stat</span>();</span><br><span class="line">    byte[] data = client.getData().storingStatIn(stat).forPath(nodePath);</span><br><span class="line">    <span class="type">System</span>.out.println(<span class="string">"节点数据:"</span> + <span class="keyword">new</span> <span class="type">String</span>(data));</span><br><span class="line">    <span class="type">System</span>.out.println(<span class="string">"节点信息:"</span> + stat.toString());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>如上所示，节点信息被封装在 <code>Stat</code> 类中，其主要属性如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Stat</span> <span class="keyword">implements</span> <span class="title">Record</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">long</span> czxid;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">long</span> mzxid;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">long</span> ctime;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">long</span> mtime;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> version;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> cversion;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> aversion;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">long</span> ephemeralOwner;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> dataLength;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> numChildren;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">long</span> pzxid;</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>每个属性的含义如下：</p>
<table>
<thead>
<tr>
<th><strong>状态属性</strong></th>
<th><strong>说明</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>czxid</td>
<td>数据节点创建时的事务 ID</td>
</tr>
<tr>
<td>ctime</td>
<td>数据节点创建时的时间</td>
</tr>
<tr>
<td>mzxid</td>
<td>数据节点最后一次更新时的事务 ID</td>
</tr>
<tr>
<td>mtime</td>
<td>数据节点最后一次更新时的时间</td>
</tr>
<tr>
<td>pzxid</td>
<td>数据节点的子节点最后一次被修改时的事务 ID</td>
</tr>
<tr>
<td>cversion</td>
<td>子节点的更改次数</td>
</tr>
<tr>
<td>version</td>
<td>节点数据的更改次数</td>
</tr>
<tr>
<td>aversion</td>
<td>节点的 ACL 的更改次数</td>
</tr>
<tr>
<td>ephemeralOwner</td>
<td>如果节点是临时节点，则表示创建该节点的会话的 SessionID；如果节点是持久节点，则该属性值为 0</td>
</tr>
<tr>
<td>dataLength</td>
<td>数据内容的长度</td>
</tr>
<tr>
<td>numChildren</td>
<td>数据节点当前的子节点个数</td>
</tr>
</tbody>
</table>
<h3 id="2-3-获取子节点列表"><a href="#2-3-获取子节点列表" class="headerlink" title="2.3 获取子节点列表"></a>2.3 获取子节点列表</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">getChildrenNodes</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    List&lt;String&gt; childNodes = client.getChildren().forPath(<span class="string">"/hadoop"</span>);</span><br><span class="line">    <span class="keyword">for</span> (String s : childNodes) &#123;</span><br><span class="line">        System.out.println(s);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="2-4-更新节点"><a href="#2-4-更新节点" class="headerlink" title="2.4 更新节点"></a>2.4 更新节点</h3><p>更新时可以传入版本号也可以不传入，如果传入则类似于乐观锁机制，只有在版本号正确的时候才会被更新。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line">public void updateNode() <span class="keyword">throws</span> <span class="type">Exception</span> &#123;</span><br><span class="line">    byte[] newData = <span class="string">"defg"</span>.getBytes();</span><br><span class="line">    client.setData().withVersion(<span class="number">0</span>)     <span class="comment">// 传入版本号，如果版本号错误则拒绝更新操作,并抛出 BadVersion 异常</span></span><br><span class="line">            .forPath(nodePath, newData);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="2-5-删除节点"><a href="#2-5-删除节点" class="headerlink" title="2.5 删除节点"></a>2.5 删除节点</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">deleteNodes</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    client.delete()</span><br><span class="line">            .guaranteed()                <span class="comment">// 如果删除失败，那么在会继续执行，直到成功</span></span><br><span class="line">            .deletingChildrenIfNeeded()  <span class="comment">// 如果有子节点，则递归删除</span></span><br><span class="line">            .withVersion(<span class="number">0</span>)              <span class="comment">// 传入版本号，如果版本号错误则拒绝删除操作,并抛出 BadVersion 异常</span></span><br><span class="line">            .forPath(nodePath);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="2-6-判断节点是否存在"><a href="#2-6-判断节点是否存在" class="headerlink" title="2.6 判断节点是否存在"></a>2.6 判断节点是否存在</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">existNode</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    <span class="comment">// 如果节点存在则返回其状态信息如果不存在则为 null</span></span><br><span class="line">    Stat stat = client.checkExists().forPath(nodePath + <span class="string">"aa/bb/cc"</span>);</span><br><span class="line">    System.out.println(<span class="string">"节点是否存在:"</span> + !(stat == <span class="keyword">null</span>));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="三、监听事件"><a href="#三、监听事件" class="headerlink" title="三、监听事件"></a>三、监听事件</h2><h3 id="3-1-创建一次性监听"><a href="#3-1-创建一次性监听" class="headerlink" title="3.1 创建一次性监听"></a>3.1 创建一次性监听</h3><p>和 Zookeeper 原生监听一样，使用 <code>usingWatcher</code> 注册的监听是一次性的，即监听只会触发一次，触发后就销毁。示例如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">DisposableWatch</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    client.getData().usingWatcher(<span class="keyword">new</span> CuratorWatcher() &#123;</span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(WatchedEvent event)</span> </span>&#123;</span><br><span class="line">            System.out.println(<span class="string">"节点"</span> + event.getPath() + <span class="string">"发生了事件:"</span> + event.getType());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;).forPath(nodePath);</span><br><span class="line">    Thread.sleep(<span class="number">1000</span> * <span class="number">1000</span>);  <span class="comment">//休眠以观察测试效果</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="3-2-创建永久监听"><a href="#3-2-创建永久监听" class="headerlink" title="3.2 创建永久监听"></a>3.2 创建永久监听</h3><p>Curator 还提供了创建永久监听的 API，其使用方式如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">permanentWatch</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    <span class="comment">// 使用 NodeCache 包装节点，对其注册的监听作用于节点，且是永久性的</span></span><br><span class="line">    NodeCache nodeCache = <span class="keyword">new</span> NodeCache(client, nodePath);</span><br><span class="line">    <span class="comment">// 通常设置为 true, 代表创建 nodeCache 时,就去获取对应节点的值并缓存</span></span><br><span class="line">    nodeCache.start(<span class="keyword">true</span>);</span><br><span class="line">    nodeCache.getListenable().addListener(<span class="keyword">new</span> NodeCacheListener() &#123;</span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">nodeChanged</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            ChildData currentData = nodeCache.getCurrentData();</span><br><span class="line">            <span class="keyword">if</span> (currentData != <span class="keyword">null</span>) &#123;</span><br><span class="line">                System.out.println(<span class="string">"节点路径："</span> + currentData.getPath() +</span><br><span class="line">                        <span class="string">"数据："</span> + <span class="keyword">new</span> String(currentData.getData()));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br><span class="line">    Thread.sleep(<span class="number">1000</span> * <span class="number">1000</span>);  <span class="comment">//休眠以观察测试效果</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="3-3-监听子节点"><a href="#3-3-监听子节点" class="headerlink" title="3.3 监听子节点"></a>3.3 监听子节点</h3><p>这里以监听 <code>/hadoop</code> 下所有子节点为例，实现方式如下：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line">public void permanentChildrenNodesWatch() <span class="keyword">throws</span> <span class="type">Exception</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 第三个参数代表除了节点状态外，是否还缓存节点内容</span></span><br><span class="line">    <span class="type">PathChildrenCache</span> childrenCache = <span class="keyword">new</span> <span class="type">PathChildrenCache</span>(client, <span class="string">"/hadoop"</span>, <span class="literal">true</span>);</span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * StartMode 代表初始化方式:</span></span><br><span class="line"><span class="comment">         *    NORMAL: 异步初始化</span></span><br><span class="line"><span class="comment">         *    BUILD_INITIAL_CACHE: 同步初始化</span></span><br><span class="line"><span class="comment">         *    POST_INITIALIZED_EVENT: 异步并通知,初始化之后会触发 INITIALIZED 事件</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">    childrenCache.start(<span class="type">StartMode</span>.<span class="type">POST_INITIALIZED_EVENT</span>);</span><br><span class="line"></span><br><span class="line">    <span class="type">List</span>&lt;<span class="type">ChildData</span>&gt; childDataList = childrenCache.getCurrentData();</span><br><span class="line">    <span class="type">System</span>.out.println(<span class="string">"当前数据节点的子节点列表："</span>);</span><br><span class="line">    childDataList.forEach(x -&gt; <span class="type">System</span>.out.println(x.getPath()));</span><br><span class="line"></span><br><span class="line">    childrenCache.getListenable().addListener(<span class="keyword">new</span> <span class="type">PathChildrenCacheListener</span>() &#123;</span><br><span class="line">        public void childEvent(<span class="type">CuratorFramework</span> client, <span class="type">PathChildrenCacheEvent</span> event) &#123;</span><br><span class="line">            switch (event.getType()) &#123;</span><br><span class="line">                <span class="keyword">case</span> <span class="type">INITIALIZED</span>:</span><br><span class="line">                <span class="type">System</span>.out.println(<span class="string">"childrenCache 初始化完成"</span>);</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">                <span class="keyword">case</span> <span class="type">CHILD_ADDED</span>:</span><br><span class="line">                <span class="comment">// 需要注意的是: 即使是之前已经存在的子节点，也会触发该监听，因为会把该子节点加入 childrenCache 缓存中</span></span><br><span class="line">                <span class="type">System</span>.out.println(<span class="string">"增加子节点:"</span> + event.getData().getPath());</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">                <span class="keyword">case</span> <span class="type">CHILD_REMOVED</span>:</span><br><span class="line">                <span class="type">System</span>.out.println(<span class="string">"删除子节点:"</span> + event.getData().getPath());</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">                <span class="keyword">case</span> <span class="type">CHILD_UPDATED</span>:</span><br><span class="line">                <span class="type">System</span>.out.println(<span class="string">"被修改的子节点的路径:"</span> + event.getData().getPath());</span><br><span class="line">                <span class="type">System</span>.out.println(<span class="string">"修改后的数据:"</span> + <span class="keyword">new</span> <span class="type">String</span>(event.getData().getData()));</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br><span class="line">    <span class="type">Thread</span>.sleep(<span class="number">1000</span> * <span class="number">1000</span>); <span class="comment">//休眠以观察测试效果</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>技术</category>
        <category>Zookeeper</category>
      </categories>
      <tags>
        <tag>Zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title>Zookeeper常用Shell命令</title>
    <url>/2021/03/17/Zookeeper%E5%B8%B8%E7%94%A8Shell%E5%91%BD%E4%BB%A4/</url>
    <content><![CDATA[<h2 id="一、节点增删改查"><a href="#一、节点增删改查" class="headerlink" title="一、节点增删改查"></a>一、节点增删改查</h2><h3 id="1-1-启动服务和连接服务"><a href="#1-1-启动服务和连接服务" class="headerlink" title="1.1 启动服务和连接服务"></a>1.1 启动服务和连接服务</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 启动服务</span></span><br><span class="line">bin/zkServer.sh start</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">连接服务 不指定服务地址则默认连接到localhost:2181</span></span><br><span class="line">zkCli.sh -server hadoop001:2181</span><br></pre></td></tr></table></figure>
<h3 id="1-2-help命令"><a href="#1-2-help命令" class="headerlink" title="1.2 help命令"></a>1.2 help命令</h3><p>使用 <code>help</code> 可以查看所有命令及格式。</p>
<h3 id="1-3-查看节点列表"><a href="#1-3-查看节点列表" class="headerlink" title="1.3 查看节点列表"></a>1.3 查看节点列表</h3><p>查看节点列表有 <code>ls path</code> 和 <code>ls2 path</code> 两个命令，后者是前者的增强，不仅可以查看指定路径下的所有节点，还可以查看当前节点的信息。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 0] ls /</span><br><span class="line">[cluster, controller_epoch, brokers, storm, zookeeper, admin,  ...]</span><br><span class="line">[zk: localhost:2181(CONNECTED) 1] ls2 /</span><br><span class="line">[cluster, controller_epoch, brokers, storm, zookeeper, admin, ....]</span><br><span class="line">cZxid = 0x0</span><br><span class="line">ctime = Thu Jan 01 08:00:00 CST 1970</span><br><span class="line">mZxid = 0x0</span><br><span class="line">mtime = Thu Jan 01 08:00:00 CST 1970</span><br><span class="line">pZxid = 0x130</span><br><span class="line">cversion = 19</span><br><span class="line">dataVersion = 0</span><br><span class="line">aclVersion = 0</span><br><span class="line">ephemeralOwner = 0x0</span><br><span class="line">dataLength = 0</span><br><span class="line">numChildren = 11</span><br></pre></td></tr></table></figure>
<h3 id="1-4-新增节点"><a href="#1-4-新增节点" class="headerlink" title="1.4 新增节点"></a>1.4 新增节点</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">create [-s] [-e] path data acl   #其中-s 为有序节点，-e 临时节点</span><br></pre></td></tr></table></figure>
<p>创建节点并写入数据：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">create /hadoop 123456</span><br></pre></td></tr></table></figure>
<p>创建有序节点，此时创建的节点名为指定节点名 + 自增序号：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 23] create -s /a  "aaa"</span><br><span class="line">Created /a0000000022</span><br><span class="line">[zk: localhost:2181(CONNECTED) 24] create -s /b  "bbb"</span><br><span class="line">Created /b0000000023</span><br><span class="line">[zk: localhost:2181(CONNECTED) 25] create -s /c  "ccc"</span><br><span class="line">Created /c0000000024</span><br></pre></td></tr></table></figure>
<p>创建临时节点，临时节点会在会话过期后被删除：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 26] create -e /tmp  "tmp"</span><br><span class="line">Created /tmp</span><br></pre></td></tr></table></figure>
<h3 id="1-5-查看节点"><a href="#1-5-查看节点" class="headerlink" title="1.5 查看节点"></a>1.5 查看节点</h3><h4 id="1-获取节点数据"><a href="#1-获取节点数据" class="headerlink" title="1. 获取节点数据"></a>1. 获取节点数据</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 格式</span></span><br><span class="line">get path [watch]</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 31] get /hadoop</span><br><span class="line">123456   #节点数据</span><br><span class="line">cZxid = 0x14b</span><br><span class="line">ctime = Fri May 24 17:03:06 CST 2019</span><br><span class="line">mZxid = 0x14b</span><br><span class="line">mtime = Fri May 24 17:03:06 CST 2019</span><br><span class="line">pZxid = 0x14b</span><br><span class="line">cversion = 0</span><br><span class="line">dataVersion = 0</span><br><span class="line">aclVersion = 0</span><br><span class="line">ephemeralOwner = 0x0</span><br><span class="line">dataLength = 6</span><br><span class="line">numChildren = 0</span><br></pre></td></tr></table></figure>
<p>节点各个属性如下表。其中一个重要的概念是 Zxid(ZooKeeper Transaction  Id)，ZooKeeper 节点的每一次更改都具有唯一的 Zxid，如果 Zxid1 小于 Zxid2，则 Zxid1 的更改发生在 Zxid2 更改之前。</p>
<table>
<thead>
<tr>
<th><strong>状态属性</strong></th>
<th><strong>说明</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>cZxid</td>
<td>数据节点创建时的事务 ID</td>
</tr>
<tr>
<td>ctime</td>
<td>数据节点创建时的时间</td>
</tr>
<tr>
<td>mZxid</td>
<td>数据节点最后一次更新时的事务 ID</td>
</tr>
<tr>
<td>mtime</td>
<td>数据节点最后一次更新时的时间</td>
</tr>
<tr>
<td>pZxid</td>
<td>数据节点的子节点最后一次被修改时的事务 ID</td>
</tr>
<tr>
<td>cversion</td>
<td>子节点的更改次数</td>
</tr>
<tr>
<td>dataVersion</td>
<td>节点数据的更改次数</td>
</tr>
<tr>
<td>aclVersion</td>
<td>节点的 ACL 的更改次数</td>
</tr>
<tr>
<td>ephemeralOwner</td>
<td>如果节点是临时节点，则表示创建该节点的会话的 SessionID；如果节点是持久节点，则该属性值为 0</td>
</tr>
<tr>
<td>dataLength</td>
<td>数据内容的长度</td>
</tr>
<tr>
<td>numChildren</td>
<td>数据节点当前的子节点个数</td>
</tr>
</tbody>
</table>
<h4 id="2-查看节点状态"><a href="#2-查看节点状态" class="headerlink" title="2. 查看节点状态"></a>2. 查看节点状态</h4><p>可以使用 <code>stat</code> 命令查看节点状态，它的返回值和 <code>get</code> 命令类似，但不会返回节点数据。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 32] stat /hadoop</span><br><span class="line">cZxid = 0x14b</span><br><span class="line">ctime = Fri May 24 17:03:06 CST 2019</span><br><span class="line">mZxid = 0x14b</span><br><span class="line">mtime = Fri May 24 17:03:06 CST 2019</span><br><span class="line">pZxid = 0x14b</span><br><span class="line">cversion = 0</span><br><span class="line">dataVersion = 0</span><br><span class="line">aclVersion = 0</span><br><span class="line">ephemeralOwner = 0x0</span><br><span class="line">dataLength = 6</span><br><span class="line">numChildren = 0</span><br></pre></td></tr></table></figure>
<h3 id="1-6-更新节点"><a href="#1-6-更新节点" class="headerlink" title="1.6 更新节点"></a>1.6 更新节点</h3><p>更新节点的命令是 <code>set</code>，可以直接进行修改，如下：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 33] set /hadoop 345</span><br><span class="line">cZxid = 0x14b</span><br><span class="line">ctime = Fri May 24 17:03:06 CST 2019</span><br><span class="line">mZxid = 0x14c</span><br><span class="line">mtime = Fri May 24 17:13:05 CST 2019</span><br><span class="line">pZxid = 0x14b</span><br><span class="line">cversion = 0</span><br><span class="line">dataVersion = 1  # 注意更改后此时版本号为 1，默认创建时为 0</span><br><span class="line">aclVersion = 0</span><br><span class="line">ephemeralOwner = 0x0</span><br><span class="line">dataLength = 3</span><br><span class="line">numChildren = 0</span><br></pre></td></tr></table></figure>
<p>也可以基于版本号进行更改，此时类似于乐观锁机制，当你传入的数据版本号 (dataVersion) 和当前节点的数据版本号不符合时，zookeeper 会拒绝本次修改：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 34] set /hadoop 678 0</span><br><span class="line">version No is not valid : /hadoop    #无效的版本号</span><br></pre></td></tr></table></figure>
<h3 id="1-7-删除节点"><a href="#1-7-删除节点" class="headerlink" title="1.7 删除节点"></a>1.7 删除节点</h3><p>删除节点的语法如下：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">delete path [version]</span><br></pre></td></tr></table></figure>
<p>和更新节点数据一样，也可以传入版本号，当你传入的数据版本号 (dataVersion) 和当前节点的数据版本号不符合时，zookeeper 不会执行删除操作。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 36] delete /hadoop 0</span><br><span class="line">version No is not valid : /hadoop   #无效的版本号</span><br><span class="line">[zk: localhost:2181(CONNECTED) 37] delete /hadoop 1</span><br><span class="line">[zk: localhost:2181(CONNECTED) 38]</span><br></pre></td></tr></table></figure>
<p>要想删除某个节点及其所有后代节点，可以使用递归删除，命令为 <code>rmr path</code>。</p>
<h2 id="二、监听器"><a href="#二、监听器" class="headerlink" title="二、监听器"></a>二、监听器</h2><h3 id="2-1-get-path-watch"><a href="#2-1-get-path-watch" class="headerlink" title="2.1 get path [watch]"></a>2.1 get path [watch]</h3><p>使用 <code>get path [watch]</code> 注册的监听器能够在节点内容发生改变的时候，向客户端发出通知。需要注意的是 zookeeper 的触发器是一次性的 (One-time trigger)，即触发一次后就会立即失效。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 4] get /hadoop  watch</span><br><span class="line">[zk: localhost:2181(CONNECTED) 5] set /hadoop 45678</span><br><span class="line">WATCHER::</span><br><span class="line">WatchedEvent state:SyncConnected type:NodeDataChanged path:/hadoop  #节点值改变</span><br></pre></td></tr></table></figure>
<h3 id="2-2-stat-path-watch"><a href="#2-2-stat-path-watch" class="headerlink" title="2.2 stat path [watch]"></a>2.2 stat path [watch]</h3><p>使用 <code>stat path [watch]</code> 注册的监听器能够在节点状态发生改变的时候，向客户端发出通知。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 7] stat /hadoop watch</span><br><span class="line">[zk: localhost:2181(CONNECTED) 8] set /hadoop 112233</span><br><span class="line">WATCHER::</span><br><span class="line">WatchedEvent state:SyncConnected type:NodeDataChanged path:/hadoop  #节点值改变</span><br></pre></td></tr></table></figure>
<h3 id="2-3-ls-ls2-path-watch"><a href="#2-3-ls-ls2-path-watch" class="headerlink" title="2.3 ls\ls2 path  [watch]"></a>2.3 ls\ls2 path  [watch]</h3><p>使用 <code>ls path [watch]</code> 或 <code>ls2 path [watch]</code> 注册的监听器能够监听该节点下所有<strong>子节点</strong>的增加和删除操作。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 9] ls /hadoop watch</span><br><span class="line">[]</span><br><span class="line">[zk: localhost:2181(CONNECTED) 10] create  /hadoop/yarn "aaa"</span><br><span class="line">WATCHER::</span><br><span class="line">WatchedEvent state:SyncConnected type:NodeChildrenChanged path:/hadoop</span><br></pre></td></tr></table></figure>
<h2 id="三、-zookeeper-四字命令"><a href="#三、-zookeeper-四字命令" class="headerlink" title="三、 zookeeper 四字命令"></a>三、 zookeeper 四字命令</h2><table>
<thead>
<tr>
<th>命令</th>
<th>功能描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>conf</td>
<td>打印服务配置的详细信息。</td>
</tr>
<tr>
<td>cons</td>
<td>列出连接到此服务器的所有客户端的完整连接/会话详细信息。包括接收/发送的数据包数量，会话 ID，操作延迟，上次执行的操作等信息。</td>
</tr>
<tr>
<td>dump</td>
<td>列出未完成的会话和临时节点。这只适用于 Leader 节点。</td>
</tr>
<tr>
<td>envi</td>
<td>打印服务环境的详细信息。</td>
</tr>
<tr>
<td>ruok</td>
<td>测试服务是否处于正确状态。如果正确则返回“imok”，否则不做任何相应。</td>
</tr>
<tr>
<td>stat</td>
<td>列出服务器和连接客户端的简要详细信息。</td>
</tr>
<tr>
<td>wchs</td>
<td>列出所有 watch 的简单信息。</td>
</tr>
<tr>
<td>wchc</td>
<td>按会话列出服务器 watch 的详细信息。</td>
</tr>
<tr>
<td>wchp</td>
<td>按路径列出服务器 watch 的详细信息。</td>
</tr>
</tbody>
</table>
<blockquote>
<p>更多四字命令可以参阅官方文档：<a href="https://zookeeper.apache.org/doc/current/zookeeperAdmin.html" target="_blank" rel="noopener">https://zookeeper.apache.org/doc/current/zookeeperAdmin.html</a></p>
</blockquote>
<p>使用前需要使用 <code>yum install nc</code> 安装 nc 命令，使用示例如下：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@hadoop001 bin]# echo stat | nc localhost 2181</span><br><span class="line">Zookeeper version: 3.4.13-2d71af4dbe22557fda74f9a9b4309b15a7487f03, </span><br><span class="line">built on 06/29/2018 04:05 GMT</span><br><span class="line">Clients:</span><br><span class="line"> /0:0:0:0:0:0:0:1:50584[1](queued=0,recved=371,sent=371)</span><br><span class="line"> /0:0:0:0:0:0:0:1:50656[0](queued=0,recved=1,sent=0)</span><br><span class="line">Latency min/avg/max: 0/0/19</span><br><span class="line">Received: 372</span><br><span class="line">Sent: 371</span><br><span class="line">Connections: 2</span><br><span class="line">Outstanding: 0</span><br><span class="line">Zxid: 0x150</span><br><span class="line">Mode: standalone</span><br><span class="line">Node count: 167</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>技术</category>
        <category>Zookeeper</category>
      </categories>
      <tags>
        <tag>Zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title>Zookeeper简介及核心概念</title>
    <url>/2021/03/17/Zookeeper%E7%AE%80%E4%BB%8B%E5%8F%8A%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5/</url>
    <content><![CDATA[<h2 id="一、Zookeeper简介"><a href="#一、Zookeeper简介" class="headerlink" title="一、Zookeeper简介"></a>一、Zookeeper简介</h2><p>Zookeeper 是一个开源的分布式协调服务，目前由 Apache 进行维护。Zookeeper 可以用于实现分布式系统中常见的发布/订阅、负载均衡、命令服务、分布式协调/通知、集群管理、Master 选举、分布式锁和分布式队列等功能。它具有以下特性：</p>
<ul>
<li><strong>顺序一致性</strong>：从一个客户端发起的事务请求，最终都会严格按照其发起顺序被应用到 Zookeeper 中；</li>
<li><strong>原子性</strong>：所有事务请求的处理结果在整个集群中所有机器上都是一致的；不存在部分机器应用了该事务，而另一部分没有应用的情况；</li>
<li><strong>单一视图</strong>：所有客户端看到的服务端数据模型都是一致的；</li>
<li><strong>可靠性</strong>：一旦服务端成功应用了一个事务，则其引起的改变会一直保留，直到被另外一个事务所更改；</li>
<li><strong>实时性</strong>：一旦一个事务被成功应用后，Zookeeper 可以保证客户端立即可以读取到这个事务变更后的最新状态的数据。</li>
</ul>
<h2 id="二、Zookeeper设计目标"><a href="#二、Zookeeper设计目标" class="headerlink" title="二、Zookeeper设计目标"></a>二、Zookeeper设计目标</h2><p>Zookeeper 致力于为那些高吞吐的大型分布式系统提供一个高性能、高可用、且具有严格顺序访问控制能力的分布式协调服务。它具有以下四个目标：</p>
<h3 id="2-1-目标一：简单的数据模型"><a href="#2-1-目标一：简单的数据模型" class="headerlink" title="2.1 目标一：简单的数据模型"></a>2.1 目标一：简单的数据模型</h3><p>Zookeeper 通过树形结构来存储数据，它由一系列被称为 ZNode 的数据节点组成，类似于常见的文件系统。不过和常见的文件系统不同，Zookeeper 将数据全量存储在内存中，以此来实现高吞吐，减少访问延迟。</p>
<div align="center"> <img src="../pictures/zookeeper-zknamespace.jpg"> </div>

<h3 id="2-2-目标二：构建集群"><a href="#2-2-目标二：构建集群" class="headerlink" title="2.2 目标二：构建集群"></a>2.2 目标二：构建集群</h3><p>可以由一组 Zookeeper 服务构成 Zookeeper 集群，集群中每台机器都会单独在内存中维护自身的状态，并且每台机器之间都保持着通讯，只要集群中有半数机器能够正常工作，那么整个集群就可以正常提供服务。</p>
<div align="center"> <img src="../pictures/zookeeper-zkservice.jpg"> </div>

<h3 id="2-3-目标三：顺序访问"><a href="#2-3-目标三：顺序访问" class="headerlink" title="2.3 目标三：顺序访问"></a>2.3 目标三：顺序访问</h3><p>对于来自客户端的每个更新请求，Zookeeper 都会分配一个全局唯一的递增 ID，这个 ID 反映了所有事务请求的先后顺序。</p>
<h3 id="2-4-目标四：高性能高可用"><a href="#2-4-目标四：高性能高可用" class="headerlink" title="2.4 目标四：高性能高可用"></a>2.4 目标四：高性能高可用</h3><p>ZooKeeper 将数据存全量储在内存中以保持高性能，并通过服务集群来实现高可用，由于 Zookeeper 的所有更新和删除都是基于事务的，所以其在读多写少的应用场景中有着很高的性能表现。</p>
<h2 id="三、核心概念"><a href="#三、核心概念" class="headerlink" title="三、核心概念"></a>三、核心概念</h2><h3 id="3-1-集群角色"><a href="#3-1-集群角色" class="headerlink" title="3.1 集群角色"></a>3.1 集群角色</h3><p>Zookeeper 集群中的机器分为以下三种角色：</p>
<ul>
<li><strong>Leader</strong> ：为客户端提供读写服务，并维护集群状态，它是由集群选举所产生的；</li>
<li><strong>Follower</strong> ：为客户端提供读写服务，并定期向 Leader 汇报自己的节点状态。同时也参与写操作“过半写成功”的策略和 Leader 的选举；</li>
<li><strong>Observer</strong> ：为客户端提供读写服务，并定期向 Leader 汇报自己的节点状态，但不参与写操作“过半写成功”的策略和 Leader 的选举，因此 Observer 可以在不影响写性能的情况下提升集群的读性能。</li>
</ul>
<h3 id="3-2-会话"><a href="#3-2-会话" class="headerlink" title="3.2 会话"></a>3.2 会话</h3><p>Zookeeper 客户端通过 TCP 长连接连接到服务集群，会话 (Session) 从第一次连接开始就已经建立，之后通过心跳检测机制来保持有效的会话状态。通过这个连接，客户端可以发送请求并接收响应，同时也可以接收到 Watch 事件的通知。</p>
<p>关于会话中另外一个核心的概念是 sessionTimeOut(会话超时时间)，当由于网络故障或者客户端主动断开等原因，导致连接断开，此时只要在会话超时时间之内重新建立连接，则之前创建的会话依然有效。</p>
<h3 id="3-3-数据节点"><a href="#3-3-数据节点" class="headerlink" title="3.3 数据节点"></a>3.3 数据节点</h3><p>Zookeeper 数据模型是由一系列基本数据单元 <code>Znode</code>(数据节点) 组成的节点树，其中根节点为 <code>/</code>。每个节点上都会保存自己的数据和节点信息。Zookeeper 中节点可以分为两大类：</p>
<ul>
<li><strong>持久节点</strong> ：节点一旦创建，除非被主动删除，否则一直存在；</li>
<li><strong>临时节点</strong> ：一旦创建该节点的客户端会话失效，则所有该客户端创建的临时节点都会被删除。</li>
</ul>
<p>临时节点和持久节点都可以添加一个特殊的属性：<code>SEQUENTIAL</code>，代表该节点是否具有递增属性。如果指定该属性，那么在这个节点创建时，Zookeeper 会自动在其节点名称后面追加一个由父节点维护的递增数字。</p>
<h3 id="3-4-节点信息"><a href="#3-4-节点信息" class="headerlink" title="3.4 节点信息"></a>3.4 节点信息</h3><p>每个 ZNode 节点在存储数据的同时，都会维护一个叫做 <code>Stat</code> 的数据结构，里面存储了关于该节点的全部状态信息。如下：</p>
<table>
<thead>
<tr>
<th><strong>状态属性</strong></th>
<th><strong>说明</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>czxid</td>
<td>数据节点创建时的事务 ID</td>
</tr>
<tr>
<td>ctime</td>
<td>数据节点创建时的时间</td>
</tr>
<tr>
<td>mzxid</td>
<td>数据节点最后一次更新时的事务 ID</td>
</tr>
<tr>
<td>mtime</td>
<td>数据节点最后一次更新时的时间</td>
</tr>
<tr>
<td>pzxid</td>
<td>数据节点的子节点最后一次被修改时的事务 ID</td>
</tr>
<tr>
<td>cversion</td>
<td>子节点的更改次数</td>
</tr>
<tr>
<td>version</td>
<td>节点数据的更改次数</td>
</tr>
<tr>
<td>aversion</td>
<td>节点的 ACL 的更改次数</td>
</tr>
<tr>
<td>ephemeralOwner</td>
<td>如果节点是临时节点，则表示创建该节点的会话的 SessionID；如果节点是持久节点，则该属性值为 0</td>
</tr>
<tr>
<td>dataLength</td>
<td>数据内容的长度</td>
</tr>
<tr>
<td>numChildren</td>
<td>数据节点当前的子节点个数</td>
</tr>
</tbody>
</table>
<h3 id="3-5-Watcher"><a href="#3-5-Watcher" class="headerlink" title="3.5 Watcher"></a>3.5 Watcher</h3><p>Zookeeper 中一个常用的功能是 Watcher(事件监听器)，它允许用户在指定节点上针对感兴趣的事件注册监听，当事件发生时，监听器会被触发，并将事件信息推送到客户端。该机制是 Zookeeper 实现分布式协调服务的重要特性。</p>
<h3 id="3-6-ACL"><a href="#3-6-ACL" class="headerlink" title="3.6 ACL"></a>3.6 ACL</h3><p>Zookeeper 采用 ACL(Access Control Lists) 策略来进行权限控制，类似于 UNIX 文件系统的权限控制。它定义了如下五种权限：</p>
<ul>
<li><strong>CREATE</strong>：允许创建子节点；</li>
<li><strong>READ</strong>：允许从节点获取数据并列出其子节点；</li>
<li><strong>WRITE</strong>：允许为节点设置数据；</li>
<li><strong>DELETE</strong>：允许删除子节点；</li>
<li><strong>ADMIN</strong>：允许为节点设置权限。  </li>
</ul>
<h2 id="四、ZAB协议"><a href="#四、ZAB协议" class="headerlink" title="四、ZAB协议"></a>四、ZAB协议</h2><h3 id="4-1-ZAB协议与数据一致性"><a href="#4-1-ZAB协议与数据一致性" class="headerlink" title="4.1 ZAB协议与数据一致性"></a>4.1 ZAB协议与数据一致性</h3><p>ZAB 协议是 Zookeeper 专门设计的一种支持崩溃恢复的原子广播协议。通过该协议，Zookeepe 基于主从模式的系统架构来保持集群中各个副本之间数据的一致性。具体如下：</p>
<p>Zookeeper 使用一个单一的主进程来接收并处理客户端的所有事务请求，并采用原子广播协议将数据状态的变更以事务 Proposal 的形式广播到所有的副本进程上去。如下图：</p>
<div align="center"> <img src="../pictures/zookeeper-zkcomponents.jpg"> </div>

<p>具体流程如下：</p>
<p>所有的事务请求必须由唯一的 Leader 服务来处理，Leader 服务将事务请求转换为事务 Proposal，并将该 Proposal 分发给集群中所有的 Follower 服务。如果有半数的 Follower 服务进行了正确的反馈，那么 Leader 就会再次向所有的 Follower 发出 Commit 消息，要求将前一个 Proposal 进行提交。</p>
<h3 id="4-2-ZAB协议的内容"><a href="#4-2-ZAB协议的内容" class="headerlink" title="4.2  ZAB协议的内容"></a>4.2  ZAB协议的内容</h3><p>ZAB 协议包括两种基本的模式，分别是崩溃恢复和消息广播：</p>
<h4 id="1-崩溃恢复"><a href="#1-崩溃恢复" class="headerlink" title="1. 崩溃恢复"></a>1. 崩溃恢复</h4><p>当整个服务框架在启动过程中，或者当 Leader 服务器出现异常时，ZAB 协议就会进入恢复模式，通过过半选举机制产生新的 Leader，之后其他机器将从新的 Leader 上同步状态，当有过半机器完成状态同步后，就退出恢复模式，进入消息广播模式。</p>
<h4 id="2-消息广播"><a href="#2-消息广播" class="headerlink" title="2. 消息广播"></a>2. 消息广播</h4><p>ZAB 协议的消息广播过程使用的是原子广播协议。在整个消息的广播过程中，Leader 服务器会每个事物请求生成对应的 Proposal，并为其分配一个全局唯一的递增的事务 ID(ZXID)，之后再对其进行广播。具体过程如下：</p>
<p>Leader 服务会为每一个 Follower 服务器分配一个单独的队列，然后将事务 Proposal 依次放入队列中，并根据 FIFO(先进先出) 的策略进行消息发送。Follower 服务在接收到 Proposal 后，会将其以事务日志的形式写入本地磁盘中，并在写入成功后反馈给 Leader 一个 Ack 响应。当 Leader 接收到超过半数 Follower 的 Ack 响应后，就会广播一个 Commit 消息给所有的 Follower 以通知其进行事务提交，之后 Leader 自身也会完成对事务的提交。而每一个 Follower 则在接收到 Commit 消息后，完成事务的提交。</p>
<div align="center"> <img src="../pictures/zookeeper-brocast.jpg"> </div>



<h2 id="五、Zookeeper的典型应用场景"><a href="#五、Zookeeper的典型应用场景" class="headerlink" title="五、Zookeeper的典型应用场景"></a>五、Zookeeper的典型应用场景</h2><h3 id="5-1数据的发布-订阅"><a href="#5-1数据的发布-订阅" class="headerlink" title="5.1数据的发布/订阅"></a>5.1数据的发布/订阅</h3><p>数据的发布/订阅系统，通常也用作配置中心。在分布式系统中，你可能有成千上万个服务节点，如果想要对所有服务的某项配置进行更改，由于数据节点过多，你不可逐台进行修改，而应该在设计时采用统一的配置中心。之后发布者只需要将新的配置发送到配置中心，所有服务节点即可自动下载并进行更新，从而实现配置的集中管理和动态更新。</p>
<p>Zookeeper 通过 Watcher 机制可以实现数据的发布和订阅。分布式系统的所有的服务节点可以对某个 ZNode 注册监听，之后只需要将新的配置写入该 ZNode，所有服务节点都会收到该事件。</p>
<h3 id="5-2-命名服务"><a href="#5-2-命名服务" class="headerlink" title="5.2 命名服务"></a>5.2 命名服务</h3><p>在分布式系统中，通常需要一个全局唯一的名字，如生成全局唯一的订单号等，Zookeeper 可以通过顺序节点的特性来生成全局唯一 ID，从而可以对分布式系统提供命名服务。</p>
<h3 id="5-3-Master选举"><a href="#5-3-Master选举" class="headerlink" title="5.3 Master选举"></a>5.3 Master选举</h3><p>分布式系统一个重要的模式就是主从模式 (Master/Salves)，Zookeeper 可以用于该模式下的 Matser 选举。可以让所有服务节点去竞争性地创建同一个 ZNode，由于 Zookeeper 不能有路径相同的 ZNode，必然只有一个服务节点能够创建成功，这样该服务节点就可以成为 Master 节点。</p>
<h3 id="5-4-分布式锁"><a href="#5-4-分布式锁" class="headerlink" title="5.4 分布式锁"></a>5.4 分布式锁</h3><p>可以通过 Zookeeper 的临时节点和 Watcher 机制来实现分布式锁，这里以排它锁为例进行说明：</p>
<p>分布式系统的所有服务节点可以竞争性地去创建同一个临时 ZNode，由于 Zookeeper 不能有路径相同的 ZNode，必然只有一个服务节点能够创建成功，此时可以认为该节点获得了锁。其他没有获得锁的服务节点通过在该 ZNode 上注册监听，从而当锁释放时再去竞争获得锁。锁的释放情况有以下两种：</p>
<ul>
<li>当正常执行完业务逻辑后，客户端主动将临时 ZNode 删除，此时锁被释放；</li>
<li>当获得锁的客户端发生宕机时，临时 ZNode 会被自动删除，此时认为锁已经释放。</li>
</ul>
<p>当锁被释放后，其他服务节点则再次去竞争性地进行创建，但每次都只有一个服务节点能够获取到锁，这就是排他锁。</p>
<h3 id="5-5-集群管理"><a href="#5-5-集群管理" class="headerlink" title="5.5 集群管理"></a>5.5 集群管理</h3><p>Zookeeper 还能解决大多数分布式系统中的问题：</p>
<ul>
<li>如可以通过创建临时节点来建立心跳检测机制。如果分布式系统的某个服务节点宕机了，则其持有的会话会超时，此时该临时节点会被删除，相应的监听事件就会被触发。</li>
<li>分布式系统的每个服务节点还可以将自己的节点状态写入临时节点，从而完成状态报告或节点工作进度汇报。</li>
<li>通过数据的订阅和发布功能，Zookeeper 还能对分布式系统进行模块的解耦和任务的调度。</li>
<li>通过监听机制，还能对分布式系统的服务节点进行动态上下线，从而实现服务的动态扩容。</li>
</ul>
<p><br></p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol>
<li>倪超 . 从 Paxos 到 Zookeeper——分布式一致性原理与实践 . 电子工业出版社 . 2015-02-01</li>
</ol>
]]></content>
      <categories>
        <category>技术</category>
        <category>Zookeeper</category>
      </categories>
      <tags>
        <tag>Zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title>Welcome to 我的世界，这里是菜鸟进阶ing……</title>
    <url>/2021/03/15/hello-world/</url>
    <content><![CDATA[<p>欢迎来到我的世界，这是进阶测试版本(*￣︶￣)<br><!--Welcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).--></p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure>
<!--More info: [Writing](https://hexo.io/docs/writing.html)-->
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<!--More info: [Server](https://hexo.io/docs/server.html)-->
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<!--More info: [Generating](https://hexo.io/docs/generating.html)-->
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<!--More info: [Deployment](https://hexo.io/docs/deployment.html)-->
]]></content>
      <tags>
        <tag>Java</tag>
        <tag>Python</tag>
        <tag>GitHub</tag>
      </tags>
  </entry>
  <entry>
    <title>Welcome to 我的世界，这里是菜鸟进阶第一篇文章……</title>
    <url>/2021/03/15/test/</url>
    <content><![CDATA[<p>Hello，什么鬼，我是测试！</p>
]]></content>
      <categories>
        <category>测试ing……</category>
      </categories>
  </entry>
  <entry>
    <title>散文欣赏</title>
    <url>/2021/03/15/ormosia/</url>
    <content><![CDATA[<p>红豆生南国<br>人生如逆旅，吾也亦不过是行人<br>残秋月夜<br>一条雨巷，半边情愁<br>有牵挂的时候，承受不起任何意外<br>风中，我在等你<br>秋风，秋雨，秋之意<br>退而及进的芳华<br>我们都是久别的人啊<br>老井<br>青春遗落，思念难忘<br>你若强大，一切皆浮云</p>
]]></content>
      <categories>
        <category>散文</category>
      </categories>
      <tags>
        <tag>散文</tag>
      </tags>
  </entry>
</search>
