<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Spark,">










<meta name="description" content="一、简介1.1 多数据源支持Spark 支持以下六个核心数据源，同时 Spark 社区还提供了多达上百种数据源的读取方式，能够满足绝大部分使用场景。  CSV JSON Parquet ORC JDBC/ODBC connections Plain-text files   注：以下所有测试文件均可从本仓库的resources 目录进行下载  1.2 读数据格式所有读取 API 遵循以下调用格式：">
<meta name="keywords" content="Spark">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark SQL 外部数据源">
<meta property="og:url" content="http://yoursite.com/2021/03/17/SparkSQL外部数据源/index.html">
<meta property="og:site_name" content="菜鸟@进阶记">
<meta property="og:description" content="一、简介1.1 多数据源支持Spark 支持以下六个核心数据源，同时 Spark 社区还提供了多达上百种数据源的读取方式，能够满足绝大部分使用场景。  CSV JSON Parquet ORC JDBC/ODBC connections Plain-text files   注：以下所有测试文件均可从本仓库的resources 目录进行下载  1.2 读数据格式所有读取 API 遵循以下调用格式：">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://yoursite.com/2021/03/17/pictures/spark-mysql-分区上下限.png">
<meta property="og:image" content="http://yoursite.com/2021/03/17/pictures/spark-分区.png">
<meta property="og:updated_time" content="2021-03-17T15:08:40.247Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Spark SQL 外部数据源">
<meta name="twitter:description" content="一、简介1.1 多数据源支持Spark 支持以下六个核心数据源，同时 Spark 社区还提供了多达上百种数据源的读取方式，能够满足绝大部分使用场景。  CSV JSON Parquet ORC JDBC/ODBC connections Plain-text files   注：以下所有测试文件均可从本仓库的resources 目录进行下载  1.2 读数据格式所有读取 API 遵循以下调用格式：">
<meta name="twitter:image" content="http://yoursite.com/2021/03/17/pictures/spark-mysql-分区上下限.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
  (function(){
        if(''){
            if (prompt('请输入文章密码') !== ''){
                alert('密码错误！');
                history.back();
            }
        }
    })();
</script>



  <link rel="canonical" href="http://yoursite.com/2021/03/17/SparkSQL外部数据源/">





  <title>Spark SQL 外部数据源 | 菜鸟@进阶记</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

   <a href="https://github.com/LY464782123"><img style="position: absolute; top: 0; right: 0; border: 0;" src="https://s3.amazonaws.com/github/ribbons/forkme_right_red_aa0000.png" alt="Fork me on GitHub"></a>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">菜鸟@进阶记</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-notes">
          <a href="/notes/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-edit"></i> <br>
            
            笔记
          </a>
        </li>
      
        
        <li class="menu-item menu-item-codes">
          <a href="/codes/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-code"></i> <br>
            
            编程
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-search">
          <a href="/search/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-search"></i> <br>
            
            搜索
          </a>
        </li>
      
        
        <li class="menu-item menu-item-commonweal">
          <a href="/404/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-heartbeat"></i> <br>
            
            公益404
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2021/03/17/SparkSQL外部数据源/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mr.刘">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/2.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="菜鸟@进阶记">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Spark SQL 外部数据源</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2021-03-17T23:05:09+08:00">
                2021-03-17
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/notes/" itemprop="url" rel="index">
                    <span itemprop="name">notes</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/notes/Spark/" itemprop="url" rel="index">
                    <span itemprop="name">Spark</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2021/03/17/SparkSQL外部数据源/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2021/03/17/SparkSQL外部数据源/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="page-pv">阅读数 <i class="fa fa-file-o"></i>
            <span class="busuanzi-value" id="busuanzi_value_page_pv"></span>次
            </span>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h2 id="一、简介"><a href="#一、简介" class="headerlink" title="一、简介"></a>一、简介</h2><h3 id="1-1-多数据源支持"><a href="#1-1-多数据源支持" class="headerlink" title="1.1 多数据源支持"></a>1.1 多数据源支持</h3><p>Spark 支持以下六个核心数据源，同时 Spark 社区还提供了多达上百种数据源的读取方式，能够满足绝大部分使用场景。</p>
<ul>
<li>CSV</li>
<li>JSON</li>
<li>Parquet</li>
<li>ORC</li>
<li>JDBC/ODBC connections</li>
<li>Plain-text files</li>
</ul>
<blockquote>
<p>注：以下所有测试文件均可从本仓库的<a href="https://github.com/heibaiying/BigData-Notes/tree/master/resources" target="_blank" rel="noopener">resources</a> 目录进行下载</p>
</blockquote>
<h3 id="1-2-读数据格式"><a href="#1-2-读数据格式" class="headerlink" title="1.2 读数据格式"></a>1.2 读数据格式</h3><p>所有读取 API 遵循以下调用格式：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 格式</span></span><br><span class="line"><span class="type">DataFrameReader</span>.format(...).option(<span class="string">"key"</span>, <span class="string">"value"</span>).schema(...).load()</span><br><span class="line"></span><br><span class="line"><span class="comment">// 示例</span></span><br><span class="line">spark.read.format(<span class="string">"csv"</span>)</span><br><span class="line">.option(<span class="string">"mode"</span>, <span class="string">"FAILFAST"</span>)          <span class="comment">// 读取模式</span></span><br><span class="line">.option(<span class="string">"inferSchema"</span>, <span class="string">"true"</span>)       <span class="comment">// 是否自动推断 schema</span></span><br><span class="line">.option(<span class="string">"path"</span>, <span class="string">"path/to/file(s)"</span>)   <span class="comment">// 文件路径</span></span><br><span class="line">.schema(someSchema)                  <span class="comment">// 使用预定义的 schema      </span></span><br><span class="line">.load()</span><br></pre></td></tr></table></figure>
<p>读取模式有以下三种可选项：</p>
<table>
<thead>
<tr>
<th>读模式</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>permissive</code></td>
<td>当遇到损坏的记录时，将其所有字段设置为 null，并将所有损坏的记录放在名为 _corruption t_record 的字符串列中</td>
</tr>
<tr>
<td><code>dropMalformed</code></td>
<td>删除格式不正确的行</td>
</tr>
<tr>
<td><code>failFast</code></td>
<td>遇到格式不正确的数据时立即失败</td>
</tr>
</tbody>
</table>
<h3 id="1-3-写数据格式"><a href="#1-3-写数据格式" class="headerlink" title="1.3 写数据格式"></a>1.3 写数据格式</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 格式</span></span><br><span class="line"><span class="type">DataFrameWriter</span>.format(...).option(...).partitionBy(...).bucketBy(...).sortBy(...).save()</span><br><span class="line"></span><br><span class="line"><span class="comment">//示例</span></span><br><span class="line">dataframe.write.format(<span class="string">"csv"</span>)</span><br><span class="line">.option(<span class="string">"mode"</span>, <span class="string">"OVERWRITE"</span>)         <span class="comment">//写模式</span></span><br><span class="line">.option(<span class="string">"dateFormat"</span>, <span class="string">"yyyy-MM-dd"</span>)  <span class="comment">//日期格式</span></span><br><span class="line">.option(<span class="string">"path"</span>, <span class="string">"path/to/file(s)"</span>)</span><br><span class="line">.save()</span><br></pre></td></tr></table></figure>
<p>写数据模式有以下四种可选项：</p>
<table>
<thead>
<tr>
<th style="text-align:left">Scala/Java</th>
<th style="text-align:left">描述</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>SaveMode.ErrorIfExists</code></td>
<td style="text-align:left">如果给定的路径已经存在文件，则抛出异常，这是写数据默认的模式</td>
</tr>
<tr>
<td style="text-align:left"><code>SaveMode.Append</code></td>
<td style="text-align:left">数据以追加的方式写入</td>
</tr>
<tr>
<td style="text-align:left"><code>SaveMode.Overwrite</code></td>
<td style="text-align:left">数据以覆盖的方式写入</td>
</tr>
<tr>
<td style="text-align:left"><code>SaveMode.Ignore</code></td>
<td style="text-align:left">如果给定的路径已经存在文件，则不做任何操作</td>
</tr>
</tbody>
</table>
<p><br></p>
<h2 id="二、CSV"><a href="#二、CSV" class="headerlink" title="二、CSV"></a>二、CSV</h2><p>CSV 是一种常见的文本文件格式，其中每一行表示一条记录，记录中的每个字段用逗号分隔。</p>
<h3 id="2-1-读取CSV文件"><a href="#2-1-读取CSV文件" class="headerlink" title="2.1 读取CSV文件"></a>2.1 读取CSV文件</h3><p>自动推断类型读取读取示例：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">spark.read.format(<span class="string">"csv"</span>)</span><br><span class="line">.option(<span class="string">"header"</span>, <span class="string">"false"</span>)        <span class="comment">// 文件中的第一行是否为列的名称</span></span><br><span class="line">.option(<span class="string">"mode"</span>, <span class="string">"FAILFAST"</span>)      <span class="comment">// 是否快速失败</span></span><br><span class="line">.option(<span class="string">"inferSchema"</span>, <span class="string">"true"</span>)   <span class="comment">// 是否自动推断 schema</span></span><br><span class="line">.load(<span class="string">"/usr/file/csv/dept.csv"</span>)</span><br><span class="line">.show()</span><br></pre></td></tr></table></figure>
<p>使用预定义类型：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.&#123;<span class="type">StructField</span>, <span class="type">StructType</span>, <span class="type">StringType</span>,<span class="type">LongType</span>&#125;</span><br><span class="line"><span class="comment">//预定义数据格式</span></span><br><span class="line"><span class="keyword">val</span> myManualSchema = <span class="keyword">new</span> <span class="type">StructType</span>(<span class="type">Array</span>(</span><br><span class="line">    <span class="type">StructField</span>(<span class="string">"deptno"</span>, <span class="type">LongType</span>, nullable = <span class="literal">false</span>),</span><br><span class="line">    <span class="type">StructField</span>(<span class="string">"dname"</span>, <span class="type">StringType</span>,nullable = <span class="literal">true</span>),</span><br><span class="line">    <span class="type">StructField</span>(<span class="string">"loc"</span>, <span class="type">StringType</span>,nullable = <span class="literal">true</span>)</span><br><span class="line">))</span><br><span class="line">spark.read.format(<span class="string">"csv"</span>)</span><br><span class="line">.option(<span class="string">"mode"</span>, <span class="string">"FAILFAST"</span>)</span><br><span class="line">.schema(myManualSchema)</span><br><span class="line">.load(<span class="string">"/usr/file/csv/dept.csv"</span>)</span><br><span class="line">.show()</span><br></pre></td></tr></table></figure>
<h3 id="2-2-写入CSV文件"><a href="#2-2-写入CSV文件" class="headerlink" title="2.2 写入CSV文件"></a>2.2 写入CSV文件</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.write.format(<span class="string">"csv"</span>).mode(<span class="string">"overwrite"</span>).save(<span class="string">"/tmp/csv/dept2"</span>)</span><br></pre></td></tr></table></figure>
<p>也可以指定具体的分隔符：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.write.format(<span class="string">"csv"</span>).mode(<span class="string">"overwrite"</span>).option(<span class="string">"sep"</span>, <span class="string">"\t"</span>).save(<span class="string">"/tmp/csv/dept2"</span>)</span><br></pre></td></tr></table></figure>
<h3 id="2-3-可选配置"><a href="#2-3-可选配置" class="headerlink" title="2.3 可选配置"></a>2.3 可选配置</h3><p>为节省主文篇幅，所有读写配置项见文末 9.1 小节。</p>
<p><br></p>
<h2 id="三、JSON"><a href="#三、JSON" class="headerlink" title="三、JSON"></a>三、JSON</h2><h3 id="3-1-读取JSON文件"><a href="#3-1-读取JSON文件" class="headerlink" title="3.1 读取JSON文件"></a>3.1 读取JSON文件</h3><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.read.format("json").option("mode", "FAILFAST").load("/usr/file/json/dept.json").show(5)</span><br></pre></td></tr></table></figure>
<p>需要注意的是：默认不支持一条数据记录跨越多行 (如下)，可以通过配置 <code>multiLine</code> 为 <code>true</code> 来进行更改，其默认值为 <code>false</code>。</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 默认支持单行</span></span><br><span class="line">&#123;<span class="attr">"DEPTNO"</span>: <span class="number">10</span>,<span class="attr">"DNAME"</span>: <span class="string">"ACCOUNTING"</span>,<span class="attr">"LOC"</span>: <span class="string">"NEW YORK"</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//默认不支持多行</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">"DEPTNO"</span>: <span class="number">10</span>,</span><br><span class="line">  <span class="attr">"DNAME"</span>: <span class="string">"ACCOUNTING"</span>,</span><br><span class="line">  <span class="attr">"LOC"</span>: <span class="string">"NEW YORK"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="3-2-写入JSON文件"><a href="#3-2-写入JSON文件" class="headerlink" title="3.2 写入JSON文件"></a>3.2 写入JSON文件</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.write.format(<span class="string">"json"</span>).mode(<span class="string">"overwrite"</span>).save(<span class="string">"/tmp/spark/json/dept"</span>)</span><br></pre></td></tr></table></figure>
<h3 id="3-3-可选配置"><a href="#3-3-可选配置" class="headerlink" title="3.3 可选配置"></a>3.3 可选配置</h3><p>为节省主文篇幅，所有读写配置项见文末 9.2 小节。</p>
<p><br></p>
<h2 id="四、Parquet"><a href="#四、Parquet" class="headerlink" title="四、Parquet"></a>四、Parquet</h2><p> Parquet 是一个开源的面向列的数据存储，它提供了多种存储优化，允许读取单独的列非整个文件，这不仅节省了存储空间而且提升了读取效率，它是 Spark 是默认的文件格式。</p>
<h3 id="4-1-读取Parquet文件"><a href="#4-1-读取Parquet文件" class="headerlink" title="4.1 读取Parquet文件"></a>4.1 读取Parquet文件</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.read.format(<span class="string">"parquet"</span>).load(<span class="string">"/usr/file/parquet/dept.parquet"</span>).show(<span class="number">5</span>)</span><br></pre></td></tr></table></figure>
<h3 id="2-2-写入Parquet文件"><a href="#2-2-写入Parquet文件" class="headerlink" title="2.2 写入Parquet文件"></a>2.2 写入Parquet文件</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.write.format(<span class="string">"parquet"</span>).mode(<span class="string">"overwrite"</span>).save(<span class="string">"/tmp/spark/parquet/dept"</span>)</span><br></pre></td></tr></table></figure>
<h3 id="2-3-可选配置-1"><a href="#2-3-可选配置-1" class="headerlink" title="2.3 可选配置"></a>2.3 可选配置</h3><p>Parquet 文件有着自己的存储规则，因此其可选配置项比较少，常用的有如下两个：</p>
<table>
<thead>
<tr>
<th>读写操作</th>
<th>配置项</th>
<th>可选值</th>
<th>默认值</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>Write</td>
<td>compression or codec</td>
<td>None,<br>uncompressed,<br>bzip2,<br>deflate, gzip,<br>lz4, or snappy</td>
<td>None</td>
<td>压缩文件格式</td>
</tr>
<tr>
<td>Read</td>
<td>mergeSchema</td>
<td>true, false</td>
<td>取决于配置项 <code>spark.sql.parquet.mergeSchema</code></td>
<td>当为真时，Parquet 数据源将所有数据文件收集的 Schema 合并在一起，否则将从摘要文件中选择 Schema，如果没有可用的摘要文件，则从随机数据文件中选择 Schema。</td>
</tr>
</tbody>
</table>
<blockquote>
<p>更多可选配置可以参阅官方文档：<a href="https://spark.apache.org/docs/latest/sql-data-sources-parquet.html" target="_blank" rel="noopener">https://spark.apache.org/docs/latest/sql-data-sources-parquet.html</a></p>
</blockquote>
<p><br></p>
<h2 id="五、ORC"><a href="#五、ORC" class="headerlink" title="五、ORC"></a>五、ORC</h2><p>ORC 是一种自描述的、类型感知的列文件格式，它针对大型数据的读写进行了优化，也是大数据中常用的文件格式。</p>
<h3 id="5-1-读取ORC文件"><a href="#5-1-读取ORC文件" class="headerlink" title="5.1 读取ORC文件"></a>5.1 读取ORC文件</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.read.format(<span class="string">"orc"</span>).load(<span class="string">"/usr/file/orc/dept.orc"</span>).show(<span class="number">5</span>)</span><br></pre></td></tr></table></figure>
<h3 id="4-2-写入ORC文件"><a href="#4-2-写入ORC文件" class="headerlink" title="4.2 写入ORC文件"></a>4.2 写入ORC文件</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">csvFile.write.format(<span class="string">"orc"</span>).mode(<span class="string">"overwrite"</span>).save(<span class="string">"/tmp/spark/orc/dept"</span>)</span><br></pre></td></tr></table></figure>
<p><br></p>
<h2 id="六、SQL-Databases"><a href="#六、SQL-Databases" class="headerlink" title="六、SQL Databases"></a>六、SQL Databases</h2><p>Spark 同样支持与传统的关系型数据库进行数据读写。但是 Spark 程序默认是没有提供数据库驱动的，所以在使用前需要将对应的数据库驱动上传到安装目录下的 <code>jars</code> 目录中。下面示例使用的是 Mysql 数据库，使用前需要将对应的 <code>mysql-connector-java-x.x.x.jar</code> 上传到 <code>jars</code> 目录下。</p>
<h3 id="6-1-读取数据"><a href="#6-1-读取数据" class="headerlink" title="6.1 读取数据"></a>6.1 读取数据</h3><p>读取全表数据示例如下，这里的 <code>help_keyword</code> 是 mysql 内置的字典表，只有 <code>help_keyword_id</code> 和 <code>name</code> 两个字段。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">spark.read</span><br><span class="line">.format(<span class="string">"jdbc"</span>)</span><br><span class="line">.option(<span class="string">"driver"</span>, <span class="string">"com.mysql.jdbc.Driver"</span>)            <span class="comment">//驱动</span></span><br><span class="line">.option(<span class="string">"url"</span>, <span class="string">"jdbc:mysql://127.0.0.1:3306/mysql"</span>)   <span class="comment">//数据库地址</span></span><br><span class="line">.option(<span class="string">"dbtable"</span>, <span class="string">"help_keyword"</span>)                    <span class="comment">//表名</span></span><br><span class="line">.option(<span class="string">"user"</span>, <span class="string">"root"</span>).option(<span class="string">"password"</span>,<span class="string">"root"</span>).load().show(<span class="number">10</span>)</span><br></pre></td></tr></table></figure>
<p>从查询结果读取数据：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> pushDownQuery = <span class="string">""</span><span class="string">"(SELECT * FROM help_keyword WHERE help_keyword_id &lt;20) AS help_keywords"</span><span class="string">""</span></span><br><span class="line">spark.read.format(<span class="string">"jdbc"</span>)</span><br><span class="line">.option(<span class="string">"url"</span>, <span class="string">"jdbc:mysql://127.0.0.1:3306/mysql"</span>)</span><br><span class="line">.option(<span class="string">"driver"</span>, <span class="string">"com.mysql.jdbc.Driver"</span>)</span><br><span class="line">.option(<span class="string">"user"</span>, <span class="string">"root"</span>).option(<span class="string">"password"</span>, <span class="string">"root"</span>)</span><br><span class="line">.option(<span class="string">"dbtable"</span>, pushDownQuery)</span><br><span class="line">.load().show()</span><br><span class="line"></span><br><span class="line"><span class="comment">//输出</span></span><br><span class="line">+---------------+-----------+</span><br><span class="line">|help_keyword_id|       name|</span><br><span class="line">+---------------+-----------+</span><br><span class="line">|              <span class="number">0</span>|         &lt;&gt;|</span><br><span class="line">|              <span class="number">1</span>|     <span class="type">ACTION</span>|</span><br><span class="line">|              <span class="number">2</span>|        <span class="type">ADD</span>|</span><br><span class="line">|              <span class="number">3</span>|<span class="type">AES_DECRYPT</span>|</span><br><span class="line">|              <span class="number">4</span>|<span class="type">AES_ENCRYPT</span>|</span><br><span class="line">|              <span class="number">5</span>|      <span class="type">AFTER</span>|</span><br><span class="line">|              <span class="number">6</span>|    <span class="type">AGAINST</span>|</span><br><span class="line">|              <span class="number">7</span>|  <span class="type">AGGREGATE</span>|</span><br><span class="line">|              <span class="number">8</span>|  <span class="type">ALGORITHM</span>|</span><br><span class="line">|              <span class="number">9</span>|        <span class="type">ALL</span>|</span><br><span class="line">|             <span class="number">10</span>|      <span class="type">ALTER</span>|</span><br><span class="line">|             <span class="number">11</span>|    <span class="type">ANALYSE</span>|</span><br><span class="line">|             <span class="number">12</span>|    <span class="type">ANALYZE</span>|</span><br><span class="line">|             <span class="number">13</span>|        <span class="type">AND</span>|</span><br><span class="line">|             <span class="number">14</span>|    <span class="type">ARCHIVE</span>|</span><br><span class="line">|             <span class="number">15</span>|       <span class="type">AREA</span>|</span><br><span class="line">|             <span class="number">16</span>|         <span class="type">AS</span>|</span><br><span class="line">|             <span class="number">17</span>|   <span class="type">ASBINARY</span>|</span><br><span class="line">|             <span class="number">18</span>|        <span class="type">ASC</span>|</span><br><span class="line">|             <span class="number">19</span>|     <span class="type">ASTEXT</span>|</span><br><span class="line">+---------------+-----------+</span><br></pre></td></tr></table></figure>
<p>也可以使用如下的写法进行数据的过滤：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> props = <span class="keyword">new</span> java.util.<span class="type">Properties</span></span><br><span class="line">props.setProperty(<span class="string">"driver"</span>, <span class="string">"com.mysql.jdbc.Driver"</span>)</span><br><span class="line">props.setProperty(<span class="string">"user"</span>, <span class="string">"root"</span>)</span><br><span class="line">props.setProperty(<span class="string">"password"</span>, <span class="string">"root"</span>)</span><br><span class="line"><span class="keyword">val</span> predicates = <span class="type">Array</span>(<span class="string">"help_keyword_id &lt; 10  OR name = 'WHEN'"</span>)   <span class="comment">//指定数据过滤条件</span></span><br><span class="line">spark.read.jdbc(<span class="string">"jdbc:mysql://127.0.0.1:3306/mysql"</span>, <span class="string">"help_keyword"</span>, predicates, props).show() </span><br><span class="line"></span><br><span class="line"><span class="comment">//输出：</span></span><br><span class="line">+---------------+-----------+</span><br><span class="line">|help_keyword_id|       name|</span><br><span class="line">+---------------+-----------+</span><br><span class="line">|              <span class="number">0</span>|         &lt;&gt;|</span><br><span class="line">|              <span class="number">1</span>|     <span class="type">ACTION</span>|</span><br><span class="line">|              <span class="number">2</span>|        <span class="type">ADD</span>|</span><br><span class="line">|              <span class="number">3</span>|<span class="type">AES_DECRYPT</span>|</span><br><span class="line">|              <span class="number">4</span>|<span class="type">AES_ENCRYPT</span>|</span><br><span class="line">|              <span class="number">5</span>|      <span class="type">AFTER</span>|</span><br><span class="line">|              <span class="number">6</span>|    <span class="type">AGAINST</span>|</span><br><span class="line">|              <span class="number">7</span>|  <span class="type">AGGREGATE</span>|</span><br><span class="line">|              <span class="number">8</span>|  <span class="type">ALGORITHM</span>|</span><br><span class="line">|              <span class="number">9</span>|        <span class="type">ALL</span>|</span><br><span class="line">|            <span class="number">604</span>|       <span class="type">WHEN</span>|</span><br><span class="line">+---------------+-----------+</span><br></pre></td></tr></table></figure>
<p>可以使用 <code>numPartitions</code> 指定读取数据的并行度：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">option(<span class="string">"numPartitions"</span>, <span class="number">10</span>)</span><br></pre></td></tr></table></figure>
<p>在这里，除了可以指定分区外，还可以设置上界和下界，任何小于下界的值都会被分配在第一个分区中，任何大于上界的值都会被分配在最后一个分区中。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> colName = <span class="string">"help_keyword_id"</span>   <span class="comment">//用于判断上下界的列</span></span><br><span class="line"><span class="keyword">val</span> lowerBound = <span class="number">300</span>L    <span class="comment">//下界</span></span><br><span class="line"><span class="keyword">val</span> upperBound = <span class="number">500</span>L    <span class="comment">//上界</span></span><br><span class="line"><span class="keyword">val</span> numPartitions = <span class="number">10</span>   <span class="comment">//分区综述</span></span><br><span class="line"><span class="keyword">val</span> jdbcDf = spark.read.jdbc(<span class="string">"jdbc:mysql://127.0.0.1:3306/mysql"</span>,<span class="string">"help_keyword"</span>,</span><br><span class="line">                             colName,lowerBound,upperBound,numPartitions,props)</span><br></pre></td></tr></table></figure>
<p>想要验证分区内容，可以使用 <code>mapPartitionsWithIndex</code> 这个算子，代码如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">jdbcDf.rdd.mapPartitionsWithIndex((index, iterator) =&gt; &#123;</span><br><span class="line">    <span class="keyword">val</span> buffer = <span class="keyword">new</span> <span class="type">ListBuffer</span>[<span class="type">String</span>]</span><br><span class="line">    <span class="keyword">while</span> (iterator.hasNext) &#123;</span><br><span class="line">        buffer.append(index + <span class="string">"分区:"</span> + iterator.next())</span><br><span class="line">    &#125;</span><br><span class="line">    buffer.toIterator</span><br><span class="line">&#125;).foreach(println)</span><br></pre></td></tr></table></figure>
<p>执行结果如下：<code>help_keyword</code> 这张表只有 600 条左右的数据，本来数据应该均匀分布在 10 个分区，但是 0 分区里面却有 319 条数据，这是因为设置了下限，所有小于 300 的数据都会被限制在第一个分区，即 0 分区。同理所有大于 500 的数据被分配在 9 分区，即最后一个分区。</p>
<div align="center"> <img src="../pictures/spark-mysql-分区上下限.png"> </div>

<h3 id="6-2-写入数据"><a href="#6-2-写入数据" class="headerlink" title="6.2 写入数据"></a>6.2 写入数据</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> df = spark.read.format(<span class="string">"json"</span>).load(<span class="string">"/usr/file/json/emp.json"</span>)</span><br><span class="line">df.write</span><br><span class="line">.format(<span class="string">"jdbc"</span>)</span><br><span class="line">.option(<span class="string">"url"</span>, <span class="string">"jdbc:mysql://127.0.0.1:3306/mysql"</span>)</span><br><span class="line">.option(<span class="string">"user"</span>, <span class="string">"root"</span>).option(<span class="string">"password"</span>, <span class="string">"root"</span>)</span><br><span class="line">.option(<span class="string">"dbtable"</span>, <span class="string">"emp"</span>)</span><br><span class="line">.save()</span><br></pre></td></tr></table></figure>
<p><br></p>
<h2 id="七、Text"><a href="#七、Text" class="headerlink" title="七、Text"></a>七、Text</h2><p>Text 文件在读写性能方面并没有任何优势，且不能表达明确的数据结构，所以其使用的比较少，读写操作如下：</p>
<h3 id="7-1-读取Text数据"><a href="#7-1-读取Text数据" class="headerlink" title="7.1 读取Text数据"></a>7.1 读取Text数据</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.read.textFile(<span class="string">"/usr/file/txt/dept.txt"</span>).show()</span><br></pre></td></tr></table></figure>
<h3 id="7-2-写入Text数据"><a href="#7-2-写入Text数据" class="headerlink" title="7.2 写入Text数据"></a>7.2 写入Text数据</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.write.text(<span class="string">"/tmp/spark/txt/dept"</span>)</span><br></pre></td></tr></table></figure>
<p><br></p>
<h2 id="八、数据读写高级特性"><a href="#八、数据读写高级特性" class="headerlink" title="八、数据读写高级特性"></a>八、数据读写高级特性</h2><h3 id="8-1-并行读"><a href="#8-1-并行读" class="headerlink" title="8.1 并行读"></a>8.1 并行读</h3><p>多个 Executors 不能同时读取同一个文件，但它们可以同时读取不同的文件。这意味着当您从一个包含多个文件的文件夹中读取数据时，这些文件中的每一个都将成为 DataFrame 中的一个分区，并由可用的 Executors 并行读取。</p>
<h3 id="8-2-并行写"><a href="#8-2-并行写" class="headerlink" title="8.2 并行写"></a>8.2 并行写</h3><p>写入的文件或数据的数量取决于写入数据时 DataFrame 拥有的分区数量。默认情况下，每个数据分区写一个文件。</p>
<h3 id="8-3-分区写入"><a href="#8-3-分区写入" class="headerlink" title="8.3 分区写入"></a>8.3 分区写入</h3><p>分区和分桶这两个概念和 Hive 中分区表和分桶表是一致的。都是将数据按照一定规则进行拆分存储。需要注意的是 <code>partitionBy</code> 指定的分区和 RDD 中分区不是一个概念：这里的<strong>分区表现为输出目录的子目录</strong>，数据分别存储在对应的子目录中。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> df = spark.read.format(<span class="string">"json"</span>).load(<span class="string">"/usr/file/json/emp.json"</span>)</span><br><span class="line">df.write.mode(<span class="string">"overwrite"</span>).partitionBy(<span class="string">"deptno"</span>).save(<span class="string">"/tmp/spark/partitions"</span>)</span><br></pre></td></tr></table></figure>
<p>输出结果如下：可以看到输出被按照部门编号分为三个子目录，子目录中才是对应的输出文件。</p>
<div align="center"> <img src="../pictures/spark-分区.png"> </div>

<h3 id="8-3-分桶写入"><a href="#8-3-分桶写入" class="headerlink" title="8.3 分桶写入"></a>8.3 分桶写入</h3><p>分桶写入就是将数据按照指定的列和桶数进行散列，目前分桶写入只支持保存为表，实际上这就是 Hive 的分桶表。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> numberBuckets = <span class="number">10</span></span><br><span class="line"><span class="keyword">val</span> columnToBucketBy = <span class="string">"empno"</span></span><br><span class="line">df.write.format(<span class="string">"parquet"</span>).mode(<span class="string">"overwrite"</span>)</span><br><span class="line">.bucketBy(numberBuckets, columnToBucketBy).saveAsTable(<span class="string">"bucketedFiles"</span>)</span><br></pre></td></tr></table></figure>
<h3 id="8-5-文件大小管理"><a href="#8-5-文件大小管理" class="headerlink" title="8.5 文件大小管理"></a>8.5 文件大小管理</h3><p>如果写入产生小文件数量过多，这时会产生大量的元数据开销。Spark 和 HDFS 一样，都不能很好的处理这个问题，这被称为“small file problem”。同时数据文件也不能过大，否则在查询时会有不必要的性能开销，因此要把文件大小控制在一个合理的范围内。</p>
<p>在上文我们已经介绍过可以通过分区数量来控制生成文件的数量，从而间接控制文件大小。Spark 2.2 引入了一种新的方法，以更自动化的方式控制文件大小，这就是 <code>maxRecordsPerFile</code> 参数，它允许你通过控制写入文件的记录数来控制文件大小。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"> <span class="comment">// Spark 将确保文件最多包含 5000 条记录</span></span><br><span class="line">df.write.option(“maxRecordsPerFile”, <span class="number">5000</span>)</span><br></pre></td></tr></table></figure>
<p><br></p>
<h2 id="九、可选配置附录"><a href="#九、可选配置附录" class="headerlink" title="九、可选配置附录"></a>九、可选配置附录</h2><h3 id="9-1-CSV读写可选配置"><a href="#9-1-CSV读写可选配置" class="headerlink" title="9.1 CSV读写可选配置"></a>9.1 CSV读写可选配置</h3><table>
<thead>
<tr>
<th>读\写操作</th>
<th>配置项</th>
<th>可选值</th>
<th>默认值</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>Both</td>
<td>seq</td>
<td>任意字符</td>
<td><code>,</code>(逗号)</td>
<td>分隔符</td>
</tr>
<tr>
<td>Both</td>
<td>header</td>
<td>true, false</td>
<td>false</td>
<td>文件中的第一行是否为列的名称。</td>
</tr>
<tr>
<td>Read</td>
<td>escape</td>
<td>任意字符</td>
<td>\</td>
<td>转义字符</td>
</tr>
<tr>
<td>Read</td>
<td>inferSchema</td>
<td>true, false</td>
<td>false</td>
<td>是否自动推断列类型</td>
</tr>
<tr>
<td>Read</td>
<td>ignoreLeadingWhiteSpace</td>
<td>true, false</td>
<td>false</td>
<td>是否跳过值前面的空格</td>
</tr>
<tr>
<td>Both</td>
<td>ignoreTrailingWhiteSpace</td>
<td>true, false</td>
<td>false</td>
<td>是否跳过值后面的空格</td>
</tr>
<tr>
<td>Both</td>
<td>nullValue</td>
<td>任意字符</td>
<td>“”</td>
<td>声明文件中哪个字符表示空值</td>
</tr>
<tr>
<td>Both</td>
<td>nanValue</td>
<td>任意字符</td>
<td>NaN</td>
<td>声明哪个值表示 NaN 或者缺省值</td>
</tr>
<tr>
<td>Both</td>
<td>positiveInf</td>
<td>任意字符</td>
<td>Inf</td>
<td>正无穷</td>
</tr>
<tr>
<td>Both</td>
<td>negativeInf</td>
<td>任意字符</td>
<td>-Inf</td>
<td>负无穷</td>
</tr>
<tr>
<td>Both</td>
<td>compression or codec</td>
<td>None,<br>uncompressed,<br>bzip2, deflate,<br>gzip, lz4, or<br>snappy</td>
<td>none</td>
<td>文件压缩格式</td>
</tr>
<tr>
<td>Both</td>
<td>dateFormat</td>
<td>任何能转换为 Java 的 <br>SimpleDataFormat 的字符串</td>
<td>yyyy-MM-dd</td>
<td>日期格式</td>
</tr>
<tr>
<td>Both</td>
<td>timestampFormat</td>
<td>任何能转换为 Java 的 <br>SimpleDataFormat 的字符串</td>
<td>yyyy-MMdd’T’HH:mm:ss.SSSZZ</td>
<td>时间戳格式</td>
</tr>
<tr>
<td>Read</td>
<td>maxColumns</td>
<td>任意整数</td>
<td>20480</td>
<td>声明文件中的最大列数</td>
</tr>
<tr>
<td>Read</td>
<td>maxCharsPerColumn</td>
<td>任意整数</td>
<td>1000000</td>
<td>声明一个列中的最大字符数。</td>
</tr>
<tr>
<td>Read</td>
<td>escapeQuotes</td>
<td>true, false</td>
<td>true</td>
<td>是否应该转义行中的引号。</td>
</tr>
<tr>
<td>Read</td>
<td>maxMalformedLogPerPartition</td>
<td>任意整数</td>
<td>10</td>
<td>声明每个分区中最多允许多少条格式错误的数据，超过这个值后格式错误的数据将不会被读取</td>
</tr>
<tr>
<td>Write</td>
<td>quoteAll</td>
<td>true, false</td>
<td>false</td>
<td>指定是否应该将所有值都括在引号中，而不只是转义具有引号字符的值。</td>
</tr>
<tr>
<td>Read</td>
<td>multiLine</td>
<td>true, false</td>
<td>false</td>
<td>是否允许每条完整记录跨域多行</td>
</tr>
</tbody>
</table>
<h3 id="9-2-JSON读写可选配置"><a href="#9-2-JSON读写可选配置" class="headerlink" title="9.2 JSON读写可选配置"></a>9.2 JSON读写可选配置</h3><table>
<thead>
<tr>
<th>读\写操作</th>
<th>配置项</th>
<th>可选值</th>
<th>默认值</th>
</tr>
</thead>
<tbody>
<tr>
<td>Both</td>
<td>compression or codec</td>
<td>None,<br>uncompressed,<br>bzip2, deflate,<br>gzip, lz4, or<br>snappy</td>
<td>none</td>
</tr>
<tr>
<td>Both</td>
<td>dateFormat</td>
<td>任何能转换为 Java 的 SimpleDataFormat 的字符串</td>
<td>yyyy-MM-dd</td>
</tr>
<tr>
<td>Both</td>
<td>timestampFormat</td>
<td>任何能转换为 Java 的 SimpleDataFormat 的字符串</td>
<td>yyyy-MMdd’T’HH:mm:ss.SSSZZ</td>
</tr>
<tr>
<td>Read</td>
<td>primitiveAsString</td>
<td>true, false</td>
<td>false</td>
</tr>
<tr>
<td>Read</td>
<td>allowComments</td>
<td>true, false</td>
<td>false</td>
</tr>
<tr>
<td>Read</td>
<td>allowUnquotedFieldNames</td>
<td>true, false</td>
<td>false</td>
</tr>
<tr>
<td>Read</td>
<td>allowSingleQuotes</td>
<td>true, false</td>
<td>true</td>
</tr>
<tr>
<td>Read</td>
<td>allowNumericLeadingZeros</td>
<td>true, false</td>
<td>false</td>
</tr>
<tr>
<td>Read</td>
<td>allowBackslashEscapingAnyCharacter</td>
<td>true, false</td>
<td>false</td>
</tr>
<tr>
<td>Read</td>
<td>columnNameOfCorruptRecord</td>
<td>true, false</td>
<td>Value of spark.sql.column&amp;NameOf</td>
</tr>
<tr>
<td>Read</td>
<td>multiLine</td>
<td>true, false</td>
<td>false</td>
</tr>
</tbody>
</table>
<h3 id="9-3-数据库读写可选配置"><a href="#9-3-数据库读写可选配置" class="headerlink" title="9.3 数据库读写可选配置"></a>9.3 数据库读写可选配置</h3><table>
<thead>
<tr>
<th>属性名称</th>
<th>含义</th>
</tr>
</thead>
<tbody>
<tr>
<td>url</td>
<td>数据库地址</td>
</tr>
<tr>
<td>dbtable</td>
<td>表名称</td>
</tr>
<tr>
<td>driver</td>
<td>数据库驱动</td>
</tr>
<tr>
<td>partitionColumn,<br>lowerBound, upperBoun</td>
<td>分区总数，上界，下界</td>
</tr>
<tr>
<td>numPartitions</td>
<td>可用于表读写并行性的最大分区数。如果要写的分区数量超过这个限制，那么可以调用 coalesce(numpartition) 重置分区数。</td>
</tr>
<tr>
<td>fetchsize</td>
<td>每次往返要获取多少行数据。此选项仅适用于读取数据。</td>
</tr>
<tr>
<td>batchsize</td>
<td>每次往返插入多少行数据，这个选项只适用于写入数据。默认值是 1000。</td>
</tr>
<tr>
<td>isolationLevel</td>
<td>事务隔离级别：可以是 NONE，READ_COMMITTED, READ_UNCOMMITTED，REPEATABLE_READ 或 SERIALIZABLE，即标准事务隔离级别。<br>默认值是 READ_UNCOMMITTED。这个选项只适用于数据读取。</td>
</tr>
<tr>
<td>createTableOptions</td>
<td>写入数据时自定义创建表的相关配置</td>
</tr>
<tr>
<td>createTableColumnTypes</td>
<td>写入数据时自定义创建列的列类型</td>
</tr>
</tbody>
</table>
<blockquote>
<p>数据库读写更多配置可以参阅官方文档：<a href="https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html" target="_blank" rel="noopener">https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html</a></p>
</blockquote>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol>
<li>Matei Zaharia, Bill Chambers . Spark: The Definitive Guide[M] . 2018-02 </li>
<li><a href="https://spark.apache.org/docs/latest/sql-data-sources.html" target="_blank" rel="noopener">https://spark.apache.org/docs/latest/sql-data-sources.html</a></li>
</ol>

      
    </div>
    
    
    

    
      <div>
        <div>
    
        <div style="text-align:center;color: #ccc;font-size:14px;">
        	-------------本文结束<i class="fa fa-paw"></i>感谢您的阅读-------------
        </div>
    
    
    	<div style="text-align:left;color: #ccc;font-size:12px;">
    		作者：Mr.刘 <br>
    		来源：GitHub <br>
			原文：https://ly464782123.github.io <br>
			版权声明：本文为博主原创文章，转载请附上博文链接,谢谢(*￣︶￣)！
    	</div>
    
</div>
      </div>
    

    
      <div>
        
      </div>
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Spark/" rel="tag"># Spark</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2021/03/17/Spark简介/" rel="next" title="Spark简介">
                <i class="fa fa-chevron-left"></i> Spark简介
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
    </div>
  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/2.gif" alt="Mr.刘">
            
              <p class="site-author-name" itemprop="name">Mr.刘</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">44</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">13</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">13</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/LY464782123" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:464782123@qq.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#一、简介"><span class="nav-number">1.</span> <span class="nav-text">一、简介</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-多数据源支持"><span class="nav-number">1.1.</span> <span class="nav-text">1.1 多数据源支持</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-读数据格式"><span class="nav-number">1.2.</span> <span class="nav-text">1.2 读数据格式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-写数据格式"><span class="nav-number">1.3.</span> <span class="nav-text">1.3 写数据格式</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#二、CSV"><span class="nav-number">2.</span> <span class="nav-text">二、CSV</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-读取CSV文件"><span class="nav-number">2.1.</span> <span class="nav-text">2.1 读取CSV文件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-写入CSV文件"><span class="nav-number">2.2.</span> <span class="nav-text">2.2 写入CSV文件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-可选配置"><span class="nav-number">2.3.</span> <span class="nav-text">2.3 可选配置</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#三、JSON"><span class="nav-number">3.</span> <span class="nav-text">三、JSON</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-读取JSON文件"><span class="nav-number">3.1.</span> <span class="nav-text">3.1 读取JSON文件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-写入JSON文件"><span class="nav-number">3.2.</span> <span class="nav-text">3.2 写入JSON文件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-可选配置"><span class="nav-number">3.3.</span> <span class="nav-text">3.3 可选配置</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#四、Parquet"><span class="nav-number">4.</span> <span class="nav-text">四、Parquet</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-读取Parquet文件"><span class="nav-number">4.1.</span> <span class="nav-text">4.1 读取Parquet文件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-写入Parquet文件"><span class="nav-number">4.2.</span> <span class="nav-text">2.2 写入Parquet文件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-可选配置-1"><span class="nav-number">4.3.</span> <span class="nav-text">2.3 可选配置</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#五、ORC"><span class="nav-number">5.</span> <span class="nav-text">五、ORC</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#5-1-读取ORC文件"><span class="nav-number">5.1.</span> <span class="nav-text">5.1 读取ORC文件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-写入ORC文件"><span class="nav-number">5.2.</span> <span class="nav-text">4.2 写入ORC文件</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#六、SQL-Databases"><span class="nav-number">6.</span> <span class="nav-text">六、SQL Databases</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#6-1-读取数据"><span class="nav-number">6.1.</span> <span class="nav-text">6.1 读取数据</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-2-写入数据"><span class="nav-number">6.2.</span> <span class="nav-text">6.2 写入数据</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#七、Text"><span class="nav-number">7.</span> <span class="nav-text">七、Text</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#7-1-读取Text数据"><span class="nav-number">7.1.</span> <span class="nav-text">7.1 读取Text数据</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-2-写入Text数据"><span class="nav-number">7.2.</span> <span class="nav-text">7.2 写入Text数据</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#八、数据读写高级特性"><span class="nav-number">8.</span> <span class="nav-text">八、数据读写高级特性</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#8-1-并行读"><span class="nav-number">8.1.</span> <span class="nav-text">8.1 并行读</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-2-并行写"><span class="nav-number">8.2.</span> <span class="nav-text">8.2 并行写</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-3-分区写入"><span class="nav-number">8.3.</span> <span class="nav-text">8.3 分区写入</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-3-分桶写入"><span class="nav-number">8.4.</span> <span class="nav-text">8.3 分桶写入</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-5-文件大小管理"><span class="nav-number">8.5.</span> <span class="nav-text">8.5 文件大小管理</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#九、可选配置附录"><span class="nav-number">9.</span> <span class="nav-text">九、可选配置附录</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#9-1-CSV读写可选配置"><span class="nav-number">9.1.</span> <span class="nav-text">9.1 CSV读写可选配置</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#9-2-JSON读写可选配置"><span class="nav-number">9.2.</span> <span class="nav-text">9.2 JSON读写可选配置</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#9-3-数据库读写可选配置"><span class="nav-number">9.3.</span> <span class="nav-text">9.3 数据库读写可选配置</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#参考资料"><span class="nav-number">10.</span> <span class="nav-text">参考资料</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

<div class="copyright">&copy; <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Mr.刘</span>

  
  
  
    <span class="post-meta-divider">|</span>
  

  <!--<div class="powered-by">
    <i class="fa fa-user-md"></i>
    <span id="busuanzi_container_site_uv">
      本站访客数:<span id="busuanzi_value_site_uv"></span>
    </span>
  </div>-->
</div>

<!--
  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>
-->



        
<div class="busuanzi-count">
  <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      访问数<i class="fa fa-user"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      人
    </span>
  

  
    <span class="site-pv">
      总访问量<i class="fa fa-eye"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      次
    </span>
  
</div>








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  










  <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
  <script src="//unpkg.com/valine/dist/Valine.min.js"></script>
  
  <script type="text/javascript">
    var GUEST = ['nick','mail','link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item=>{
      return GUEST.indexOf(item)>-1;
    });
    new Valine({
        el: '#comments' ,
        verify: false,
        notify: true,
        appId: 'nMqf9jn6wQxm15s5uFHXVJFE-gzGzoHsz',
        appKey: '3tyIVLem7Iy9uN4vY8BjbLlG',
        placeholder: 'ヾﾉ≧∀≦)o来啊，快活啊!',
        avatar:'mm',
        guest_info:guest,
        pageSize:'10' || 10,
    });
  </script>



  





  

  

  

  
  

  

  

  

</body>
</html>
<script type="text/javascript" src="/js/src/clickLove.js"></script>
